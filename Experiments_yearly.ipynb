{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddbeb10",
   "metadata": {},
   "source": [
    "# Create data sets for yearly isotope emulation tasks and train models on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5afc99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from predict import *\n",
    "from evaluate import *\n",
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa874e",
   "metadata": {},
   "source": [
    "## 1) Create datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5847b",
   "metadata": {},
   "source": [
    "## Tas, pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fb49a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94133b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738b00a",
   "metadata": {},
   "source": [
    "### tas only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e26d7",
   "metadata": {},
   "source": [
    "### pr only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c62f82",
   "metadata": {},
   "source": [
    "### pr, tas precip weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734600ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_precip_weighted_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38aa91",
   "metadata": {},
   "source": [
    "### pr, tas ico grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25077f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20c6e3",
   "metadata": {},
   "source": [
    "## 2) Run experiments yearly dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b41ce",
   "metadata": {},
   "source": [
    "## Testing the effect of modifications to flat UNet\n",
    "\n",
    "Start by selecting the tas, pr dataset without precipitation weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b4a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d242f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 1e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [\"Masked_MSELoss\", \"Masked_AreaWeightedMSELoss\"]\n",
    "use_coord_conv = [False, True]\n",
    "use_cylindrical_padding = [False, True]\n",
    "n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in loss:\n",
    "    for c_conv in use_coord_conv:\n",
    "        for c_pad in use_cylindrical_padding:\n",
    "            for i in range(n_runs):\n",
    "                print(l, c_conv, c_pad, i)\n",
    "                model_training_description[\"USE_CYLINDRICAL_PADDING\"] = c_pad\n",
    "                model_training_description[\"USE_COORD_CONV\"] = c_conv\n",
    "                model_training_description[\"LOSS\"] = l  # \"MSELoss\" # \"AreaWeightedMSELoss\"\n",
    "                model_training_description[\"RUN_NR\"] = i\n",
    "                unet = train_unet(description, model_training_description, output_folder)\n",
    "                predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af46e8",
   "metadata": {},
   "source": [
    "## Comparing different ML methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f322b3",
   "metadata": {},
   "source": [
    "Results for modified and unmodified flat UNet are already obtained in last cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b39d7a",
   "metadata": {},
   "source": [
    "### Flat grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404cf512",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61395cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "model_training_description[\"N_PC_PREDICTORS\"] = 450\n",
    "model_training_description[\"N_PC_TARGETS\"] = 300\n",
    "model_training_description[\"REGTYPE\"] = \"lasso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, pca_targets, model = train_pca(description, model_training_description, output_folder)\n",
    "print(\"finished training\")\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e92cc",
   "metadata": {},
   "source": [
    "### Linear regression baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3171580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"LinReg_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0629af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = train_linreg_pixelwise(description, model_training_description, output_folder)\n",
    "predict_save_linreg_pixelwise(description, model_training_description, output_folder, models, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61aba6b",
   "metadata": {},
   "source": [
    "### PCA-regression baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4da8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb57476",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning, compute 50x50 logarithmic grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73634a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 50)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb4f1e",
   "metadata": {},
   "source": [
    "### Random-forest baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"RandomForest_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74416dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_random_forest_pixelwise(description, model_training_description, output_folder, verbose=3, n_jobs=-1)\n",
    "predict_save_randomforest_pixelwise(description, model_training_description, output_folder, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766e0bd",
   "metadata": {},
   "source": [
    "### Icosahedral grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f14fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724648f",
   "metadata": {},
   "source": [
    "### PCA baseline on icosahedral grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7421985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01964634",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 20)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f136a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b4266",
   "metadata": {},
   "source": [
    "### Ico UNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534db2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 1e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = IcoBatchNorm2d # torch.nn.BatchNorm2d\n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "model_training_description[\"LOSS\"] = \"MSELoss\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbf8e0",
   "metadata": {},
   "source": [
    "## Comparing different predictor variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f8e08",
   "metadata": {},
   "source": [
    "### tas only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 1e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075438b1",
   "metadata": {},
   "source": [
    "### precip only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea388ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 1e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder, use_tensorboard=True)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aad21",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "### Learning rate tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba323ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.logspace(-4,-1,20)\n",
    "runs_per_lr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d75ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new_lr\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "508a3734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7119\n",
      "Epoch [2], Iter [91/101] Loss: 0.6673\n",
      "Epoch [3], Iter [91/101] Loss: 0.6078\n",
      "Epoch [4], Iter [91/101] Loss: 0.5898\n",
      "Epoch [5], Iter [91/101] Loss: 0.5818\n",
      "Epoch [6], Iter [91/101] Loss: 0.5704\n",
      "Epoch [7], Iter [91/101] Loss: 0.5853\n",
      "Epoch [8], Iter [91/101] Loss: 0.5513\n",
      "Epoch [9], Iter [91/101] Loss: 0.5654\n",
      "Epoch [10], Iter [91/101] Loss: 0.5454\n",
      "Epoch [11], Iter [91/101] Loss: 0.5383\n",
      "Epoch [12], Iter [91/101] Loss: 0.5547\n",
      "Epoch [13], Iter [91/101] Loss: 0.5546\n",
      "Epoch [14], Iter [91/101] Loss: 0.5380\n",
      "Epoch [15], Iter [91/101] Loss: 0.5231\n",
      "Epoch [16], Iter [91/101] Loss: 0.5415\n",
      "Epoch [17], Iter [91/101] Loss: 0.4902\n",
      "Epoch [18], Iter [91/101] Loss: 0.4913\n",
      "Epoch [19], Iter [91/101] Loss: 0.4780\n",
      "Epoch [20], Iter [91/101] Loss: 0.4940\n",
      "Epoch [21], Iter [91/101] Loss: 0.4936\n",
      "Epoch [22], Iter [91/101] Loss: 0.4906\n",
      "Test MSE: 0.6538720726966858\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6883\n",
      "Epoch [2], Iter [91/101] Loss: 0.6237\n",
      "Epoch [3], Iter [91/101] Loss: 0.6024\n",
      "Epoch [4], Iter [91/101] Loss: 0.6417\n",
      "Epoch [5], Iter [91/101] Loss: 0.5891\n",
      "Epoch [6], Iter [91/101] Loss: 0.5675\n",
      "Epoch [7], Iter [91/101] Loss: 0.5715\n",
      "Epoch [8], Iter [91/101] Loss: 0.5617\n",
      "Epoch [9], Iter [91/101] Loss: 0.5806\n",
      "Epoch [10], Iter [91/101] Loss: 0.5397\n",
      "Epoch [11], Iter [91/101] Loss: 0.5514\n",
      "Epoch [12], Iter [91/101] Loss: 0.5501\n",
      "Epoch [13], Iter [91/101] Loss: 0.5128\n",
      "Epoch [14], Iter [91/101] Loss: 0.5901\n",
      "Epoch [15], Iter [91/101] Loss: 0.5186\n",
      "Epoch [16], Iter [91/101] Loss: 0.5073\n",
      "Epoch [17], Iter [91/101] Loss: 0.4981\n",
      "Epoch [18], Iter [91/101] Loss: 0.5039\n",
      "Epoch [19], Iter [91/101] Loss: 0.5168\n",
      "Epoch [20], Iter [91/101] Loss: 0.5178\n",
      "Epoch [21], Iter [91/101] Loss: 0.4907\n",
      "Epoch [22], Iter [91/101] Loss: 0.4920\n",
      "Epoch [23], Iter [91/101] Loss: 0.4895\n",
      "Epoch [24], Iter [91/101] Loss: 0.4773\n",
      "Epoch [25], Iter [91/101] Loss: 0.5591\n",
      "Epoch [26], Iter [91/101] Loss: 0.4775\n",
      "Test MSE: 0.6531788110733032\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7123\n",
      "Epoch [2], Iter [91/101] Loss: 1.4667\n",
      "Epoch [3], Iter [91/101] Loss: 0.6327\n",
      "Epoch [4], Iter [91/101] Loss: 0.6020\n",
      "Epoch [5], Iter [91/101] Loss: 0.5716\n",
      "Epoch [6], Iter [91/101] Loss: 0.5554\n",
      "Epoch [7], Iter [91/101] Loss: 0.6610\n",
      "Epoch [8], Iter [91/101] Loss: 0.5740\n",
      "Epoch [9], Iter [91/101] Loss: 0.5890\n",
      "Epoch [10], Iter [91/101] Loss: 0.5395\n",
      "Epoch [11], Iter [91/101] Loss: 0.5266\n",
      "Epoch [12], Iter [91/101] Loss: 0.5418\n",
      "Epoch [13], Iter [91/101] Loss: 0.5113\n",
      "Epoch [14], Iter [91/101] Loss: 0.6351\n",
      "Epoch [15], Iter [91/101] Loss: 0.5037\n",
      "Epoch [16], Iter [91/101] Loss: 0.4953\n",
      "Epoch [17], Iter [91/101] Loss: 0.5194\n",
      "Test MSE: 0.6468571424484253\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6579\n",
      "Epoch [2], Iter [91/101] Loss: 0.6108\n",
      "Epoch [3], Iter [91/101] Loss: 0.6281\n",
      "Epoch [4], Iter [91/101] Loss: 0.5933\n",
      "Epoch [5], Iter [91/101] Loss: 0.5983\n",
      "Epoch [6], Iter [91/101] Loss: 0.5774\n",
      "Epoch [7], Iter [91/101] Loss: 0.5577\n",
      "Epoch [8], Iter [91/101] Loss: 0.5570\n",
      "Epoch [9], Iter [91/101] Loss: 0.5644\n",
      "Epoch [10], Iter [91/101] Loss: 0.5676\n",
      "Epoch [11], Iter [91/101] Loss: 0.5590\n",
      "Epoch [12], Iter [91/101] Loss: 0.5317\n",
      "Epoch [13], Iter [91/101] Loss: 0.5119\n",
      "Test MSE: 0.6540399193763733\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6854\n",
      "Epoch [2], Iter [91/101] Loss: 0.6640\n",
      "Epoch [3], Iter [91/101] Loss: 0.6335\n",
      "Epoch [4], Iter [91/101] Loss: 0.5951\n",
      "Epoch [5], Iter [91/101] Loss: 0.5848\n",
      "Epoch [6], Iter [91/101] Loss: 0.5717\n",
      "Epoch [7], Iter [91/101] Loss: 0.5759\n",
      "Epoch [8], Iter [91/101] Loss: 0.5569\n",
      "Epoch [9], Iter [91/101] Loss: 0.5536\n",
      "Epoch [10], Iter [91/101] Loss: 0.5641\n",
      "Epoch [11], Iter [91/101] Loss: 0.5305\n",
      "Epoch [12], Iter [91/101] Loss: 0.5969\n",
      "Epoch [13], Iter [91/101] Loss: 0.5590\n",
      "Epoch [14], Iter [91/101] Loss: 0.4969\n",
      "Epoch [15], Iter [91/101] Loss: 0.5281\n",
      "Epoch [16], Iter [91/101] Loss: 0.5217\n",
      "Test MSE: 0.6450433135032654\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6737\n",
      "Epoch [2], Iter [91/101] Loss: 0.6550\n",
      "Epoch [3], Iter [91/101] Loss: 0.6459\n",
      "Epoch [4], Iter [91/101] Loss: 0.7068\n",
      "Epoch [5], Iter [91/101] Loss: 0.5997\n",
      "Epoch [6], Iter [91/101] Loss: 0.5728\n",
      "Epoch [7], Iter [91/101] Loss: 0.5429\n",
      "Epoch [8], Iter [91/101] Loss: 0.5602\n",
      "Epoch [9], Iter [91/101] Loss: 0.5557\n",
      "Epoch [10], Iter [91/101] Loss: 0.5376\n",
      "Epoch [11], Iter [91/101] Loss: 0.5507\n",
      "Epoch [12], Iter [91/101] Loss: 0.5560\n",
      "Epoch [13], Iter [91/101] Loss: 0.5275\n",
      "Epoch [14], Iter [91/101] Loss: 0.5034\n",
      "Epoch [15], Iter [91/101] Loss: 0.4943\n",
      "Epoch [16], Iter [91/101] Loss: 0.5156\n",
      "Epoch [17], Iter [91/101] Loss: 0.5192\n",
      "Epoch [18], Iter [91/101] Loss: 0.5097\n",
      "Epoch [19], Iter [91/101] Loss: 0.4864\n",
      "Epoch [20], Iter [91/101] Loss: 0.4942\n",
      "Test MSE: 0.6422095894813538\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6359\n",
      "Epoch [2], Iter [91/101] Loss: 0.6159\n",
      "Epoch [3], Iter [91/101] Loss: 0.7812\n",
      "Epoch [4], Iter [91/101] Loss: 0.5920\n",
      "Epoch [5], Iter [91/101] Loss: 0.5864\n",
      "Epoch [6], Iter [91/101] Loss: 0.5568\n",
      "Epoch [7], Iter [91/101] Loss: 0.5382\n",
      "Epoch [8], Iter [91/101] Loss: 0.5380\n",
      "Epoch [9], Iter [91/101] Loss: 1.2131\n",
      "Epoch [10], Iter [91/101] Loss: 0.5148\n",
      "Epoch [11], Iter [91/101] Loss: 0.5242\n",
      "Epoch [12], Iter [91/101] Loss: 0.5267\n",
      "Epoch [13], Iter [91/101] Loss: 0.5047\n",
      "Epoch [14], Iter [91/101] Loss: 0.4916\n",
      "Test MSE: 0.6491112112998962\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7924\n",
      "Epoch [2], Iter [91/101] Loss: 0.6519\n",
      "Epoch [3], Iter [91/101] Loss: 0.6094\n",
      "Epoch [4], Iter [91/101] Loss: 0.6071\n",
      "Epoch [5], Iter [91/101] Loss: 0.6229\n",
      "Epoch [6], Iter [91/101] Loss: 0.5846\n",
      "Epoch [7], Iter [91/101] Loss: 0.5671\n",
      "Epoch [8], Iter [91/101] Loss: 0.6263\n",
      "Epoch [9], Iter [91/101] Loss: 0.5566\n",
      "Epoch [10], Iter [91/101] Loss: 0.5239\n",
      "Epoch [11], Iter [91/101] Loss: 0.5421\n",
      "Epoch [12], Iter [91/101] Loss: 0.5488\n",
      "Epoch [13], Iter [91/101] Loss: 0.5178\n",
      "Epoch [14], Iter [91/101] Loss: 0.4879\n",
      "Epoch [15], Iter [91/101] Loss: 0.5474\n",
      "Epoch [16], Iter [91/101] Loss: 0.4937\n",
      "Epoch [17], Iter [91/101] Loss: 0.5024\n",
      "Epoch [18], Iter [91/101] Loss: 0.4790\n",
      "Test MSE: 0.6354610919952393\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6684\n",
      "Epoch [2], Iter [91/101] Loss: 0.6496\n",
      "Epoch [3], Iter [91/101] Loss: 0.6136\n",
      "Epoch [4], Iter [91/101] Loss: 0.5801\n",
      "Epoch [5], Iter [91/101] Loss: 0.6102\n",
      "Epoch [6], Iter [91/101] Loss: 0.5542\n",
      "Epoch [7], Iter [91/101] Loss: 0.5598\n",
      "Epoch [8], Iter [91/101] Loss: 0.5515\n",
      "Epoch [9], Iter [91/101] Loss: 0.5350\n",
      "Epoch [10], Iter [91/101] Loss: 0.5439\n",
      "Epoch [11], Iter [91/101] Loss: 0.5249\n",
      "Epoch [12], Iter [91/101] Loss: 0.6234\n",
      "Epoch [13], Iter [91/101] Loss: 0.5681\n",
      "Epoch [14], Iter [91/101] Loss: 0.5181\n",
      "Epoch [15], Iter [91/101] Loss: 0.4975\n",
      "Epoch [16], Iter [91/101] Loss: 0.4806\n",
      "Epoch [17], Iter [91/101] Loss: 0.4926\n",
      "Epoch [18], Iter [91/101] Loss: 0.4997\n",
      "Epoch [19], Iter [91/101] Loss: 0.4775\n",
      "Epoch [20], Iter [91/101] Loss: 0.4699\n",
      "Epoch [21], Iter [91/101] Loss: 0.4583\n",
      "Test MSE: 0.6366812586784363\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6523\n",
      "Epoch [2], Iter [91/101] Loss: 0.6539\n",
      "Epoch [3], Iter [91/101] Loss: 0.6434\n",
      "Epoch [4], Iter [91/101] Loss: 0.5922\n",
      "Epoch [5], Iter [91/101] Loss: 0.5535\n",
      "Epoch [6], Iter [91/101] Loss: 0.5856\n",
      "Epoch [7], Iter [91/101] Loss: 0.5294\n",
      "Epoch [8], Iter [91/101] Loss: 0.5390\n",
      "Epoch [9], Iter [91/101] Loss: 0.5455\n",
      "Epoch [10], Iter [91/101] Loss: 0.5466\n",
      "Epoch [11], Iter [91/101] Loss: 0.5164\n",
      "Epoch [12], Iter [91/101] Loss: 0.5156\n",
      "Epoch [13], Iter [91/101] Loss: 0.4956\n",
      "Epoch [14], Iter [91/101] Loss: 0.5229\n",
      "Epoch [15], Iter [91/101] Loss: 0.4854\n",
      "Epoch [16], Iter [91/101] Loss: 0.4900\n",
      "Epoch [17], Iter [91/101] Loss: 0.4972\n",
      "Epoch [18], Iter [91/101] Loss: 0.4904\n",
      "Epoch [19], Iter [91/101] Loss: 0.4690\n",
      "Test MSE: 0.6380133628845215\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6598\n",
      "Epoch [2], Iter [91/101] Loss: 0.6447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Iter [91/101] Loss: 0.6828\n",
      "Epoch [4], Iter [91/101] Loss: 0.5857\n",
      "Epoch [5], Iter [91/101] Loss: 0.5801\n",
      "Epoch [6], Iter [91/101] Loss: 0.5745\n",
      "Epoch [7], Iter [91/101] Loss: 0.5648\n",
      "Epoch [8], Iter [91/101] Loss: 0.5689\n",
      "Epoch [9], Iter [91/101] Loss: 0.5580\n",
      "Epoch [10], Iter [91/101] Loss: 0.5279\n",
      "Epoch [11], Iter [91/101] Loss: 0.6115\n",
      "Epoch [12], Iter [91/101] Loss: 0.5091\n",
      "Epoch [13], Iter [91/101] Loss: 0.4861\n",
      "Epoch [14], Iter [91/101] Loss: 0.4871\n",
      "Test MSE: 0.6354402899742126\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6620\n",
      "Epoch [2], Iter [91/101] Loss: 0.6458\n",
      "Epoch [3], Iter [91/101] Loss: 0.7688\n",
      "Epoch [4], Iter [91/101] Loss: 0.5739\n",
      "Epoch [5], Iter [91/101] Loss: 0.5884\n",
      "Epoch [6], Iter [91/101] Loss: 0.5650\n",
      "Epoch [7], Iter [91/101] Loss: 0.5396\n",
      "Epoch [8], Iter [91/101] Loss: 0.5559\n",
      "Epoch [9], Iter [91/101] Loss: 0.5518\n",
      "Epoch [10], Iter [91/101] Loss: 0.5230\n",
      "Epoch [11], Iter [91/101] Loss: 0.5540\n",
      "Epoch [12], Iter [91/101] Loss: 0.5111\n",
      "Test MSE: 0.637643039226532\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7087\n",
      "Epoch [2], Iter [91/101] Loss: 0.6177\n",
      "Epoch [3], Iter [91/101] Loss: 0.6279\n",
      "Epoch [4], Iter [91/101] Loss: 0.6133\n",
      "Epoch [5], Iter [91/101] Loss: 0.5822\n",
      "Epoch [6], Iter [91/101] Loss: 0.5606\n",
      "Epoch [7], Iter [91/101] Loss: 0.5621\n",
      "Epoch [8], Iter [91/101] Loss: 0.5407\n",
      "Epoch [9], Iter [91/101] Loss: 0.5122\n",
      "Epoch [10], Iter [91/101] Loss: 0.5447\n",
      "Epoch [11], Iter [91/101] Loss: 0.5312\n",
      "Epoch [12], Iter [91/101] Loss: 0.5181\n",
      "Epoch [13], Iter [91/101] Loss: 0.5124\n",
      "Epoch [14], Iter [91/101] Loss: 0.5085\n",
      "Epoch [15], Iter [91/101] Loss: 0.4944\n",
      "Epoch [16], Iter [91/101] Loss: 0.5097\n",
      "Epoch [17], Iter [91/101] Loss: 0.4672\n",
      "Epoch [18], Iter [91/101] Loss: 0.4790\n",
      "Epoch [19], Iter [91/101] Loss: 0.4858\n",
      "Epoch [20], Iter [91/101] Loss: 0.4702\n",
      "Epoch [21], Iter [91/101] Loss: 0.4754\n",
      "Epoch [22], Iter [91/101] Loss: 0.4550\n",
      "Test MSE: 0.6386589407920837\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7210\n",
      "Epoch [2], Iter [91/101] Loss: 0.6223\n",
      "Epoch [3], Iter [91/101] Loss: 0.5989\n",
      "Epoch [4], Iter [91/101] Loss: 0.5690\n",
      "Epoch [5], Iter [91/101] Loss: 0.5621\n",
      "Epoch [6], Iter [91/101] Loss: 0.5808\n",
      "Epoch [7], Iter [91/101] Loss: 0.5606\n",
      "Epoch [8], Iter [91/101] Loss: 0.5617\n",
      "Epoch [9], Iter [91/101] Loss: 0.7183\n",
      "Epoch [10], Iter [91/101] Loss: 0.5292\n",
      "Epoch [11], Iter [91/101] Loss: 0.5615\n",
      "Epoch [12], Iter [91/101] Loss: 0.5201\n",
      "Epoch [13], Iter [91/101] Loss: 0.5041\n",
      "Epoch [14], Iter [91/101] Loss: 0.5006\n",
      "Epoch [15], Iter [91/101] Loss: 0.4970\n",
      "Epoch [16], Iter [91/101] Loss: 0.4994\n",
      "Test MSE: 0.6315594911575317\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6093\n",
      "Epoch [2], Iter [91/101] Loss: 0.6160\n",
      "Epoch [3], Iter [91/101] Loss: 0.6117\n",
      "Epoch [4], Iter [91/101] Loss: 0.6929\n",
      "Epoch [5], Iter [91/101] Loss: 0.5500\n",
      "Epoch [6], Iter [91/101] Loss: 0.5687\n",
      "Epoch [7], Iter [91/101] Loss: 0.5721\n",
      "Epoch [8], Iter [91/101] Loss: 0.5404\n",
      "Epoch [9], Iter [91/101] Loss: 0.5231\n",
      "Epoch [10], Iter [91/101] Loss: 0.6071\n",
      "Epoch [11], Iter [91/101] Loss: 0.5648\n",
      "Epoch [12], Iter [91/101] Loss: 0.5383\n",
      "Epoch [13], Iter [91/101] Loss: 0.5115\n",
      "Epoch [14], Iter [91/101] Loss: 0.5283\n",
      "Epoch [15], Iter [91/101] Loss: 0.5165\n",
      "Epoch [16], Iter [91/101] Loss: 0.4840\n",
      "Epoch [17], Iter [91/101] Loss: 0.4926\n",
      "Epoch [18], Iter [91/101] Loss: 0.4796\n",
      "Epoch [19], Iter [91/101] Loss: 0.4662\n",
      "Test MSE: 0.6267971396446228\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6503\n",
      "Epoch [2], Iter [91/101] Loss: 0.6411\n",
      "Epoch [3], Iter [91/101] Loss: 0.5790\n",
      "Epoch [4], Iter [91/101] Loss: 0.5583\n",
      "Epoch [5], Iter [91/101] Loss: 0.6193\n",
      "Epoch [6], Iter [91/101] Loss: 0.5766\n",
      "Epoch [7], Iter [91/101] Loss: 0.5744\n",
      "Epoch [8], Iter [91/101] Loss: 0.5745\n",
      "Epoch [9], Iter [91/101] Loss: 0.5447\n",
      "Epoch [10], Iter [91/101] Loss: 0.5454\n",
      "Epoch [11], Iter [91/101] Loss: 0.5259\n",
      "Epoch [12], Iter [91/101] Loss: 0.5279\n",
      "Epoch [13], Iter [91/101] Loss: 0.5276\n",
      "Epoch [14], Iter [91/101] Loss: 0.4979\n",
      "Epoch [15], Iter [91/101] Loss: 0.4805\n",
      "Epoch [16], Iter [91/101] Loss: 0.5003\n",
      "Epoch [17], Iter [91/101] Loss: 0.5538\n",
      "Epoch [18], Iter [91/101] Loss: 0.4874\n",
      "Test MSE: 0.6222020983695984\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6581\n",
      "Epoch [2], Iter [91/101] Loss: 0.6379\n",
      "Epoch [3], Iter [91/101] Loss: 0.6149\n",
      "Epoch [4], Iter [91/101] Loss: 0.5847\n",
      "Epoch [5], Iter [91/101] Loss: 0.5709\n",
      "Epoch [6], Iter [91/101] Loss: 0.5740\n",
      "Epoch [7], Iter [91/101] Loss: 0.5728\n",
      "Epoch [8], Iter [91/101] Loss: 0.5359\n",
      "Epoch [9], Iter [91/101] Loss: 0.6169\n",
      "Epoch [10], Iter [91/101] Loss: 0.5142\n",
      "Epoch [11], Iter [91/101] Loss: 0.5315\n",
      "Epoch [12], Iter [91/101] Loss: 0.5367\n",
      "Epoch [13], Iter [91/101] Loss: 0.5190\n",
      "Epoch [14], Iter [91/101] Loss: 0.5043\n",
      "Epoch [15], Iter [91/101] Loss: 0.5031\n",
      "Epoch [16], Iter [91/101] Loss: 0.5037\n",
      "Epoch [17], Iter [91/101] Loss: 0.4935\n",
      "Epoch [18], Iter [91/101] Loss: 0.4930\n",
      "Epoch [19], Iter [91/101] Loss: 0.4585\n",
      "Epoch [20], Iter [91/101] Loss: 0.4716\n",
      "Epoch [21], Iter [91/101] Loss: 0.4807\n",
      "Epoch [22], Iter [91/101] Loss: 0.4532\n",
      "Test MSE: 0.6258244514465332\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6428\n",
      "Epoch [2], Iter [91/101] Loss: 0.6356\n",
      "Epoch [3], Iter [91/101] Loss: 0.7169\n",
      "Epoch [4], Iter [91/101] Loss: 0.5872\n",
      "Epoch [5], Iter [91/101] Loss: 1.3336\n",
      "Epoch [6], Iter [91/101] Loss: 0.5852\n",
      "Epoch [7], Iter [91/101] Loss: 0.5421\n",
      "Epoch [8], Iter [91/101] Loss: 0.5374\n",
      "Epoch [9], Iter [91/101] Loss: 0.5432\n",
      "Epoch [10], Iter [91/101] Loss: 0.5411\n",
      "Epoch [11], Iter [91/101] Loss: 0.5234\n",
      "Epoch [12], Iter [91/101] Loss: 0.5242\n",
      "Epoch [13], Iter [91/101] Loss: 0.5341\n",
      "Epoch [14], Iter [91/101] Loss: 0.5284\n",
      "Epoch [15], Iter [91/101] Loss: 0.5039\n",
      "Epoch [16], Iter [91/101] Loss: 0.4792\n",
      "Epoch [17], Iter [91/101] Loss: 0.4946\n",
      "Epoch [18], Iter [91/101] Loss: 0.4786\n",
      "Epoch [19], Iter [91/101] Loss: 0.4778\n",
      "Test MSE: 0.6280187964439392\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6169\n",
      "Epoch [2], Iter [91/101] Loss: 0.6764\n",
      "Epoch [3], Iter [91/101] Loss: 0.6879\n",
      "Epoch [4], Iter [91/101] Loss: 0.6344\n",
      "Epoch [5], Iter [91/101] Loss: 0.5715\n",
      "Epoch [6], Iter [91/101] Loss: 0.5598\n",
      "Epoch [7], Iter [91/101] Loss: 0.5784\n",
      "Epoch [8], Iter [91/101] Loss: 0.5593\n",
      "Epoch [9], Iter [91/101] Loss: 0.5196\n",
      "Epoch [10], Iter [91/101] Loss: 0.5527\n",
      "Epoch [11], Iter [91/101] Loss: 0.5251\n",
      "Epoch [12], Iter [91/101] Loss: 0.5299\n",
      "Epoch [13], Iter [91/101] Loss: 0.5229\n",
      "Test MSE: 0.6345149278640747\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6558\n",
      "Epoch [2], Iter [91/101] Loss: 0.6001\n",
      "Epoch [3], Iter [91/101] Loss: 0.6116\n",
      "Epoch [4], Iter [91/101] Loss: 0.5983\n",
      "Epoch [5], Iter [91/101] Loss: 0.5781\n",
      "Epoch [6], Iter [91/101] Loss: 0.5658\n",
      "Epoch [7], Iter [91/101] Loss: 0.5750\n",
      "Epoch [8], Iter [91/101] Loss: 0.5500\n",
      "Epoch [9], Iter [91/101] Loss: 0.5292\n",
      "Epoch [10], Iter [91/101] Loss: 0.5580\n",
      "Epoch [11], Iter [91/101] Loss: 0.5491\n",
      "Epoch [12], Iter [91/101] Loss: 0.5682\n",
      "Epoch [13], Iter [91/101] Loss: 0.5486\n",
      "Epoch [14], Iter [91/101] Loss: 0.5205\n",
      "Epoch [15], Iter [91/101] Loss: 0.5165\n",
      "Epoch [16], Iter [91/101] Loss: 0.5119\n",
      "Epoch [17], Iter [91/101] Loss: 0.4924\n",
      "Epoch [18], Iter [91/101] Loss: 0.5292\n",
      "Epoch [19], Iter [91/101] Loss: 0.4856\n",
      "Epoch [20], Iter [91/101] Loss: 0.4889\n",
      "Epoch [21], Iter [91/101] Loss: 0.5210\n",
      "Epoch [22], Iter [91/101] Loss: 0.4840\n",
      "Test MSE: 0.6126623153686523\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 1.3688\n",
      "Epoch [2], Iter [91/101] Loss: 0.5988\n",
      "Epoch [3], Iter [91/101] Loss: 0.6024\n",
      "Epoch [4], Iter [91/101] Loss: 0.5920\n",
      "Epoch [5], Iter [91/101] Loss: 0.5651\n",
      "Epoch [6], Iter [91/101] Loss: 0.5705\n",
      "Epoch [7], Iter [91/101] Loss: 0.5417\n",
      "Epoch [8], Iter [91/101] Loss: 0.6475\n",
      "Epoch [9], Iter [91/101] Loss: 0.5453\n",
      "Epoch [10], Iter [91/101] Loss: 0.5631\n",
      "Epoch [11], Iter [91/101] Loss: 0.5380\n",
      "Epoch [12], Iter [91/101] Loss: 0.5568\n",
      "Epoch [13], Iter [91/101] Loss: 0.5138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Iter [91/101] Loss: 0.5242\n",
      "Epoch [15], Iter [91/101] Loss: 0.5134\n",
      "Epoch [16], Iter [91/101] Loss: 0.5124\n",
      "Epoch [17], Iter [91/101] Loss: 0.5259\n",
      "Epoch [18], Iter [91/101] Loss: 0.4783\n",
      "Epoch [19], Iter [91/101] Loss: 0.4809\n",
      "Epoch [20], Iter [91/101] Loss: 0.4796\n",
      "Epoch [21], Iter [91/101] Loss: 0.4820\n",
      "Epoch [22], Iter [91/101] Loss: 0.4879\n",
      "Test MSE: 0.6237897872924805\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6500\n",
      "Epoch [2], Iter [91/101] Loss: 0.6025\n",
      "Epoch [3], Iter [91/101] Loss: 0.6050\n",
      "Epoch [4], Iter [91/101] Loss: 0.5638\n",
      "Epoch [5], Iter [91/101] Loss: 0.5842\n",
      "Epoch [6], Iter [91/101] Loss: 0.6896\n",
      "Epoch [7], Iter [91/101] Loss: 0.5646\n",
      "Epoch [8], Iter [91/101] Loss: 0.6568\n",
      "Epoch [9], Iter [91/101] Loss: 0.5580\n",
      "Epoch [10], Iter [91/101] Loss: 0.5523\n",
      "Epoch [11], Iter [91/101] Loss: 0.5375\n",
      "Epoch [12], Iter [91/101] Loss: 0.5364\n",
      "Epoch [13], Iter [91/101] Loss: 0.5208\n",
      "Epoch [14], Iter [91/101] Loss: 0.5122\n",
      "Epoch [15], Iter [91/101] Loss: 0.5252\n",
      "Epoch [16], Iter [91/101] Loss: 0.5020\n",
      "Epoch [17], Iter [91/101] Loss: 0.5113\n",
      "Test MSE: 0.6194495558738708\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6643\n",
      "Epoch [2], Iter [91/101] Loss: 0.6317\n",
      "Epoch [3], Iter [91/101] Loss: 0.5967\n",
      "Epoch [4], Iter [91/101] Loss: 0.6037\n",
      "Epoch [5], Iter [91/101] Loss: 0.6905\n",
      "Epoch [6], Iter [91/101] Loss: 0.6650\n",
      "Epoch [7], Iter [91/101] Loss: 0.5780\n",
      "Epoch [8], Iter [91/101] Loss: 0.6042\n",
      "Epoch [9], Iter [91/101] Loss: 0.5521\n",
      "Epoch [10], Iter [91/101] Loss: 0.5628\n",
      "Epoch [11], Iter [91/101] Loss: 0.5451\n",
      "Epoch [12], Iter [91/101] Loss: 0.5468\n",
      "Epoch [13], Iter [91/101] Loss: 0.5426\n",
      "Epoch [14], Iter [91/101] Loss: 0.5353\n",
      "Epoch [15], Iter [91/101] Loss: 0.5164\n",
      "Epoch [16], Iter [91/101] Loss: 0.5205\n",
      "Epoch [17], Iter [91/101] Loss: 0.4991\n",
      "Epoch [18], Iter [91/101] Loss: 0.5159\n",
      "Epoch [19], Iter [91/101] Loss: 0.5005\n",
      "Epoch [20], Iter [91/101] Loss: 0.4799\n",
      "Epoch [21], Iter [91/101] Loss: 0.4773\n",
      "Epoch [22], Iter [91/101] Loss: 0.5282\n",
      "Test MSE: 0.6210842132568359\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6502\n",
      "Epoch [2], Iter [91/101] Loss: 0.6631\n",
      "Epoch [3], Iter [91/101] Loss: 0.6087\n",
      "Epoch [4], Iter [91/101] Loss: 0.6008\n",
      "Epoch [5], Iter [91/101] Loss: 0.6267\n",
      "Epoch [6], Iter [91/101] Loss: 0.6025\n",
      "Epoch [7], Iter [91/101] Loss: 0.5893\n",
      "Epoch [8], Iter [91/101] Loss: 0.5663\n",
      "Epoch [9], Iter [91/101] Loss: 0.5554\n",
      "Epoch [10], Iter [91/101] Loss: 0.5347\n",
      "Epoch [11], Iter [91/101] Loss: 0.5467\n",
      "Epoch [12], Iter [91/101] Loss: 0.5313\n",
      "Epoch [13], Iter [91/101] Loss: 0.5291\n",
      "Epoch [14], Iter [91/101] Loss: 0.5299\n",
      "Epoch [15], Iter [91/101] Loss: 0.4949\n",
      "Epoch [16], Iter [91/101] Loss: 0.5650\n",
      "Epoch [17], Iter [91/101] Loss: 0.5147\n",
      "Epoch [18], Iter [91/101] Loss: 0.5235\n",
      "Epoch [19], Iter [91/101] Loss: 0.5131\n",
      "Epoch [20], Iter [91/101] Loss: 0.4998\n",
      "Test MSE: 0.6265616416931152\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 1.4214\n",
      "Epoch [2], Iter [91/101] Loss: 0.7289\n",
      "Epoch [3], Iter [91/101] Loss: 0.6117\n",
      "Epoch [4], Iter [91/101] Loss: 0.6081\n",
      "Epoch [5], Iter [91/101] Loss: 0.5849\n",
      "Epoch [6], Iter [91/101] Loss: 0.5759\n",
      "Epoch [7], Iter [91/101] Loss: 0.5827\n",
      "Epoch [8], Iter [91/101] Loss: 0.5922\n",
      "Epoch [9], Iter [91/101] Loss: 0.5536\n",
      "Epoch [10], Iter [91/101] Loss: 0.5567\n",
      "Epoch [11], Iter [91/101] Loss: 0.5511\n",
      "Epoch [12], Iter [91/101] Loss: 0.5409\n",
      "Epoch [13], Iter [91/101] Loss: 0.5340\n",
      "Epoch [14], Iter [91/101] Loss: 0.5193\n",
      "Epoch [15], Iter [91/101] Loss: 0.5454\n",
      "Epoch [16], Iter [91/101] Loss: 0.5230\n",
      "Epoch [17], Iter [91/101] Loss: 0.5292\n",
      "Epoch [18], Iter [91/101] Loss: 0.5129\n",
      "Epoch [19], Iter [91/101] Loss: 0.5292\n",
      "Epoch [20], Iter [91/101] Loss: 0.5076\n",
      "Test MSE: 0.6171776056289673\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8671\n",
      "Epoch [2], Iter [91/101] Loss: 0.6551\n",
      "Epoch [3], Iter [91/101] Loss: 0.6184\n",
      "Epoch [4], Iter [91/101] Loss: 0.6545\n",
      "Epoch [5], Iter [91/101] Loss: 0.5750\n",
      "Epoch [6], Iter [91/101] Loss: 0.5718\n",
      "Epoch [7], Iter [91/101] Loss: 0.5849\n",
      "Epoch [8], Iter [91/101] Loss: 0.5689\n",
      "Epoch [9], Iter [91/101] Loss: 0.5848\n",
      "Epoch [10], Iter [91/101] Loss: 0.5743\n",
      "Epoch [11], Iter [91/101] Loss: 0.5736\n",
      "Epoch [12], Iter [91/101] Loss: 0.5601\n",
      "Epoch [13], Iter [91/101] Loss: 0.5652\n",
      "Epoch [14], Iter [91/101] Loss: 0.5389\n",
      "Epoch [15], Iter [91/101] Loss: 0.5226\n",
      "Epoch [16], Iter [91/101] Loss: 0.5315\n",
      "Epoch [17], Iter [91/101] Loss: 0.5311\n",
      "Epoch [18], Iter [91/101] Loss: 0.5387\n",
      "Epoch [19], Iter [91/101] Loss: 0.5267\n",
      "Epoch [20], Iter [91/101] Loss: 0.4932\n",
      "Epoch [21], Iter [91/101] Loss: 0.4892\n",
      "Test MSE: 0.6136918663978577\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6390\n",
      "Epoch [2], Iter [91/101] Loss: 0.6605\n",
      "Epoch [3], Iter [91/101] Loss: 0.6854\n",
      "Epoch [4], Iter [91/101] Loss: 0.6431\n",
      "Epoch [5], Iter [91/101] Loss: 0.5905\n",
      "Epoch [6], Iter [91/101] Loss: 0.7064\n",
      "Epoch [7], Iter [91/101] Loss: 0.5744\n",
      "Epoch [8], Iter [91/101] Loss: 0.5726\n",
      "Epoch [9], Iter [91/101] Loss: 0.5665\n",
      "Epoch [10], Iter [91/101] Loss: 0.5952\n",
      "Epoch [11], Iter [91/101] Loss: 0.5826\n",
      "Epoch [12], Iter [91/101] Loss: 0.5923\n",
      "Epoch [13], Iter [91/101] Loss: 0.5433\n",
      "Epoch [14], Iter [91/101] Loss: 0.6370\n",
      "Epoch [15], Iter [91/101] Loss: 0.5286\n",
      "Epoch [16], Iter [91/101] Loss: 0.5492\n",
      "Epoch [17], Iter [91/101] Loss: 0.5770\n",
      "Epoch [18], Iter [91/101] Loss: 0.5411\n",
      "Epoch [19], Iter [91/101] Loss: 0.5305\n",
      "Epoch [20], Iter [91/101] Loss: 0.5314\n",
      "Epoch [21], Iter [91/101] Loss: 0.5349\n",
      "Epoch [22], Iter [91/101] Loss: 0.5021\n",
      "Epoch [23], Iter [91/101] Loss: 0.5032\n",
      "Epoch [24], Iter [91/101] Loss: 0.4985\n",
      "Epoch [25], Iter [91/101] Loss: 0.5206\n",
      "Test MSE: 0.6087459921836853\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7088\n",
      "Epoch [2], Iter [91/101] Loss: 0.6398\n",
      "Epoch [3], Iter [91/101] Loss: 0.7696\n",
      "Epoch [4], Iter [91/101] Loss: 0.5902\n",
      "Epoch [5], Iter [91/101] Loss: 0.6181\n",
      "Epoch [6], Iter [91/101] Loss: 0.5972\n",
      "Epoch [7], Iter [91/101] Loss: 0.6167\n",
      "Epoch [8], Iter [91/101] Loss: 0.5976\n",
      "Epoch [9], Iter [91/101] Loss: 0.5814\n",
      "Epoch [10], Iter [91/101] Loss: 0.5705\n",
      "Epoch [11], Iter [91/101] Loss: 0.5440\n",
      "Epoch [12], Iter [91/101] Loss: 0.5678\n",
      "Epoch [13], Iter [91/101] Loss: 0.6454\n",
      "Epoch [14], Iter [91/101] Loss: 0.5509\n",
      "Epoch [15], Iter [91/101] Loss: 0.5465\n",
      "Epoch [16], Iter [91/101] Loss: 0.5603\n",
      "Epoch [17], Iter [91/101] Loss: 0.5220\n",
      "Epoch [18], Iter [91/101] Loss: 0.5450\n",
      "Epoch [19], Iter [91/101] Loss: 0.5368\n",
      "Epoch [20], Iter [91/101] Loss: 0.5129\n",
      "Epoch [21], Iter [91/101] Loss: 0.5297\n",
      "Epoch [22], Iter [91/101] Loss: 0.5144\n",
      "Epoch [23], Iter [91/101] Loss: 0.5041\n",
      "Epoch [24], Iter [91/101] Loss: 0.4960\n",
      "Epoch [25], Iter [91/101] Loss: 0.5022\n",
      "Epoch [26], Iter [91/101] Loss: 0.4821\n",
      "Epoch [27], Iter [91/101] Loss: 0.5005\n",
      "Epoch [28], Iter [91/101] Loss: 0.4773\n",
      "Epoch [29], Iter [91/101] Loss: 0.4870\n",
      "Test MSE: 0.6172423958778381\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6453\n",
      "Epoch [2], Iter [91/101] Loss: 0.6515\n",
      "Epoch [3], Iter [91/101] Loss: 0.6129\n",
      "Epoch [4], Iter [91/101] Loss: 0.6340\n",
      "Epoch [5], Iter [91/101] Loss: 0.6263\n",
      "Epoch [6], Iter [91/101] Loss: 0.5981\n",
      "Epoch [7], Iter [91/101] Loss: 0.5891\n",
      "Epoch [8], Iter [91/101] Loss: 0.5525\n",
      "Epoch [9], Iter [91/101] Loss: 0.5763\n",
      "Epoch [10], Iter [91/101] Loss: 0.5878\n",
      "Epoch [11], Iter [91/101] Loss: 0.5546\n",
      "Epoch [12], Iter [91/101] Loss: 0.5614\n",
      "Epoch [13], Iter [91/101] Loss: 0.5529\n",
      "Epoch [14], Iter [91/101] Loss: 0.5625\n",
      "Epoch [15], Iter [91/101] Loss: 0.5396\n",
      "Epoch [16], Iter [91/101] Loss: 0.5619\n",
      "Epoch [17], Iter [91/101] Loss: 0.5533\n",
      "Epoch [18], Iter [91/101] Loss: 0.5347\n",
      "Epoch [19], Iter [91/101] Loss: 0.5374\n",
      "Epoch [20], Iter [91/101] Loss: 0.5076\n",
      "Epoch [21], Iter [91/101] Loss: 0.5162\n",
      "Epoch [22], Iter [91/101] Loss: 0.5113\n",
      "Epoch [23], Iter [91/101] Loss: 0.5169\n",
      "Epoch [24], Iter [91/101] Loss: 0.4919\n",
      "Epoch [25], Iter [91/101] Loss: 0.5234\n",
      "Epoch [26], Iter [91/101] Loss: 0.5105\n",
      "Epoch [27], Iter [91/101] Loss: 0.5091\n",
      "Epoch [28], Iter [91/101] Loss: 0.5002\n",
      "Epoch [29], Iter [91/101] Loss: 0.4893\n",
      "Test MSE: 0.6178223490715027\n",
      "writing predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6778\n",
      "Epoch [2], Iter [91/101] Loss: 0.6499\n",
      "Epoch [3], Iter [91/101] Loss: 0.7106\n",
      "Epoch [4], Iter [91/101] Loss: 0.6477\n",
      "Epoch [5], Iter [91/101] Loss: 0.7132\n",
      "Epoch [6], Iter [91/101] Loss: 0.6214\n",
      "Epoch [7], Iter [91/101] Loss: 0.6028\n",
      "Epoch [8], Iter [91/101] Loss: 0.6089\n",
      "Epoch [9], Iter [91/101] Loss: 0.5677\n",
      "Epoch [10], Iter [91/101] Loss: 0.5788\n",
      "Epoch [11], Iter [91/101] Loss: 0.5542\n",
      "Epoch [12], Iter [91/101] Loss: 0.5557\n",
      "Epoch [13], Iter [91/101] Loss: 0.5755\n",
      "Epoch [14], Iter [91/101] Loss: 0.5796\n",
      "Epoch [15], Iter [91/101] Loss: 0.5648\n",
      "Epoch [16], Iter [91/101] Loss: 0.5471\n",
      "Epoch [17], Iter [91/101] Loss: 0.5427\n",
      "Epoch [18], Iter [91/101] Loss: 0.5444\n",
      "Epoch [19], Iter [91/101] Loss: 0.5266\n",
      "Epoch [20], Iter [91/101] Loss: 0.5085\n",
      "Epoch [21], Iter [91/101] Loss: 0.5348\n",
      "Epoch [22], Iter [91/101] Loss: 0.5211\n",
      "Epoch [23], Iter [91/101] Loss: 0.5260\n",
      "Epoch [24], Iter [91/101] Loss: 0.5186\n",
      "Epoch [25], Iter [91/101] Loss: 0.4973\n",
      "Epoch [26], Iter [91/101] Loss: 0.5263\n",
      "Epoch [27], Iter [91/101] Loss: 0.5190\n",
      "Epoch [28], Iter [91/101] Loss: 0.4886\n",
      "Epoch [29], Iter [91/101] Loss: 0.5153\n",
      "Epoch [30], Iter [91/101] Loss: 0.5030\n",
      "Epoch [31], Iter [91/101] Loss: 0.4847\n",
      "Epoch [32], Iter [91/101] Loss: 0.4944\n",
      "Test MSE: 0.616318941116333\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7443\n",
      "Epoch [2], Iter [91/101] Loss: 0.6586\n",
      "Epoch [3], Iter [91/101] Loss: 0.6765\n",
      "Epoch [4], Iter [91/101] Loss: 0.6134\n",
      "Epoch [5], Iter [91/101] Loss: 0.6249\n",
      "Epoch [6], Iter [91/101] Loss: 0.6873\n",
      "Epoch [7], Iter [91/101] Loss: 0.6058\n",
      "Epoch [8], Iter [91/101] Loss: 0.6041\n",
      "Epoch [9], Iter [91/101] Loss: 0.5940\n",
      "Epoch [10], Iter [91/101] Loss: 0.5897\n",
      "Epoch [11], Iter [91/101] Loss: 0.5522\n",
      "Epoch [12], Iter [91/101] Loss: 0.5378\n",
      "Epoch [13], Iter [91/101] Loss: 0.5583\n",
      "Epoch [14], Iter [91/101] Loss: 0.5462\n",
      "Epoch [15], Iter [91/101] Loss: 0.5473\n",
      "Epoch [16], Iter [91/101] Loss: 0.5431\n",
      "Epoch [17], Iter [91/101] Loss: 0.6550\n",
      "Epoch [18], Iter [91/101] Loss: 0.5068\n",
      "Epoch [19], Iter [91/101] Loss: 0.5276\n",
      "Epoch [20], Iter [91/101] Loss: 0.5022\n",
      "Epoch [21], Iter [91/101] Loss: 0.5472\n",
      "Epoch [22], Iter [91/101] Loss: 0.5277\n",
      "Epoch [23], Iter [91/101] Loss: 0.5223\n",
      "Epoch [24], Iter [91/101] Loss: 0.5067\n",
      "Epoch [25], Iter [91/101] Loss: 0.5217\n",
      "Epoch [26], Iter [91/101] Loss: 0.5405\n",
      "Epoch [27], Iter [91/101] Loss: 0.5129\n",
      "Epoch [28], Iter [91/101] Loss: 0.4772\n",
      "Epoch [29], Iter [91/101] Loss: 0.4760\n",
      "Epoch [30], Iter [91/101] Loss: 0.4762\n",
      "Epoch [31], Iter [91/101] Loss: 0.4849\n",
      "Epoch [32], Iter [91/101] Loss: 0.4803\n",
      "Test MSE: 0.6084305644035339\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6857\n",
      "Epoch [2], Iter [91/101] Loss: 0.6503\n",
      "Epoch [3], Iter [91/101] Loss: 0.6257\n",
      "Epoch [4], Iter [91/101] Loss: 0.6044\n",
      "Epoch [5], Iter [91/101] Loss: 0.6194\n",
      "Epoch [6], Iter [91/101] Loss: 0.6596\n",
      "Epoch [7], Iter [91/101] Loss: 0.6066\n",
      "Epoch [8], Iter [91/101] Loss: 0.5615\n",
      "Epoch [9], Iter [91/101] Loss: 0.6001\n",
      "Epoch [10], Iter [91/101] Loss: 0.6095\n",
      "Epoch [11], Iter [91/101] Loss: 0.5563\n",
      "Epoch [12], Iter [91/101] Loss: 0.5689\n",
      "Epoch [13], Iter [91/101] Loss: 0.5599\n",
      "Epoch [14], Iter [91/101] Loss: 0.5547\n",
      "Epoch [15], Iter [91/101] Loss: 0.5551\n",
      "Epoch [16], Iter [91/101] Loss: 0.5451\n",
      "Epoch [17], Iter [91/101] Loss: 0.5603\n",
      "Epoch [18], Iter [91/101] Loss: 0.5296\n",
      "Epoch [19], Iter [91/101] Loss: 0.5467\n",
      "Epoch [20], Iter [91/101] Loss: 0.6006\n",
      "Epoch [21], Iter [91/101] Loss: 0.5377\n",
      "Epoch [22], Iter [91/101] Loss: 0.5124\n",
      "Epoch [23], Iter [91/101] Loss: 0.5223\n",
      "Epoch [24], Iter [91/101] Loss: 0.5093\n",
      "Epoch [25], Iter [91/101] Loss: 0.4775\n",
      "Epoch [26], Iter [91/101] Loss: 0.5262\n",
      "Epoch [27], Iter [91/101] Loss: 0.5025\n",
      "Epoch [28], Iter [91/101] Loss: 0.4964\n",
      "Epoch [29], Iter [91/101] Loss: 0.4832\n",
      "Epoch [30], Iter [91/101] Loss: 0.5021\n",
      "Epoch [31], Iter [91/101] Loss: 0.4943\n",
      "Epoch [32], Iter [91/101] Loss: 0.5251\n",
      "Epoch [33], Iter [91/101] Loss: 0.4711\n",
      "Epoch [34], Iter [91/101] Loss: 0.4782\n",
      "Epoch [35], Iter [91/101] Loss: 0.4659\n",
      "Epoch [36], Iter [91/101] Loss: 0.4503\n",
      "Test MSE: 0.6161860823631287\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7004\n",
      "Epoch [2], Iter [91/101] Loss: 0.6350\n",
      "Epoch [3], Iter [91/101] Loss: 0.6461\n",
      "Epoch [4], Iter [91/101] Loss: 0.5964\n",
      "Epoch [5], Iter [91/101] Loss: 0.5915\n",
      "Epoch [6], Iter [91/101] Loss: 0.6811\n",
      "Epoch [7], Iter [91/101] Loss: 0.5835\n",
      "Epoch [8], Iter [91/101] Loss: 0.5824\n",
      "Epoch [9], Iter [91/101] Loss: 0.5897\n",
      "Epoch [10], Iter [91/101] Loss: 0.5525\n",
      "Epoch [11], Iter [91/101] Loss: 0.5869\n",
      "Epoch [12], Iter [91/101] Loss: 0.5598\n",
      "Epoch [13], Iter [91/101] Loss: 0.5612\n",
      "Epoch [14], Iter [91/101] Loss: 0.5710\n",
      "Epoch [15], Iter [91/101] Loss: 0.5579\n",
      "Epoch [16], Iter [91/101] Loss: 0.5200\n",
      "Epoch [17], Iter [91/101] Loss: 0.5978\n",
      "Epoch [18], Iter [91/101] Loss: 0.5338\n",
      "Epoch [19], Iter [91/101] Loss: 0.5433\n",
      "Epoch [20], Iter [91/101] Loss: 0.5447\n",
      "Epoch [21], Iter [91/101] Loss: 0.5804\n",
      "Epoch [22], Iter [91/101] Loss: 0.5379\n",
      "Epoch [23], Iter [91/101] Loss: 0.5323\n",
      "Test MSE: 0.6300728917121887\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6825\n",
      "Epoch [2], Iter [91/101] Loss: 0.6161\n",
      "Epoch [3], Iter [91/101] Loss: 0.6167\n",
      "Epoch [4], Iter [91/101] Loss: 0.6105\n",
      "Epoch [5], Iter [91/101] Loss: 0.5975\n",
      "Epoch [6], Iter [91/101] Loss: 0.6307\n",
      "Epoch [7], Iter [91/101] Loss: 0.6160\n",
      "Epoch [8], Iter [91/101] Loss: 0.5939\n",
      "Epoch [9], Iter [91/101] Loss: 0.6019\n",
      "Epoch [10], Iter [91/101] Loss: 0.5671\n",
      "Epoch [11], Iter [91/101] Loss: 0.6136\n",
      "Epoch [12], Iter [91/101] Loss: 0.5667\n",
      "Epoch [13], Iter [91/101] Loss: 0.5737\n",
      "Epoch [14], Iter [91/101] Loss: 0.5710\n",
      "Epoch [15], Iter [91/101] Loss: 0.5727\n",
      "Epoch [16], Iter [91/101] Loss: 0.5610\n",
      "Epoch [17], Iter [91/101] Loss: 0.5711\n",
      "Epoch [18], Iter [91/101] Loss: 0.5441\n",
      "Epoch [19], Iter [91/101] Loss: 0.5836\n",
      "Epoch [20], Iter [91/101] Loss: 0.5540\n",
      "Epoch [21], Iter [91/101] Loss: 0.5340\n",
      "Epoch [22], Iter [91/101] Loss: 0.5231\n",
      "Epoch [23], Iter [91/101] Loss: 0.5333\n",
      "Epoch [24], Iter [91/101] Loss: 0.5384\n",
      "Epoch [25], Iter [91/101] Loss: 0.5345\n",
      "Epoch [26], Iter [91/101] Loss: 0.5254\n",
      "Epoch [27], Iter [91/101] Loss: 0.4812\n",
      "Epoch [28], Iter [91/101] Loss: 0.4898\n",
      "Test MSE: 0.6137579679489136\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7445\n",
      "Epoch [2], Iter [91/101] Loss: 0.6781\n",
      "Epoch [3], Iter [91/101] Loss: 0.7601\n",
      "Epoch [4], Iter [91/101] Loss: 0.6143\n",
      "Epoch [5], Iter [91/101] Loss: 0.6393\n",
      "Epoch [6], Iter [91/101] Loss: 0.6288\n",
      "Epoch [7], Iter [91/101] Loss: 0.5908\n",
      "Epoch [8], Iter [91/101] Loss: 0.5843\n",
      "Epoch [9], Iter [91/101] Loss: 0.5830\n",
      "Epoch [10], Iter [91/101] Loss: 0.5895\n",
      "Epoch [11], Iter [91/101] Loss: 0.5876\n",
      "Epoch [12], Iter [91/101] Loss: 0.5881\n",
      "Epoch [13], Iter [91/101] Loss: 0.5807\n",
      "Epoch [14], Iter [91/101] Loss: 0.5698\n",
      "Epoch [15], Iter [91/101] Loss: 0.5617\n",
      "Epoch [16], Iter [91/101] Loss: 0.5668\n",
      "Epoch [17], Iter [91/101] Loss: 0.5970\n",
      "Epoch [18], Iter [91/101] Loss: 0.5582\n",
      "Epoch [19], Iter [91/101] Loss: 0.5440\n",
      "Epoch [20], Iter [91/101] Loss: 0.5490\n",
      "Epoch [21], Iter [91/101] Loss: 0.5391\n",
      "Epoch [22], Iter [91/101] Loss: 0.5299\n",
      "Epoch [23], Iter [91/101] Loss: 0.6377\n",
      "Epoch [24], Iter [91/101] Loss: 0.5104\n",
      "Epoch [25], Iter [91/101] Loss: 0.5586\n",
      "Epoch [26], Iter [91/101] Loss: 0.5127\n",
      "Epoch [27], Iter [91/101] Loss: 0.5240\n",
      "Epoch [28], Iter [91/101] Loss: 0.5437\n",
      "Epoch [29], Iter [91/101] Loss: 0.5097\n",
      "Epoch [30], Iter [91/101] Loss: 0.5274\n",
      "Epoch [31], Iter [91/101] Loss: 0.5059\n",
      "Epoch [32], Iter [91/101] Loss: 0.4967\n",
      "Epoch [33], Iter [91/101] Loss: 0.4932\n",
      "Epoch [34], Iter [91/101] Loss: 0.4889\n",
      "Epoch [35], Iter [91/101] Loss: 0.5063\n",
      "Epoch [36], Iter [91/101] Loss: 0.4840\n",
      "Test MSE: 0.6099871397018433\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6640\n",
      "Epoch [2], Iter [91/101] Loss: 0.6184\n",
      "Epoch [3], Iter [91/101] Loss: 0.6933\n",
      "Epoch [4], Iter [91/101] Loss: 0.6377\n",
      "Epoch [5], Iter [91/101] Loss: 0.6433\n",
      "Epoch [6], Iter [91/101] Loss: 0.5966\n",
      "Epoch [7], Iter [91/101] Loss: 0.5884\n",
      "Epoch [8], Iter [91/101] Loss: 0.6062\n",
      "Epoch [9], Iter [91/101] Loss: 0.5859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Iter [91/101] Loss: 0.5856\n",
      "Epoch [11], Iter [91/101] Loss: 0.5761\n",
      "Epoch [12], Iter [91/101] Loss: 0.5426\n",
      "Epoch [13], Iter [91/101] Loss: 0.5681\n",
      "Epoch [14], Iter [91/101] Loss: 0.5636\n",
      "Epoch [15], Iter [91/101] Loss: 0.6284\n",
      "Epoch [16], Iter [91/101] Loss: 0.5334\n",
      "Epoch [17], Iter [91/101] Loss: 0.5878\n",
      "Epoch [18], Iter [91/101] Loss: 0.5501\n",
      "Epoch [19], Iter [91/101] Loss: 0.5624\n",
      "Epoch [20], Iter [91/101] Loss: 0.5305\n",
      "Epoch [21], Iter [91/101] Loss: 0.5276\n",
      "Epoch [22], Iter [91/101] Loss: 0.5400\n",
      "Epoch [23], Iter [91/101] Loss: 0.5125\n",
      "Epoch [24], Iter [91/101] Loss: 0.5373\n",
      "Epoch [25], Iter [91/101] Loss: 0.5209\n",
      "Test MSE: 0.6340331435203552\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6669\n",
      "Epoch [2], Iter [91/101] Loss: 0.6829\n",
      "Epoch [3], Iter [91/101] Loss: 0.6533\n",
      "Epoch [4], Iter [91/101] Loss: 0.6295\n",
      "Epoch [5], Iter [91/101] Loss: 0.6366\n",
      "Epoch [6], Iter [91/101] Loss: 0.6149\n",
      "Epoch [7], Iter [91/101] Loss: 0.6122\n",
      "Epoch [8], Iter [91/101] Loss: 0.6126\n",
      "Epoch [9], Iter [91/101] Loss: 0.5829\n",
      "Epoch [10], Iter [91/101] Loss: 0.6309\n",
      "Epoch [11], Iter [91/101] Loss: 0.5791\n",
      "Epoch [12], Iter [91/101] Loss: 0.5481\n",
      "Epoch [13], Iter [91/101] Loss: 0.5900\n",
      "Epoch [14], Iter [91/101] Loss: 0.5725\n",
      "Epoch [15], Iter [91/101] Loss: 0.5846\n",
      "Epoch [16], Iter [91/101] Loss: 0.5488\n",
      "Epoch [17], Iter [91/101] Loss: 0.5523\n",
      "Epoch [18], Iter [91/101] Loss: 0.5316\n",
      "Epoch [19], Iter [91/101] Loss: 0.5524\n",
      "Epoch [20], Iter [91/101] Loss: 0.5558\n",
      "Epoch [21], Iter [91/101] Loss: 0.5402\n",
      "Epoch [22], Iter [91/101] Loss: 0.6250\n",
      "Epoch [23], Iter [91/101] Loss: 0.5345\n",
      "Epoch [24], Iter [91/101] Loss: 0.5486\n",
      "Epoch [25], Iter [91/101] Loss: 0.5186\n",
      "Epoch [26], Iter [91/101] Loss: 0.5295\n",
      "Epoch [27], Iter [91/101] Loss: 0.5192\n",
      "Epoch [28], Iter [91/101] Loss: 0.5104\n",
      "Epoch [29], Iter [91/101] Loss: 0.5147\n",
      "Epoch [30], Iter [91/101] Loss: 0.4905\n",
      "Epoch [31], Iter [91/101] Loss: 0.5281\n",
      "Epoch [32], Iter [91/101] Loss: 0.5485\n",
      "Epoch [33], Iter [91/101] Loss: 0.5054\n",
      "Epoch [34], Iter [91/101] Loss: 0.4900\n",
      "Epoch [35], Iter [91/101] Loss: 0.5085\n",
      "Epoch [36], Iter [91/101] Loss: 0.5105\n",
      "Epoch [37], Iter [91/101] Loss: 0.4775\n",
      "Epoch [38], Iter [91/101] Loss: 0.4815\n",
      "Test MSE: 0.6153922080993652\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6903\n",
      "Epoch [2], Iter [91/101] Loss: 0.6842\n",
      "Epoch [3], Iter [91/101] Loss: 0.7146\n",
      "Epoch [4], Iter [91/101] Loss: 0.6343\n",
      "Epoch [5], Iter [91/101] Loss: 0.6284\n",
      "Epoch [6], Iter [91/101] Loss: 0.5939\n",
      "Epoch [7], Iter [91/101] Loss: 0.5980\n",
      "Epoch [8], Iter [91/101] Loss: 0.5802\n",
      "Epoch [9], Iter [91/101] Loss: 0.5787\n",
      "Epoch [10], Iter [91/101] Loss: 0.5805\n",
      "Epoch [11], Iter [91/101] Loss: 0.5477\n",
      "Epoch [12], Iter [91/101] Loss: 0.5841\n",
      "Epoch [13], Iter [91/101] Loss: 0.5579\n",
      "Epoch [14], Iter [91/101] Loss: 0.6421\n",
      "Epoch [15], Iter [91/101] Loss: 0.5438\n",
      "Epoch [16], Iter [91/101] Loss: 0.5596\n",
      "Epoch [17], Iter [91/101] Loss: 0.5678\n",
      "Epoch [18], Iter [91/101] Loss: 0.5440\n",
      "Epoch [19], Iter [91/101] Loss: 0.5283\n",
      "Epoch [20], Iter [91/101] Loss: 0.5766\n",
      "Epoch [21], Iter [91/101] Loss: 0.5502\n",
      "Epoch [22], Iter [91/101] Loss: 0.5443\n",
      "Epoch [23], Iter [91/101] Loss: 0.5243\n",
      "Epoch [24], Iter [91/101] Loss: 0.5473\n",
      "Epoch [25], Iter [91/101] Loss: 0.5309\n",
      "Epoch [26], Iter [91/101] Loss: 0.5166\n",
      "Epoch [27], Iter [91/101] Loss: 0.5214\n",
      "Test MSE: 0.6161223649978638\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6776\n",
      "Epoch [2], Iter [91/101] Loss: 0.6972\n",
      "Epoch [3], Iter [91/101] Loss: 0.6524\n",
      "Epoch [4], Iter [91/101] Loss: 0.6507\n",
      "Epoch [5], Iter [91/101] Loss: 0.6529\n",
      "Epoch [6], Iter [91/101] Loss: 0.5858\n",
      "Epoch [7], Iter [91/101] Loss: 0.6233\n",
      "Epoch [8], Iter [91/101] Loss: 0.6215\n",
      "Epoch [9], Iter [91/101] Loss: 0.6954\n",
      "Epoch [10], Iter [91/101] Loss: 0.6042\n",
      "Epoch [11], Iter [91/101] Loss: 0.6102\n",
      "Epoch [12], Iter [91/101] Loss: 0.5521\n",
      "Epoch [13], Iter [91/101] Loss: 0.5870\n",
      "Epoch [14], Iter [91/101] Loss: 0.5580\n",
      "Epoch [15], Iter [91/101] Loss: 0.5629\n",
      "Epoch [16], Iter [91/101] Loss: 0.5280\n",
      "Epoch [17], Iter [91/101] Loss: 0.5507\n",
      "Epoch [18], Iter [91/101] Loss: 0.6066\n",
      "Epoch [19], Iter [91/101] Loss: 0.5577\n",
      "Epoch [20], Iter [91/101] Loss: 0.5483\n",
      "Epoch [21], Iter [91/101] Loss: 0.5097\n",
      "Epoch [22], Iter [91/101] Loss: 0.5604\n",
      "Epoch [23], Iter [91/101] Loss: 0.5630\n",
      "Epoch [24], Iter [91/101] Loss: 0.5452\n",
      "Epoch [25], Iter [91/101] Loss: 0.5184\n",
      "Epoch [26], Iter [91/101] Loss: 0.5183\n",
      "Epoch [27], Iter [91/101] Loss: 0.5072\n",
      "Epoch [28], Iter [91/101] Loss: 0.5072\n",
      "Epoch [29], Iter [91/101] Loss: 0.5761\n",
      "Epoch [30], Iter [91/101] Loss: 0.4814\n",
      "Epoch [31], Iter [91/101] Loss: 0.5347\n",
      "Test MSE: 0.6254814863204956\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6961\n",
      "Epoch [2], Iter [91/101] Loss: 0.6862\n",
      "Epoch [3], Iter [91/101] Loss: 0.6212\n",
      "Epoch [4], Iter [91/101] Loss: 0.6497\n",
      "Epoch [5], Iter [91/101] Loss: 0.6408\n",
      "Epoch [6], Iter [91/101] Loss: 0.6158\n",
      "Epoch [7], Iter [91/101] Loss: 0.5989\n",
      "Epoch [8], Iter [91/101] Loss: 0.6166\n",
      "Epoch [9], Iter [91/101] Loss: 0.6019\n",
      "Epoch [10], Iter [91/101] Loss: 0.5669\n",
      "Epoch [11], Iter [91/101] Loss: 0.5851\n",
      "Epoch [12], Iter [91/101] Loss: 0.5789\n",
      "Epoch [13], Iter [91/101] Loss: 0.5617\n",
      "Epoch [14], Iter [91/101] Loss: 0.5588\n",
      "Epoch [15], Iter [91/101] Loss: 0.5257\n",
      "Epoch [16], Iter [91/101] Loss: 0.5661\n",
      "Epoch [17], Iter [91/101] Loss: 0.5602\n",
      "Epoch [18], Iter [91/101] Loss: 0.5387\n",
      "Epoch [19], Iter [91/101] Loss: 0.5466\n",
      "Epoch [20], Iter [91/101] Loss: 0.5224\n",
      "Epoch [21], Iter [91/101] Loss: 0.5780\n",
      "Epoch [22], Iter [91/101] Loss: 0.5251\n",
      "Epoch [23], Iter [91/101] Loss: 0.5049\n",
      "Epoch [24], Iter [91/101] Loss: 0.5342\n",
      "Epoch [25], Iter [91/101] Loss: 0.5258\n",
      "Epoch [26], Iter [91/101] Loss: 0.5621\n",
      "Epoch [27], Iter [91/101] Loss: 0.5213\n",
      "Epoch [28], Iter [91/101] Loss: 0.5326\n",
      "Epoch [29], Iter [91/101] Loss: 0.5103\n",
      "Epoch [30], Iter [91/101] Loss: 0.5055\n",
      "Epoch [31], Iter [91/101] Loss: 0.5126\n",
      "Epoch [32], Iter [91/101] Loss: 0.5092\n",
      "Test MSE: 0.611980140209198\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7172\n",
      "Epoch [2], Iter [91/101] Loss: 0.6717\n",
      "Epoch [3], Iter [91/101] Loss: 0.6425\n",
      "Epoch [4], Iter [91/101] Loss: 0.6282\n",
      "Epoch [5], Iter [91/101] Loss: 0.6370\n",
      "Epoch [6], Iter [91/101] Loss: 0.6576\n",
      "Epoch [7], Iter [91/101] Loss: 0.6581\n",
      "Epoch [8], Iter [91/101] Loss: 0.6321\n",
      "Epoch [9], Iter [91/101] Loss: 0.6326\n",
      "Epoch [10], Iter [91/101] Loss: 0.6184\n",
      "Epoch [11], Iter [91/101] Loss: 0.5899\n",
      "Epoch [12], Iter [91/101] Loss: 0.7516\n",
      "Epoch [13], Iter [91/101] Loss: 0.5773\n",
      "Epoch [14], Iter [91/101] Loss: 0.5716\n",
      "Epoch [15], Iter [91/101] Loss: 0.5720\n",
      "Epoch [16], Iter [91/101] Loss: 0.5773\n",
      "Epoch [17], Iter [91/101] Loss: 0.6453\n",
      "Epoch [18], Iter [91/101] Loss: 0.5583\n",
      "Epoch [19], Iter [91/101] Loss: 0.6481\n",
      "Epoch [20], Iter [91/101] Loss: 0.5689\n",
      "Epoch [21], Iter [91/101] Loss: 0.5777\n",
      "Epoch [22], Iter [91/101] Loss: 0.5559\n",
      "Epoch [23], Iter [91/101] Loss: 0.5607\n",
      "Epoch [24], Iter [91/101] Loss: 0.5367\n",
      "Epoch [25], Iter [91/101] Loss: 0.5309\n",
      "Epoch [26], Iter [91/101] Loss: 0.5829\n",
      "Epoch [27], Iter [91/101] Loss: 0.5272\n",
      "Epoch [28], Iter [91/101] Loss: 0.5185\n",
      "Epoch [29], Iter [91/101] Loss: 0.5060\n",
      "Epoch [30], Iter [91/101] Loss: 0.5310\n",
      "Epoch [31], Iter [91/101] Loss: 0.5161\n",
      "Epoch [32], Iter [91/101] Loss: 0.5157\n",
      "Epoch [33], Iter [91/101] Loss: 0.5127\n",
      "Epoch [34], Iter [91/101] Loss: 0.4901\n",
      "Epoch [35], Iter [91/101] Loss: 0.5652\n",
      "Epoch [36], Iter [91/101] Loss: 0.4990\n",
      "Epoch [37], Iter [91/101] Loss: 0.5103\n",
      "Epoch [38], Iter [91/101] Loss: 0.4613\n",
      "Test MSE: 0.6141096353530884\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6899\n",
      "Epoch [2], Iter [91/101] Loss: 0.9782\n",
      "Epoch [3], Iter [91/101] Loss: 0.8118\n",
      "Epoch [4], Iter [91/101] Loss: 0.6533\n",
      "Epoch [5], Iter [91/101] Loss: 0.6925\n",
      "Epoch [6], Iter [91/101] Loss: 0.6453\n",
      "Epoch [7], Iter [91/101] Loss: 0.5970\n",
      "Epoch [8], Iter [91/101] Loss: 0.6073\n",
      "Epoch [9], Iter [91/101] Loss: 0.5990\n",
      "Epoch [10], Iter [91/101] Loss: 0.7633\n",
      "Epoch [11], Iter [91/101] Loss: 0.5978\n",
      "Epoch [12], Iter [91/101] Loss: 0.5959\n",
      "Epoch [13], Iter [91/101] Loss: 0.6708\n",
      "Epoch [14], Iter [91/101] Loss: 0.5899\n",
      "Epoch [15], Iter [91/101] Loss: 0.5627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Iter [91/101] Loss: 0.5750\n",
      "Epoch [17], Iter [91/101] Loss: 0.5626\n",
      "Epoch [18], Iter [91/101] Loss: 0.5591\n",
      "Epoch [19], Iter [91/101] Loss: 0.6187\n",
      "Epoch [20], Iter [91/101] Loss: 0.6329\n",
      "Epoch [21], Iter [91/101] Loss: 0.5542\n",
      "Epoch [22], Iter [91/101] Loss: 0.5485\n",
      "Epoch [23], Iter [91/101] Loss: 0.5367\n",
      "Epoch [24], Iter [91/101] Loss: 0.5125\n",
      "Epoch [25], Iter [91/101] Loss: 0.5776\n",
      "Epoch [26], Iter [91/101] Loss: 0.5978\n",
      "Epoch [27], Iter [91/101] Loss: 0.5133\n",
      "Epoch [28], Iter [91/101] Loss: 0.5263\n",
      "Epoch [29], Iter [91/101] Loss: 0.5237\n",
      "Epoch [30], Iter [91/101] Loss: 0.5125\n",
      "Epoch [31], Iter [91/101] Loss: 0.5729\n",
      "Epoch [32], Iter [91/101] Loss: 0.5204\n",
      "Epoch [33], Iter [91/101] Loss: 0.4984\n",
      "Epoch [34], Iter [91/101] Loss: 0.5346\n",
      "Epoch [35], Iter [91/101] Loss: 0.5092\n",
      "Test MSE: 0.6162530779838562\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7234\n",
      "Epoch [2], Iter [91/101] Loss: 0.7548\n",
      "Epoch [3], Iter [91/101] Loss: 0.6755\n",
      "Epoch [4], Iter [91/101] Loss: 0.7298\n",
      "Epoch [5], Iter [91/101] Loss: 0.6339\n",
      "Epoch [6], Iter [91/101] Loss: 0.6235\n",
      "Epoch [7], Iter [91/101] Loss: 0.6950\n",
      "Epoch [8], Iter [91/101] Loss: 0.6072\n",
      "Epoch [9], Iter [91/101] Loss: 0.5794\n",
      "Epoch [10], Iter [91/101] Loss: 0.6576\n",
      "Epoch [11], Iter [91/101] Loss: 0.5930\n",
      "Epoch [12], Iter [91/101] Loss: 0.5860\n",
      "Epoch [13], Iter [91/101] Loss: 0.5634\n",
      "Epoch [14], Iter [91/101] Loss: 0.5879\n",
      "Epoch [15], Iter [91/101] Loss: 0.5503\n",
      "Epoch [16], Iter [91/101] Loss: 0.5440\n",
      "Epoch [17], Iter [91/101] Loss: 0.5573\n",
      "Epoch [18], Iter [91/101] Loss: 0.5948\n",
      "Epoch [19], Iter [91/101] Loss: 0.5571\n",
      "Epoch [20], Iter [91/101] Loss: 0.5697\n",
      "Epoch [21], Iter [91/101] Loss: 0.5402\n",
      "Epoch [22], Iter [91/101] Loss: 0.5306\n",
      "Epoch [23], Iter [91/101] Loss: 0.5396\n",
      "Epoch [24], Iter [91/101] Loss: 0.5307\n",
      "Epoch [25], Iter [91/101] Loss: 0.5170\n",
      "Epoch [26], Iter [91/101] Loss: 0.5223\n",
      "Epoch [27], Iter [91/101] Loss: 0.5344\n",
      "Epoch [28], Iter [91/101] Loss: 0.5336\n",
      "Test MSE: 0.624931275844574\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7261\n",
      "Epoch [2], Iter [91/101] Loss: 0.6692\n",
      "Epoch [3], Iter [91/101] Loss: 0.6601\n",
      "Epoch [4], Iter [91/101] Loss: 0.6259\n",
      "Epoch [5], Iter [91/101] Loss: 0.6539\n",
      "Epoch [6], Iter [91/101] Loss: 0.6134\n",
      "Epoch [7], Iter [91/101] Loss: 0.6076\n",
      "Epoch [8], Iter [91/101] Loss: 0.6500\n",
      "Epoch [9], Iter [91/101] Loss: 0.6045\n",
      "Epoch [10], Iter [91/101] Loss: 0.6075\n",
      "Epoch [11], Iter [91/101] Loss: 0.5802\n",
      "Epoch [12], Iter [91/101] Loss: 0.6007\n",
      "Epoch [13], Iter [91/101] Loss: 0.6571\n",
      "Epoch [14], Iter [91/101] Loss: 0.5736\n",
      "Epoch [15], Iter [91/101] Loss: 0.6112\n",
      "Epoch [16], Iter [91/101] Loss: 0.5583\n",
      "Epoch [17], Iter [91/101] Loss: 0.5648\n",
      "Epoch [18], Iter [91/101] Loss: 0.5698\n",
      "Epoch [19], Iter [91/101] Loss: 0.5898\n",
      "Epoch [20], Iter [91/101] Loss: 0.5546\n",
      "Epoch [21], Iter [91/101] Loss: 0.5352\n",
      "Epoch [22], Iter [91/101] Loss: 0.5328\n",
      "Epoch [23], Iter [91/101] Loss: 0.5642\n",
      "Epoch [24], Iter [91/101] Loss: 0.5292\n",
      "Epoch [25], Iter [91/101] Loss: 0.5272\n",
      "Epoch [26], Iter [91/101] Loss: 0.4969\n",
      "Epoch [27], Iter [91/101] Loss: 0.5212\n",
      "Epoch [28], Iter [91/101] Loss: 0.5270\n",
      "Epoch [29], Iter [91/101] Loss: 0.5762\n",
      "Epoch [30], Iter [91/101] Loss: 0.5056\n",
      "Epoch [31], Iter [91/101] Loss: 0.5151\n",
      "Test MSE: 0.6291699409484863\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7244\n",
      "Epoch [2], Iter [91/101] Loss: 0.6279\n",
      "Epoch [3], Iter [91/101] Loss: 0.6575\n",
      "Epoch [4], Iter [91/101] Loss: 0.6190\n",
      "Epoch [5], Iter [91/101] Loss: 0.6357\n",
      "Epoch [6], Iter [91/101] Loss: 0.6263\n",
      "Epoch [7], Iter [91/101] Loss: 0.6093\n",
      "Epoch [8], Iter [91/101] Loss: 0.5850\n",
      "Epoch [9], Iter [91/101] Loss: 0.5811\n",
      "Epoch [10], Iter [91/101] Loss: 0.5914\n",
      "Epoch [11], Iter [91/101] Loss: 0.5922\n",
      "Epoch [12], Iter [91/101] Loss: 0.5929\n",
      "Epoch [13], Iter [91/101] Loss: 0.5960\n",
      "Epoch [14], Iter [91/101] Loss: 0.6048\n",
      "Epoch [15], Iter [91/101] Loss: 0.5569\n",
      "Epoch [16], Iter [91/101] Loss: 0.5565\n",
      "Epoch [17], Iter [91/101] Loss: 0.5785\n",
      "Epoch [18], Iter [91/101] Loss: 0.5877\n",
      "Epoch [19], Iter [91/101] Loss: 0.5440\n",
      "Epoch [20], Iter [91/101] Loss: 0.5492\n",
      "Epoch [21], Iter [91/101] Loss: 0.5467\n",
      "Epoch [22], Iter [91/101] Loss: 0.5280\n",
      "Epoch [23], Iter [91/101] Loss: 0.5307\n",
      "Epoch [24], Iter [91/101] Loss: 0.5420\n",
      "Epoch [25], Iter [91/101] Loss: 0.6250\n",
      "Epoch [26], Iter [91/101] Loss: 0.5256\n",
      "Test MSE: 0.6326077580451965\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7075\n",
      "Epoch [2], Iter [91/101] Loss: 0.6988\n",
      "Epoch [3], Iter [91/101] Loss: 0.6595\n",
      "Epoch [4], Iter [91/101] Loss: 0.6495\n",
      "Epoch [5], Iter [91/101] Loss: 0.5869\n",
      "Epoch [6], Iter [91/101] Loss: 0.7188\n",
      "Epoch [7], Iter [91/101] Loss: 0.5930\n",
      "Epoch [8], Iter [91/101] Loss: 0.5936\n",
      "Epoch [9], Iter [91/101] Loss: 0.6078\n",
      "Epoch [10], Iter [91/101] Loss: 0.5857\n",
      "Epoch [11], Iter [91/101] Loss: 0.5949\n",
      "Epoch [12], Iter [91/101] Loss: 0.6205\n",
      "Epoch [13], Iter [91/101] Loss: 0.5569\n",
      "Epoch [14], Iter [91/101] Loss: 0.5754\n",
      "Epoch [15], Iter [91/101] Loss: 0.5782\n",
      "Epoch [16], Iter [91/101] Loss: 0.5766\n",
      "Epoch [17], Iter [91/101] Loss: 0.5712\n",
      "Epoch [18], Iter [91/101] Loss: 0.5436\n",
      "Epoch [19], Iter [91/101] Loss: 0.5405\n",
      "Epoch [20], Iter [91/101] Loss: 0.5399\n",
      "Epoch [21], Iter [91/101] Loss: 0.5179\n",
      "Epoch [22], Iter [91/101] Loss: 0.5541\n",
      "Epoch [23], Iter [91/101] Loss: 0.6288\n",
      "Epoch [24], Iter [91/101] Loss: 0.5263\n",
      "Epoch [25], Iter [91/101] Loss: 0.5289\n",
      "Epoch [26], Iter [91/101] Loss: 1.1872\n",
      "Epoch [27], Iter [91/101] Loss: 0.5989\n",
      "Epoch [28], Iter [91/101] Loss: 0.6075\n",
      "Epoch [29], Iter [91/101] Loss: 0.5133\n",
      "Epoch [30], Iter [91/101] Loss: 0.5102\n",
      "Epoch [31], Iter [91/101] Loss: 0.5080\n",
      "Epoch [32], Iter [91/101] Loss: 0.4946\n",
      "Epoch [33], Iter [91/101] Loss: 0.5190\n",
      "Test MSE: 0.6319115161895752\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6999\n",
      "Epoch [2], Iter [91/101] Loss: 0.7079\n",
      "Epoch [3], Iter [91/101] Loss: 0.6965\n",
      "Epoch [4], Iter [91/101] Loss: 0.6587\n",
      "Epoch [5], Iter [91/101] Loss: 1.4319\n",
      "Epoch [6], Iter [91/101] Loss: 0.6684\n",
      "Epoch [7], Iter [91/101] Loss: 0.6103\n",
      "Epoch [8], Iter [91/101] Loss: 0.5853\n",
      "Epoch [9], Iter [91/101] Loss: 0.6254\n",
      "Epoch [10], Iter [91/101] Loss: 0.5911\n",
      "Epoch [11], Iter [91/101] Loss: 0.5816\n",
      "Epoch [12], Iter [91/101] Loss: 0.5827\n",
      "Epoch [13], Iter [91/101] Loss: 0.5652\n",
      "Epoch [14], Iter [91/101] Loss: 0.5702\n",
      "Epoch [15], Iter [91/101] Loss: 0.5839\n",
      "Epoch [16], Iter [91/101] Loss: 0.6399\n",
      "Epoch [17], Iter [91/101] Loss: 0.5493\n",
      "Epoch [18], Iter [91/101] Loss: 0.5472\n",
      "Epoch [19], Iter [91/101] Loss: 0.5718\n",
      "Epoch [20], Iter [91/101] Loss: 0.5482\n",
      "Epoch [21], Iter [91/101] Loss: 0.7392\n",
      "Epoch [22], Iter [91/101] Loss: 0.5622\n",
      "Epoch [23], Iter [91/101] Loss: 0.5255\n",
      "Epoch [24], Iter [91/101] Loss: 0.5442\n",
      "Epoch [25], Iter [91/101] Loss: 0.5168\n",
      "Epoch [26], Iter [91/101] Loss: 0.5102\n",
      "Epoch [27], Iter [91/101] Loss: 0.5274\n",
      "Epoch [28], Iter [91/101] Loss: 0.5247\n",
      "Test MSE: 0.6231082081794739\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7659\n",
      "Epoch [2], Iter [91/101] Loss: 0.6989\n",
      "Epoch [3], Iter [91/101] Loss: 0.6619\n",
      "Epoch [4], Iter [91/101] Loss: 0.6428\n",
      "Epoch [5], Iter [91/101] Loss: 0.6179\n",
      "Epoch [6], Iter [91/101] Loss: 0.6576\n",
      "Epoch [7], Iter [91/101] Loss: 0.5940\n",
      "Epoch [8], Iter [91/101] Loss: 0.6426\n",
      "Epoch [9], Iter [91/101] Loss: 0.7028\n",
      "Epoch [10], Iter [91/101] Loss: 0.6774\n",
      "Epoch [11], Iter [91/101] Loss: 0.5838\n",
      "Epoch [12], Iter [91/101] Loss: 0.6070\n",
      "Epoch [13], Iter [91/101] Loss: 0.5637\n",
      "Epoch [14], Iter [91/101] Loss: 0.5356\n",
      "Epoch [15], Iter [91/101] Loss: 0.5548\n",
      "Epoch [16], Iter [91/101] Loss: 0.5628\n",
      "Epoch [17], Iter [91/101] Loss: 0.5664\n",
      "Epoch [18], Iter [91/101] Loss: 0.5492\n",
      "Epoch [19], Iter [91/101] Loss: 0.5595\n",
      "Epoch [20], Iter [91/101] Loss: 0.5527\n",
      "Epoch [21], Iter [91/101] Loss: 0.5324\n",
      "Epoch [22], Iter [91/101] Loss: 0.5464\n",
      "Epoch [23], Iter [91/101] Loss: 0.5493\n",
      "Epoch [24], Iter [91/101] Loss: 0.5429\n",
      "Epoch [25], Iter [91/101] Loss: 0.5497\n",
      "Epoch [26], Iter [91/101] Loss: 0.5263\n",
      "Epoch [27], Iter [91/101] Loss: 0.5888\n",
      "Test MSE: 0.6294766664505005\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Iter [91/101] Loss: 0.6548\n",
      "Epoch [3], Iter [91/101] Loss: 0.6428\n",
      "Epoch [4], Iter [91/101] Loss: 0.6573\n",
      "Epoch [5], Iter [91/101] Loss: 0.6206\n",
      "Epoch [6], Iter [91/101] Loss: 0.6569\n",
      "Epoch [7], Iter [91/101] Loss: 0.6220\n",
      "Epoch [8], Iter [91/101] Loss: 0.5814\n",
      "Epoch [9], Iter [91/101] Loss: 0.6088\n",
      "Epoch [10], Iter [91/101] Loss: 0.6313\n",
      "Epoch [11], Iter [91/101] Loss: 0.5764\n",
      "Epoch [12], Iter [91/101] Loss: 0.7040\n",
      "Epoch [13], Iter [91/101] Loss: 0.5829\n",
      "Epoch [14], Iter [91/101] Loss: 0.5716\n",
      "Epoch [15], Iter [91/101] Loss: 0.5788\n",
      "Epoch [16], Iter [91/101] Loss: 0.5605\n",
      "Epoch [17], Iter [91/101] Loss: 0.5496\n",
      "Epoch [18], Iter [91/101] Loss: 0.5526\n",
      "Epoch [19], Iter [91/101] Loss: 0.5893\n",
      "Epoch [20], Iter [91/101] Loss: 0.5332\n",
      "Epoch [21], Iter [91/101] Loss: 0.5614\n",
      "Epoch [22], Iter [91/101] Loss: 0.5701\n",
      "Epoch [23], Iter [91/101] Loss: 0.5169\n",
      "Epoch [24], Iter [91/101] Loss: 0.5147\n",
      "Epoch [25], Iter [91/101] Loss: 0.5505\n",
      "Epoch [26], Iter [91/101] Loss: 0.5340\n",
      "Epoch [27], Iter [91/101] Loss: 0.5222\n",
      "Epoch [28], Iter [91/101] Loss: 0.5257\n",
      "Test MSE: 0.6317960619926453\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7611\n",
      "Epoch [2], Iter [91/101] Loss: 0.6809\n",
      "Epoch [3], Iter [91/101] Loss: 0.6677\n",
      "Epoch [4], Iter [91/101] Loss: 0.6694\n",
      "Epoch [5], Iter [91/101] Loss: 0.6479\n",
      "Epoch [6], Iter [91/101] Loss: 0.6677\n",
      "Epoch [7], Iter [91/101] Loss: 0.6087\n",
      "Epoch [8], Iter [91/101] Loss: 0.6728\n",
      "Epoch [9], Iter [91/101] Loss: 0.6199\n",
      "Epoch [10], Iter [91/101] Loss: 0.5943\n",
      "Epoch [11], Iter [91/101] Loss: 0.6014\n",
      "Epoch [12], Iter [91/101] Loss: 0.5849\n",
      "Epoch [13], Iter [91/101] Loss: 0.5874\n",
      "Epoch [14], Iter [91/101] Loss: 0.5913\n",
      "Epoch [15], Iter [91/101] Loss: 0.5845\n",
      "Epoch [16], Iter [91/101] Loss: 0.5853\n",
      "Epoch [17], Iter [91/101] Loss: 0.5620\n",
      "Epoch [18], Iter [91/101] Loss: 0.5954\n",
      "Epoch [19], Iter [91/101] Loss: 0.5632\n",
      "Epoch [20], Iter [91/101] Loss: 0.5512\n",
      "Epoch [21], Iter [91/101] Loss: 0.5753\n",
      "Epoch [22], Iter [91/101] Loss: 0.5519\n",
      "Epoch [23], Iter [91/101] Loss: 0.5686\n",
      "Epoch [24], Iter [91/101] Loss: 0.5674\n",
      "Epoch [25], Iter [91/101] Loss: 0.5367\n",
      "Epoch [26], Iter [91/101] Loss: 0.5340\n",
      "Epoch [27], Iter [91/101] Loss: 0.5467\n",
      "Epoch [28], Iter [91/101] Loss: 0.5283\n",
      "Epoch [29], Iter [91/101] Loss: 0.5190\n",
      "Epoch [30], Iter [91/101] Loss: 0.5083\n",
      "Test MSE: 0.6228426694869995\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6970\n",
      "Epoch [2], Iter [91/101] Loss: 0.7101\n",
      "Epoch [3], Iter [91/101] Loss: 0.6656\n",
      "Epoch [4], Iter [91/101] Loss: 0.6327\n",
      "Epoch [5], Iter [91/101] Loss: 0.6680\n",
      "Epoch [6], Iter [91/101] Loss: 0.6048\n",
      "Epoch [7], Iter [91/101] Loss: 0.6315\n",
      "Epoch [8], Iter [91/101] Loss: 0.6060\n",
      "Epoch [9], Iter [91/101] Loss: 0.5831\n",
      "Epoch [10], Iter [91/101] Loss: 0.5890\n",
      "Epoch [11], Iter [91/101] Loss: 0.5766\n",
      "Epoch [12], Iter [91/101] Loss: 0.6006\n",
      "Epoch [13], Iter [91/101] Loss: 0.5750\n",
      "Epoch [14], Iter [91/101] Loss: 0.5713\n",
      "Epoch [15], Iter [91/101] Loss: 0.5615\n",
      "Epoch [16], Iter [91/101] Loss: 0.5866\n",
      "Epoch [17], Iter [91/101] Loss: 0.5870\n",
      "Epoch [18], Iter [91/101] Loss: 0.6089\n",
      "Epoch [19], Iter [91/101] Loss: 0.5570\n",
      "Epoch [20], Iter [91/101] Loss: 0.5626\n",
      "Epoch [21], Iter [91/101] Loss: 0.5301\n",
      "Epoch [22], Iter [91/101] Loss: 0.5695\n",
      "Epoch [23], Iter [91/101] Loss: 0.5279\n",
      "Epoch [24], Iter [91/101] Loss: 0.5497\n",
      "Epoch [25], Iter [91/101] Loss: 0.5363\n",
      "Epoch [26], Iter [91/101] Loss: 0.5219\n",
      "Epoch [27], Iter [91/101] Loss: 0.5224\n",
      "Test MSE: 0.6234716773033142\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9354\n",
      "Epoch [2], Iter [91/101] Loss: 0.6950\n",
      "Epoch [3], Iter [91/101] Loss: 0.6471\n",
      "Epoch [4], Iter [91/101] Loss: 0.6450\n",
      "Epoch [5], Iter [91/101] Loss: 0.6291\n",
      "Epoch [6], Iter [91/101] Loss: 0.6329\n",
      "Epoch [7], Iter [91/101] Loss: 0.6075\n",
      "Epoch [8], Iter [91/101] Loss: 0.7128\n",
      "Epoch [9], Iter [91/101] Loss: 0.6106\n",
      "Epoch [10], Iter [91/101] Loss: 0.6039\n",
      "Epoch [11], Iter [91/101] Loss: 0.5908\n",
      "Epoch [12], Iter [91/101] Loss: 0.6149\n",
      "Epoch [13], Iter [91/101] Loss: 0.5783\n",
      "Epoch [14], Iter [91/101] Loss: 0.5825\n",
      "Epoch [15], Iter [91/101] Loss: 0.5868\n",
      "Epoch [16], Iter [91/101] Loss: 0.5845\n",
      "Epoch [17], Iter [91/101] Loss: 0.5662\n",
      "Epoch [18], Iter [91/101] Loss: 0.5458\n",
      "Epoch [19], Iter [91/101] Loss: 0.5689\n",
      "Epoch [20], Iter [91/101] Loss: 0.5596\n",
      "Epoch [21], Iter [91/101] Loss: 0.6036\n",
      "Epoch [22], Iter [91/101] Loss: 0.5521\n",
      "Epoch [23], Iter [91/101] Loss: 0.5475\n",
      "Epoch [24], Iter [91/101] Loss: 0.5799\n",
      "Epoch [25], Iter [91/101] Loss: 0.5629\n",
      "Epoch [26], Iter [91/101] Loss: 0.5402\n",
      "Epoch [27], Iter [91/101] Loss: 0.5226\n",
      "Epoch [28], Iter [91/101] Loss: 0.5136\n",
      "Epoch [29], Iter [91/101] Loss: 0.5127\n",
      "Epoch [30], Iter [91/101] Loss: 0.5098\n",
      "Epoch [31], Iter [91/101] Loss: 0.5162\n",
      "Epoch [32], Iter [91/101] Loss: 0.5205\n",
      "Epoch [33], Iter [91/101] Loss: 0.5910\n",
      "Epoch [34], Iter [91/101] Loss: 0.5237\n",
      "Epoch [35], Iter [91/101] Loss: 0.5145\n",
      "Epoch [36], Iter [91/101] Loss: 0.4895\n",
      "Epoch [37], Iter [91/101] Loss: 0.5072\n",
      "Test MSE: 0.6277332305908203\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7122\n",
      "Epoch [2], Iter [91/101] Loss: 0.7689\n",
      "Epoch [3], Iter [91/101] Loss: 0.6552\n",
      "Epoch [4], Iter [91/101] Loss: 0.7312\n",
      "Epoch [5], Iter [91/101] Loss: 0.7009\n",
      "Epoch [6], Iter [91/101] Loss: 0.6538\n",
      "Epoch [7], Iter [91/101] Loss: 0.6146\n",
      "Epoch [8], Iter [91/101] Loss: 0.7468\n",
      "Epoch [9], Iter [91/101] Loss: 0.5990\n",
      "Epoch [10], Iter [91/101] Loss: 0.6193\n",
      "Epoch [11], Iter [91/101] Loss: 0.6010\n",
      "Epoch [12], Iter [91/101] Loss: 0.6135\n",
      "Epoch [13], Iter [91/101] Loss: 0.6222\n",
      "Epoch [14], Iter [91/101] Loss: 0.5914\n",
      "Epoch [15], Iter [91/101] Loss: 0.5856\n",
      "Epoch [16], Iter [91/101] Loss: 0.5834\n",
      "Epoch [17], Iter [91/101] Loss: 0.5888\n",
      "Epoch [18], Iter [91/101] Loss: 0.6356\n",
      "Epoch [19], Iter [91/101] Loss: 0.5601\n",
      "Epoch [20], Iter [91/101] Loss: 0.5513\n",
      "Epoch [21], Iter [91/101] Loss: 0.6269\n",
      "Epoch [22], Iter [91/101] Loss: 0.5627\n",
      "Epoch [23], Iter [91/101] Loss: 0.5553\n",
      "Epoch [24], Iter [91/101] Loss: 0.5403\n",
      "Epoch [25], Iter [91/101] Loss: 0.5391\n",
      "Epoch [26], Iter [91/101] Loss: 0.5420\n",
      "Epoch [27], Iter [91/101] Loss: 0.5621\n",
      "Epoch [28], Iter [91/101] Loss: 0.5521\n",
      "Epoch [29], Iter [91/101] Loss: 0.5409\n",
      "Epoch [30], Iter [91/101] Loss: 0.5307\n",
      "Epoch [31], Iter [91/101] Loss: 0.5592\n",
      "Epoch [32], Iter [91/101] Loss: 0.5201\n",
      "Epoch [33], Iter [91/101] Loss: 0.5873\n",
      "Test MSE: 0.6259710192680359\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7173\n",
      "Epoch [2], Iter [91/101] Loss: 0.6752\n",
      "Epoch [3], Iter [91/101] Loss: 0.6779\n",
      "Epoch [4], Iter [91/101] Loss: 0.6443\n",
      "Epoch [5], Iter [91/101] Loss: 0.7486\n",
      "Epoch [6], Iter [91/101] Loss: 0.6349\n",
      "Epoch [7], Iter [91/101] Loss: 0.6139\n",
      "Epoch [8], Iter [91/101] Loss: 0.6386\n",
      "Epoch [9], Iter [91/101] Loss: 0.6217\n",
      "Epoch [10], Iter [91/101] Loss: 0.5754\n",
      "Epoch [11], Iter [91/101] Loss: 0.7247\n",
      "Epoch [12], Iter [91/101] Loss: 0.5948\n",
      "Epoch [13], Iter [91/101] Loss: 0.5463\n",
      "Epoch [14], Iter [91/101] Loss: 0.5831\n",
      "Epoch [15], Iter [91/101] Loss: 0.6009\n",
      "Epoch [16], Iter [91/101] Loss: 0.5757\n",
      "Epoch [17], Iter [91/101] Loss: 0.5760\n",
      "Epoch [18], Iter [91/101] Loss: 0.5686\n",
      "Epoch [19], Iter [91/101] Loss: 0.5790\n",
      "Epoch [20], Iter [91/101] Loss: 0.5767\n",
      "Epoch [21], Iter [91/101] Loss: 0.5863\n",
      "Epoch [22], Iter [91/101] Loss: 0.5740\n",
      "Epoch [23], Iter [91/101] Loss: 0.5566\n",
      "Epoch [24], Iter [91/101] Loss: 0.5521\n",
      "Epoch [25], Iter [91/101] Loss: 0.5475\n",
      "Epoch [26], Iter [91/101] Loss: 0.5230\n",
      "Epoch [27], Iter [91/101] Loss: 0.5705\n",
      "Epoch [28], Iter [91/101] Loss: 0.5268\n",
      "Epoch [29], Iter [91/101] Loss: 0.5358\n",
      "Epoch [30], Iter [91/101] Loss: 0.5250\n",
      "Epoch [31], Iter [91/101] Loss: 0.5270\n",
      "Epoch [32], Iter [91/101] Loss: 0.5213\n",
      "Epoch [33], Iter [91/101] Loss: 0.5160\n",
      "Test MSE: 0.6305665373802185\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7265\n",
      "Epoch [2], Iter [91/101] Loss: 0.6972\n",
      "Epoch [3], Iter [91/101] Loss: 0.6536\n",
      "Epoch [4], Iter [91/101] Loss: 0.6584\n",
      "Epoch [5], Iter [91/101] Loss: 0.6489\n",
      "Epoch [6], Iter [91/101] Loss: 0.7006\n",
      "Epoch [7], Iter [91/101] Loss: 0.6255\n",
      "Epoch [8], Iter [91/101] Loss: 0.6383\n",
      "Epoch [9], Iter [91/101] Loss: 0.6133\n",
      "Epoch [10], Iter [91/101] Loss: 0.6720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Iter [91/101] Loss: 0.6198\n",
      "Epoch [12], Iter [91/101] Loss: 0.5795\n",
      "Epoch [13], Iter [91/101] Loss: 0.6522\n",
      "Epoch [14], Iter [91/101] Loss: 0.5963\n",
      "Epoch [15], Iter [91/101] Loss: 0.5993\n",
      "Epoch [16], Iter [91/101] Loss: 0.5699\n",
      "Epoch [17], Iter [91/101] Loss: 0.5729\n",
      "Epoch [18], Iter [91/101] Loss: 0.5553\n",
      "Epoch [19], Iter [91/101] Loss: 0.5450\n",
      "Epoch [20], Iter [91/101] Loss: 0.5519\n",
      "Epoch [21], Iter [91/101] Loss: 0.5594\n",
      "Epoch [22], Iter [91/101] Loss: 0.5313\n",
      "Epoch [23], Iter [91/101] Loss: 0.5676\n",
      "Epoch [24], Iter [91/101] Loss: 0.5408\n",
      "Epoch [25], Iter [91/101] Loss: 0.5403\n",
      "Epoch [26], Iter [91/101] Loss: 0.5721\n",
      "Epoch [27], Iter [91/101] Loss: 0.5774\n",
      "Epoch [28], Iter [91/101] Loss: 0.5470\n",
      "Epoch [29], Iter [91/101] Loss: 0.5909\n",
      "Epoch [30], Iter [91/101] Loss: 0.5483\n",
      "Epoch [31], Iter [91/101] Loss: 0.5393\n",
      "Epoch [32], Iter [91/101] Loss: 0.5740\n",
      "Epoch [33], Iter [91/101] Loss: 0.5690\n",
      "Epoch [34], Iter [91/101] Loss: 0.5589\n",
      "Epoch [35], Iter [91/101] Loss: 0.5323\n",
      "Test MSE: 0.6214001774787903\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8148\n",
      "Epoch [2], Iter [91/101] Loss: 0.7845\n",
      "Epoch [3], Iter [91/101] Loss: 0.6551\n",
      "Epoch [4], Iter [91/101] Loss: 0.6542\n",
      "Epoch [5], Iter [91/101] Loss: 0.7003\n",
      "Epoch [6], Iter [91/101] Loss: 0.6022\n",
      "Epoch [7], Iter [91/101] Loss: 0.6656\n",
      "Epoch [8], Iter [91/101] Loss: 0.6703\n",
      "Epoch [9], Iter [91/101] Loss: 0.6581\n",
      "Epoch [10], Iter [91/101] Loss: 0.6411\n",
      "Epoch [11], Iter [91/101] Loss: 0.7402\n",
      "Epoch [12], Iter [91/101] Loss: 0.6112\n",
      "Epoch [13], Iter [91/101] Loss: 0.6001\n",
      "Epoch [14], Iter [91/101] Loss: 0.5920\n",
      "Epoch [15], Iter [91/101] Loss: 0.6011\n",
      "Epoch [16], Iter [91/101] Loss: 0.5649\n",
      "Epoch [17], Iter [91/101] Loss: 0.5796\n",
      "Epoch [18], Iter [91/101] Loss: 0.5656\n",
      "Epoch [19], Iter [91/101] Loss: 0.5918\n",
      "Epoch [20], Iter [91/101] Loss: 0.5667\n",
      "Epoch [21], Iter [91/101] Loss: 0.5999\n",
      "Epoch [22], Iter [91/101] Loss: 0.5633\n",
      "Epoch [23], Iter [91/101] Loss: 0.5698\n",
      "Epoch [24], Iter [91/101] Loss: 0.6294\n",
      "Epoch [25], Iter [91/101] Loss: 0.5911\n",
      "Epoch [26], Iter [91/101] Loss: 0.5208\n",
      "Epoch [27], Iter [91/101] Loss: 0.5462\n",
      "Epoch [28], Iter [91/101] Loss: 0.5303\n",
      "Epoch [29], Iter [91/101] Loss: 0.5390\n",
      "Epoch [30], Iter [91/101] Loss: 0.5257\n",
      "Epoch [31], Iter [91/101] Loss: 0.5296\n",
      "Epoch [32], Iter [91/101] Loss: 0.5380\n",
      "Epoch [33], Iter [91/101] Loss: 0.5337\n",
      "Epoch [34], Iter [91/101] Loss: 0.5454\n",
      "Epoch [35], Iter [91/101] Loss: 0.5397\n",
      "Epoch [36], Iter [91/101] Loss: 0.5071\n",
      "Test MSE: 0.6186277866363525\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7501\n",
      "Epoch [2], Iter [91/101] Loss: 0.6882\n",
      "Epoch [3], Iter [91/101] Loss: 0.6623\n",
      "Epoch [4], Iter [91/101] Loss: 0.6634\n",
      "Epoch [5], Iter [91/101] Loss: 0.6528\n",
      "Epoch [6], Iter [91/101] Loss: 0.7213\n",
      "Epoch [7], Iter [91/101] Loss: 0.6355\n",
      "Epoch [8], Iter [91/101] Loss: 0.6224\n",
      "Epoch [9], Iter [91/101] Loss: 0.6310\n",
      "Epoch [10], Iter [91/101] Loss: 0.5919\n",
      "Epoch [11], Iter [91/101] Loss: 0.5960\n",
      "Epoch [12], Iter [91/101] Loss: 0.6291\n",
      "Epoch [13], Iter [91/101] Loss: 0.6147\n",
      "Epoch [14], Iter [91/101] Loss: 0.5834\n",
      "Epoch [15], Iter [91/101] Loss: 0.6377\n",
      "Epoch [16], Iter [91/101] Loss: 0.6138\n",
      "Epoch [17], Iter [91/101] Loss: 0.5859\n",
      "Epoch [18], Iter [91/101] Loss: 0.5780\n",
      "Epoch [19], Iter [91/101] Loss: 0.6041\n",
      "Epoch [20], Iter [91/101] Loss: 0.6674\n",
      "Epoch [21], Iter [91/101] Loss: 0.5838\n",
      "Epoch [22], Iter [91/101] Loss: 0.5746\n",
      "Epoch [23], Iter [91/101] Loss: 0.5439\n",
      "Epoch [24], Iter [91/101] Loss: 0.5694\n",
      "Epoch [25], Iter [91/101] Loss: 0.5472\n",
      "Epoch [26], Iter [91/101] Loss: 0.5588\n",
      "Epoch [27], Iter [91/101] Loss: 0.5569\n",
      "Epoch [28], Iter [91/101] Loss: 0.5337\n",
      "Epoch [29], Iter [91/101] Loss: 0.5600\n",
      "Epoch [30], Iter [91/101] Loss: 0.5624\n",
      "Epoch [31], Iter [91/101] Loss: 0.5532\n",
      "Epoch [32], Iter [91/101] Loss: 0.5471\n",
      "Epoch [33], Iter [91/101] Loss: 0.6330\n",
      "Epoch [34], Iter [91/101] Loss: 0.5400\n",
      "Epoch [35], Iter [91/101] Loss: 0.5519\n",
      "Epoch [36], Iter [91/101] Loss: 0.5440\n",
      "Epoch [37], Iter [91/101] Loss: 0.5150\n",
      "Epoch [38], Iter [91/101] Loss: 0.5316\n",
      "Epoch [39], Iter [91/101] Loss: 0.5142\n",
      "Epoch [40], Iter [91/101] Loss: 0.5034\n",
      "Epoch [41], Iter [91/101] Loss: 0.5159\n",
      "Test MSE: 0.6276491284370422\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7585\n",
      "Epoch [2], Iter [91/101] Loss: 0.6705\n",
      "Epoch [3], Iter [91/101] Loss: 0.7304\n",
      "Epoch [4], Iter [91/101] Loss: 0.8483\n",
      "Epoch [5], Iter [91/101] Loss: 0.7258\n",
      "Epoch [6], Iter [91/101] Loss: 0.6524\n",
      "Epoch [7], Iter [91/101] Loss: 0.6827\n",
      "Epoch [8], Iter [91/101] Loss: 0.6300\n",
      "Epoch [9], Iter [91/101] Loss: 0.6227\n",
      "Epoch [10], Iter [91/101] Loss: 0.6270\n",
      "Epoch [11], Iter [91/101] Loss: 0.6034\n",
      "Epoch [12], Iter [91/101] Loss: 0.6126\n",
      "Epoch [13], Iter [91/101] Loss: 0.7463\n",
      "Epoch [14], Iter [91/101] Loss: 0.6789\n",
      "Epoch [15], Iter [91/101] Loss: 0.5701\n",
      "Epoch [16], Iter [91/101] Loss: 0.5873\n",
      "Epoch [17], Iter [91/101] Loss: 0.5881\n",
      "Epoch [18], Iter [91/101] Loss: 0.5760\n",
      "Epoch [19], Iter [91/101] Loss: 0.5837\n",
      "Epoch [20], Iter [91/101] Loss: 0.5678\n",
      "Epoch [21], Iter [91/101] Loss: 0.6055\n",
      "Epoch [22], Iter [91/101] Loss: 0.5385\n",
      "Epoch [23], Iter [91/101] Loss: 0.5977\n",
      "Epoch [24], Iter [91/101] Loss: 0.5523\n",
      "Epoch [25], Iter [91/101] Loss: 0.5690\n",
      "Epoch [26], Iter [91/101] Loss: 0.6199\n",
      "Epoch [27], Iter [91/101] Loss: 0.6099\n",
      "Epoch [28], Iter [91/101] Loss: 0.5627\n",
      "Test MSE: 0.638066291809082\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7437\n",
      "Epoch [2], Iter [91/101] Loss: 0.6736\n",
      "Epoch [3], Iter [91/101] Loss: 0.6978\n",
      "Epoch [4], Iter [91/101] Loss: 0.6722\n",
      "Epoch [5], Iter [91/101] Loss: 0.6656\n",
      "Epoch [6], Iter [91/101] Loss: 0.6143\n",
      "Epoch [7], Iter [91/101] Loss: 0.6561\n",
      "Epoch [8], Iter [91/101] Loss: 0.6643\n",
      "Epoch [9], Iter [91/101] Loss: 0.7138\n",
      "Epoch [10], Iter [91/101] Loss: 0.6463\n",
      "Epoch [11], Iter [91/101] Loss: 0.6068\n",
      "Epoch [12], Iter [91/101] Loss: 0.5967\n",
      "Epoch [13], Iter [91/101] Loss: 0.6267\n",
      "Epoch [14], Iter [91/101] Loss: 0.6088\n",
      "Epoch [15], Iter [91/101] Loss: 0.6094\n",
      "Epoch [16], Iter [91/101] Loss: 0.5845\n",
      "Epoch [17], Iter [91/101] Loss: 0.6419\n",
      "Epoch [18], Iter [91/101] Loss: 0.6152\n",
      "Epoch [19], Iter [91/101] Loss: 0.6027\n",
      "Epoch [20], Iter [91/101] Loss: 1.3965\n",
      "Epoch [21], Iter [91/101] Loss: 0.6605\n",
      "Epoch [22], Iter [91/101] Loss: 0.6084\n",
      "Epoch [23], Iter [91/101] Loss: 0.5560\n",
      "Epoch [24], Iter [91/101] Loss: 0.5587\n",
      "Epoch [25], Iter [91/101] Loss: 0.5186\n",
      "Epoch [26], Iter [91/101] Loss: 0.5489\n",
      "Epoch [27], Iter [91/101] Loss: 0.5544\n",
      "Epoch [28], Iter [91/101] Loss: 0.5710\n",
      "Epoch [29], Iter [91/101] Loss: 0.5573\n",
      "Epoch [30], Iter [91/101] Loss: 0.5740\n",
      "Epoch [31], Iter [91/101] Loss: 0.5396\n",
      "Test MSE: 0.6388166546821594\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7134\n",
      "Epoch [2], Iter [91/101] Loss: 0.6621\n",
      "Epoch [3], Iter [91/101] Loss: 0.6891\n",
      "Epoch [4], Iter [91/101] Loss: 0.6383\n",
      "Epoch [5], Iter [91/101] Loss: 0.6581\n",
      "Epoch [6], Iter [91/101] Loss: 0.6611\n",
      "Epoch [7], Iter [91/101] Loss: 0.6482\n",
      "Epoch [8], Iter [91/101] Loss: 0.6001\n",
      "Epoch [9], Iter [91/101] Loss: 0.6434\n",
      "Epoch [10], Iter [91/101] Loss: 0.6099\n",
      "Epoch [11], Iter [91/101] Loss: 0.6338\n",
      "Epoch [12], Iter [91/101] Loss: 0.5938\n",
      "Epoch [13], Iter [91/101] Loss: 0.6559\n",
      "Epoch [14], Iter [91/101] Loss: 0.6314\n",
      "Epoch [15], Iter [91/101] Loss: 0.6633\n",
      "Epoch [16], Iter [91/101] Loss: 0.5870\n",
      "Epoch [17], Iter [91/101] Loss: 0.5935\n",
      "Epoch [18], Iter [91/101] Loss: 0.5899\n",
      "Epoch [19], Iter [91/101] Loss: 0.5670\n",
      "Epoch [20], Iter [91/101] Loss: 0.5597\n",
      "Epoch [21], Iter [91/101] Loss: 0.5528\n",
      "Epoch [22], Iter [91/101] Loss: 0.5484\n",
      "Epoch [23], Iter [91/101] Loss: 0.5624\n",
      "Epoch [24], Iter [91/101] Loss: 0.5663\n",
      "Epoch [25], Iter [91/101] Loss: 0.5452\n",
      "Epoch [26], Iter [91/101] Loss: 0.6181\n",
      "Epoch [27], Iter [91/101] Loss: 0.5404\n",
      "Test MSE: 0.6440518498420715\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    for i in range(runs_per_lr):\n",
    "        model_training_description[\"RUN_NR\"] = i\n",
    "        model_training_description[\"LEARNING_RATE\"] = lr  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "        unet = train_unet(description, model_training_description, output_folder)\n",
    "        predict_save_unet(description, model_training_description, output_folder, unet, output_folder)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a010c0",
   "metadata": {},
   "source": [
    "### UNet wider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bda222",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 1e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 64\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (64,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3898bb",
   "metadata": {},
   "source": [
    "### UNet deeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d342a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "# training parameters\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 4 # this changes compared to standard UNet\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 1e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98051b",
   "metadata": {},
   "source": [
    "# 3) Precipitation weighting \n",
    "\n",
    "Test how much the results differ if we weight by precipitation amount in the creation of the yearly dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 1e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b72386",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "90b409070de10f2c3c5a8c504ecede612c695480a2027d21529fbe1a33a534d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
