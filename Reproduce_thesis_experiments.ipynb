{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd1b770",
   "metadata": {},
   "source": [
    "# Reimplement thesis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e90607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from predict import *\n",
    "from evaluate import *\n",
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62b8e1",
   "metadata": {},
   "source": [
    "## 1) Create datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047f5b9",
   "metadata": {},
   "source": [
    "### tas, pr, oro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10fe2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\",\n",
    "                                \"oro\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"oro\": [\"ht\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624fd208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512d24d",
   "metadata": {},
   "source": [
    "## Tas, pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80769317",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02346b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7a6fe",
   "metadata": {},
   "source": [
    "### tas, pr, slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fa64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254bb9e",
   "metadata": {},
   "source": [
    "### tas only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b0669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154cd2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916970ee",
   "metadata": {},
   "source": [
    "### pr only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23b7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ab0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef46d1a",
   "metadata": {},
   "source": [
    "### slp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7e15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "027ca037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cce60",
   "metadata": {},
   "source": [
    "### pr, tas precip weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e9ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cb889c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_precip_weighted_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b338477",
   "metadata": {},
   "source": [
    "### pr, tas ico grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31b74e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f42125f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ea85e",
   "metadata": {},
   "source": [
    "## 2) Run experiments yearly dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed8d56",
   "metadata": {},
   "source": [
    "### 4.2.1 Modifications to flat UNet\n",
    "\n",
    "Start by selecting the tas, pr dataset without precipitation weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf2fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f8117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbaabda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [\"Masked_MSELoss\", \"Masked_AreaWeightedMSELoss\"]\n",
    "use_coord_conv = [False, True]\n",
    "use_cylindrical_padding = [False, True]\n",
    "n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28270b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked_MSELoss False False 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6607\n",
      "Epoch [2], Iter [91/101] Loss: 0.6757\n",
      "Epoch [3], Iter [91/101] Loss: 0.6343\n",
      "Epoch [4], Iter [91/101] Loss: 0.5774\n",
      "Epoch [5], Iter [91/101] Loss: 0.5772\n",
      "Epoch [6], Iter [91/101] Loss: 0.5505\n",
      "Epoch [7], Iter [91/101] Loss: 0.5592\n",
      "Epoch [8], Iter [91/101] Loss: 0.5333\n",
      "Epoch [9], Iter [91/101] Loss: 0.5272\n",
      "Epoch [10], Iter [91/101] Loss: 0.5236\n",
      "Epoch [11], Iter [91/101] Loss: 0.4955\n",
      "Epoch [12], Iter [91/101] Loss: 0.4932\n",
      "Epoch [13], Iter [91/101] Loss: 0.4632\n",
      "Epoch [14], Iter [91/101] Loss: 0.4929\n",
      "Epoch [15], Iter [91/101] Loss: 0.4585\n",
      "Epoch [16], Iter [91/101] Loss: 0.4395\n",
      "Epoch [17], Iter [91/101] Loss: 0.4362\n",
      "Test MSE: 0.6291521787643433\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 1\n",
      "Starting training\n",
      "Epoch [1], Iter [31/101] Loss: 0.7068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\49159\\AppData\\Local\\Temp/ipykernel_12276/318576728.py\", line 10, in <module>\n",
      "    unet = train_unet(description, model_training_description, output_folder)\n",
      "  File \"C:\\Users\\49159\\Documents\\Uni\\IcoCNN_new\\train.py\", line 604, in train_unet\n",
      "    running_loss += loss.item()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\49159\\Anaconda3\\envs\\GrouPyTorch\\lib\\ntpath.py\", line 631, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12276/318576728.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RUN_NR\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0munet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mpredict_save_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\train.py\u001b[0m in \u001b[0;36mtrain_unet\u001b[1;34m(dataset_description, model_training_description, base_folder, use_tensorboard)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m                 \u001b[0mn_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2060\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrouPyTorch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for l in loss:\n",
    "    for c_conv in use_coord_conv:\n",
    "        for c_pad in use_cylindrical_padding:\n",
    "            for i in range(n_runs):\n",
    "                print(l, c_conv, c_pad, i)\n",
    "                model_training_description[\"USE_CYLINDRICAL_PADDING\"] = c_pad\n",
    "                model_training_description[\"USE_COORD_CONV\"] = c_conv\n",
    "                model_training_description[\"LOSS\"] = l  # \"MSELoss\" # \"AreaWeightedMSELoss\"\n",
    "                model_training_description[\"RUN_NR\"] = i\n",
    "                unet = train_unet(description, model_training_description, output_folder)\n",
    "                predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e757923",
   "metadata": {},
   "source": [
    "### 4.2.2 Comparing results for different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf587d",
   "metadata": {},
   "source": [
    "We will need to implement the interpolation before we can reproduce the whole table. Until then: Only compare ico architectures on ico grid and flat architectures on flat grid.\n",
    "\n",
    "Results for modified and unmodified flat UNet are already obtained in last cell.(4.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad454efa",
   "metadata": {},
   "source": [
    "### Flat grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974f0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eef2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "model_training_description[\"N_PC_PREDICTORS\"] = 450\n",
    "model_training_description[\"N_PC_TARGETS\"] = 300\n",
    "model_training_description[\"REGTYPE\"] = \"lasso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f45df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pca\n",
      "train pca2\n",
      "train model\n",
      "finished training\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pca, pca_targets, model = train_pca(description, model_training_description, output_folder)\n",
    "print(\"finished training\")\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445d9ae",
   "metadata": {},
   "source": [
    "### New linreg baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab67592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"LinReg_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3705d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "models = train_linreg_pixelwise(description, model_training_description, output_folder)\n",
    "predict_save_linreg_pixelwise(description, model_training_description, output_folder, models, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5010b",
   "metadata": {},
   "source": [
    "### New PCA-reg baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1a00b",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning, compute 50x50 logarithmic grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a934508",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 50)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62ba9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results: N_PC_IN: 257 N_PC_OUT: 147, R2_mean, validationset: [0.16969872]\n",
      "Retrain including validation set.\n",
      "Result on test set: [0.18911445]\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89376fae",
   "metadata": {},
   "source": [
    "### New Random-forest baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069fbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"RandomForest_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e193661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100building tree 3 of 100\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  4.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 20.0min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   55.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed: 11.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "model = train_random_forest_pixelwise(description, model_training_description, output_folder, verbose=3, n_jobs=-1)\n",
    "predict_save_randomforest_pixelwise(description, model_training_description, output_folder, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc721a",
   "metadata": {},
   "source": [
    "### Icosahedral grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e59925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca80ed",
   "metadata": {},
   "source": [
    "Ico baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe580f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff5ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 20)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1780209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results: N_PC_IN: 222 N_PC_OUT: 166, R2_mean, validationset: [0.19301843]\n",
      "Retrain including validation set.\n",
      "Result on test set: [0.2131577]\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98681c",
   "metadata": {},
   "source": [
    "Ico UNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327ecbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = IcoBatchNorm2d # torch.nn.BatchNorm2d\n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "model_training_description[\"LOSS\"] = \"MSELoss\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab785a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8002\n",
      "Epoch [2], Iter [91/100] Loss: 0.7010\n",
      "Epoch [3], Iter [91/100] Loss: 0.6858\n",
      "Epoch [4], Iter [91/100] Loss: 0.6467\n",
      "Epoch [5], Iter [91/100] Loss: 0.6227\n",
      "Epoch [6], Iter [91/100] Loss: 0.5836\n",
      "Epoch [7], Iter [91/100] Loss: 0.5944\n",
      "Epoch [8], Iter [91/100] Loss: 0.5605\n",
      "Epoch [9], Iter [91/100] Loss: 0.5584\n",
      "Epoch [10], Iter [91/100] Loss: 0.5482\n",
      "Epoch [11], Iter [91/100] Loss: 0.5457\n",
      "Epoch [12], Iter [91/100] Loss: 0.5198\n",
      "Epoch [13], Iter [91/100] Loss: 0.5154\n",
      "Epoch [14], Iter [91/100] Loss: 0.4617\n",
      "Epoch [15], Iter [91/100] Loss: 0.4886\n",
      "Test MSE: 0.6205584406852722\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8139\n",
      "Epoch [2], Iter [91/100] Loss: 0.7641\n",
      "Epoch [3], Iter [91/100] Loss: 0.6952\n",
      "Epoch [4], Iter [91/100] Loss: 0.6451\n",
      "Epoch [5], Iter [91/100] Loss: 0.6408\n",
      "Epoch [6], Iter [91/100] Loss: 0.5781\n",
      "Epoch [7], Iter [91/100] Loss: 0.5841\n",
      "Epoch [8], Iter [91/100] Loss: 0.5611\n",
      "Epoch [9], Iter [91/100] Loss: 0.5457\n",
      "Epoch [10], Iter [91/100] Loss: 0.5736\n",
      "Epoch [11], Iter [91/100] Loss: 0.5576\n",
      "Epoch [12], Iter [91/100] Loss: 0.5357\n",
      "Epoch [13], Iter [91/100] Loss: 0.4989\n",
      "Epoch [14], Iter [91/100] Loss: 0.4991\n",
      "Epoch [15], Iter [91/100] Loss: 0.4625\n",
      "Epoch [16], Iter [91/100] Loss: 0.4734\n",
      "Epoch [17], Iter [91/100] Loss: 0.4541\n",
      "Epoch [18], Iter [91/100] Loss: 0.4204\n",
      "Test MSE: 0.6036093235015869\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8109\n",
      "Epoch [2], Iter [91/100] Loss: 0.9515\n",
      "Epoch [3], Iter [91/100] Loss: 0.6567\n",
      "Epoch [4], Iter [91/100] Loss: 0.6586\n",
      "Epoch [5], Iter [91/100] Loss: 0.6293\n",
      "Epoch [6], Iter [91/100] Loss: 0.6275\n",
      "Epoch [7], Iter [91/100] Loss: 0.5998\n",
      "Epoch [8], Iter [91/100] Loss: 0.6059\n",
      "Epoch [9], Iter [91/100] Loss: 0.5747\n",
      "Epoch [10], Iter [91/100] Loss: 0.6368\n",
      "Epoch [11], Iter [91/100] Loss: 0.6088\n",
      "Epoch [12], Iter [91/100] Loss: 0.5472\n",
      "Epoch [13], Iter [91/100] Loss: 0.5291\n",
      "Epoch [14], Iter [91/100] Loss: 0.5267\n",
      "Epoch [15], Iter [91/100] Loss: 0.4906\n",
      "Epoch [16], Iter [91/100] Loss: 0.4958\n",
      "Epoch [17], Iter [91/100] Loss: 0.4504\n",
      "Epoch [18], Iter [91/100] Loss: 0.4594\n",
      "Test MSE: 0.6061821579933167\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7795\n",
      "Epoch [2], Iter [91/100] Loss: 0.6496\n",
      "Epoch [3], Iter [91/100] Loss: 0.6517\n",
      "Epoch [4], Iter [91/100] Loss: 0.6112\n",
      "Epoch [5], Iter [91/100] Loss: 0.6200\n",
      "Epoch [6], Iter [91/100] Loss: 0.5809\n",
      "Epoch [7], Iter [91/100] Loss: 0.5852\n",
      "Epoch [8], Iter [91/100] Loss: 0.5862\n",
      "Epoch [9], Iter [91/100] Loss: 0.5328\n",
      "Epoch [10], Iter [91/100] Loss: 0.5322\n",
      "Epoch [11], Iter [91/100] Loss: 0.4972\n",
      "Epoch [12], Iter [91/100] Loss: 0.4970\n",
      "Epoch [13], Iter [91/100] Loss: 0.4769\n",
      "Epoch [14], Iter [91/100] Loss: 0.4572\n",
      "Epoch [15], Iter [91/100] Loss: 0.4422\n",
      "Epoch [16], Iter [91/100] Loss: 0.4211\n",
      "Epoch [17], Iter [91/100] Loss: 0.4267\n",
      "Epoch [18], Iter [91/100] Loss: 0.3964\n",
      "Epoch [19], Iter [91/100] Loss: 0.4121\n",
      "Epoch [20], Iter [91/100] Loss: 0.3742\n",
      "Test MSE: 0.61271733045578\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8088\n",
      "Epoch [2], Iter [91/100] Loss: 0.7485\n",
      "Epoch [3], Iter [91/100] Loss: 0.6600\n",
      "Epoch [4], Iter [91/100] Loss: 0.6366\n",
      "Epoch [5], Iter [91/100] Loss: 0.6186\n",
      "Epoch [6], Iter [91/100] Loss: 0.6289\n",
      "Epoch [7], Iter [91/100] Loss: 0.5833\n",
      "Epoch [8], Iter [91/100] Loss: 0.5597\n",
      "Epoch [9], Iter [91/100] Loss: 0.5618\n",
      "Epoch [10], Iter [91/100] Loss: 0.5417\n",
      "Epoch [11], Iter [91/100] Loss: 0.5228\n",
      "Epoch [12], Iter [91/100] Loss: 0.5411\n",
      "Epoch [13], Iter [91/100] Loss: 0.5027\n",
      "Epoch [14], Iter [91/100] Loss: 0.4826\n",
      "Epoch [15], Iter [91/100] Loss: 0.4789\n",
      "Epoch [16], Iter [91/100] Loss: 0.4532\n",
      "Epoch [17], Iter [91/100] Loss: 0.4330\n",
      "Test MSE: 0.6188255548477173\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8405\n",
      "Epoch [2], Iter [91/100] Loss: 0.7518\n",
      "Epoch [3], Iter [91/100] Loss: 0.6849\n",
      "Epoch [4], Iter [91/100] Loss: 0.6672\n",
      "Epoch [5], Iter [91/100] Loss: 0.6788\n",
      "Epoch [6], Iter [91/100] Loss: 0.6013\n",
      "Epoch [7], Iter [91/100] Loss: 0.6264\n",
      "Epoch [8], Iter [91/100] Loss: 0.5895\n",
      "Epoch [9], Iter [91/100] Loss: 0.5728\n",
      "Epoch [10], Iter [91/100] Loss: 0.5805\n",
      "Epoch [11], Iter [91/100] Loss: 0.5251\n",
      "Epoch [12], Iter [91/100] Loss: 0.5329\n",
      "Epoch [13], Iter [91/100] Loss: 0.5177\n",
      "Epoch [14], Iter [91/100] Loss: 0.5007\n",
      "Epoch [15], Iter [91/100] Loss: 0.5055\n",
      "Test MSE: 0.6143753528594971\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7826\n",
      "Epoch [2], Iter [91/100] Loss: 0.7020\n",
      "Epoch [3], Iter [91/100] Loss: 0.6604\n",
      "Epoch [4], Iter [91/100] Loss: 0.6584\n",
      "Epoch [5], Iter [91/100] Loss: 0.6225\n",
      "Epoch [6], Iter [91/100] Loss: 0.6474\n",
      "Epoch [7], Iter [91/100] Loss: 0.6194\n",
      "Epoch [8], Iter [91/100] Loss: 0.5620\n",
      "Epoch [9], Iter [91/100] Loss: 0.5951\n",
      "Epoch [10], Iter [91/100] Loss: 0.5588\n",
      "Epoch [11], Iter [91/100] Loss: 0.5365\n",
      "Epoch [12], Iter [91/100] Loss: 0.5369\n",
      "Epoch [13], Iter [91/100] Loss: 0.5078\n",
      "Epoch [14], Iter [91/100] Loss: 0.5081\n",
      "Epoch [15], Iter [91/100] Loss: 0.4728\n",
      "Epoch [16], Iter [91/100] Loss: 0.4566\n",
      "Epoch [17], Iter [91/100] Loss: 0.4441\n",
      "Epoch [18], Iter [91/100] Loss: 0.4187\n",
      "Epoch [19], Iter [91/100] Loss: 0.4109\n",
      "Epoch [20], Iter [91/100] Loss: 0.3946\n",
      "Epoch [21], Iter [91/100] Loss: 0.3915\n",
      "Epoch [22], Iter [91/100] Loss: 0.3792\n",
      "Test MSE: 0.617697536945343\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8621\n",
      "Epoch [2], Iter [91/100] Loss: 0.7585\n",
      "Epoch [3], Iter [91/100] Loss: 0.8161\n",
      "Epoch [4], Iter [91/100] Loss: 0.6521\n",
      "Epoch [5], Iter [91/100] Loss: 0.7031\n",
      "Epoch [6], Iter [91/100] Loss: 0.6340\n",
      "Epoch [7], Iter [91/100] Loss: 0.6009\n",
      "Epoch [8], Iter [91/100] Loss: 0.6335\n",
      "Epoch [9], Iter [91/100] Loss: 0.6052\n",
      "Epoch [10], Iter [91/100] Loss: 0.6015\n",
      "Epoch [11], Iter [91/100] Loss: 0.5676\n",
      "Epoch [12], Iter [91/100] Loss: 0.5679\n",
      "Epoch [13], Iter [91/100] Loss: 0.5479\n",
      "Epoch [14], Iter [91/100] Loss: 0.5287\n",
      "Epoch [15], Iter [91/100] Loss: 0.5364\n",
      "Epoch [16], Iter [91/100] Loss: 0.5551\n",
      "Epoch [17], Iter [91/100] Loss: 0.5104\n",
      "Epoch [18], Iter [91/100] Loss: 0.5157\n",
      "Epoch [19], Iter [91/100] Loss: 0.5818\n",
      "Test MSE: 0.6075485348701477\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8608\n",
      "Epoch [2], Iter [91/100] Loss: 0.8255\n",
      "Epoch [3], Iter [91/100] Loss: 0.7386\n",
      "Epoch [4], Iter [91/100] Loss: 0.6809\n",
      "Epoch [5], Iter [91/100] Loss: 0.6546\n",
      "Epoch [6], Iter [91/100] Loss: 0.6738\n",
      "Epoch [7], Iter [91/100] Loss: 0.6426\n",
      "Epoch [8], Iter [91/100] Loss: 0.6527\n",
      "Epoch [9], Iter [91/100] Loss: 0.5891\n",
      "Epoch [10], Iter [91/100] Loss: 0.6246\n",
      "Epoch [11], Iter [91/100] Loss: 0.5857\n",
      "Epoch [12], Iter [91/100] Loss: 0.5792\n",
      "Epoch [13], Iter [91/100] Loss: 0.5474\n",
      "Epoch [14], Iter [91/100] Loss: 0.5524\n",
      "Epoch [15], Iter [91/100] Loss: 0.5501\n",
      "Epoch [16], Iter [91/100] Loss: 0.5123\n",
      "Epoch [17], Iter [91/100] Loss: 0.5098\n",
      "Epoch [18], Iter [91/100] Loss: 0.5057\n",
      "Epoch [19], Iter [91/100] Loss: 0.5053\n",
      "Epoch [20], Iter [91/100] Loss: 0.4465\n",
      "Epoch [21], Iter [91/100] Loss: 0.4704\n",
      "Epoch [22], Iter [91/100] Loss: 0.4439\n",
      "Test MSE: 0.5933142900466919\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8753\n",
      "Epoch [2], Iter [91/100] Loss: 0.7273\n",
      "Epoch [3], Iter [91/100] Loss: 0.7898\n",
      "Epoch [4], Iter [91/100] Loss: 0.6106\n",
      "Epoch [5], Iter [91/100] Loss: 0.6479\n",
      "Epoch [6], Iter [91/100] Loss: 0.6549\n",
      "Epoch [7], Iter [91/100] Loss: 0.6294\n",
      "Epoch [8], Iter [91/100] Loss: 0.6073\n",
      "Epoch [9], Iter [91/100] Loss: 0.5766\n",
      "Epoch [10], Iter [91/100] Loss: 0.5767\n",
      "Epoch [11], Iter [91/100] Loss: 0.5585\n",
      "Epoch [12], Iter [91/100] Loss: 0.5341\n",
      "Epoch [13], Iter [91/100] Loss: 0.5228\n",
      "Epoch [14], Iter [91/100] Loss: 0.5418\n",
      "Epoch [15], Iter [91/100] Loss: 0.5169\n",
      "Epoch [16], Iter [91/100] Loss: 0.4921\n",
      "Epoch [17], Iter [91/100] Loss: 0.4672\n",
      "Epoch [18], Iter [91/100] Loss: 0.4421\n",
      "Epoch [19], Iter [91/100] Loss: 0.4162\n",
      "Epoch [20], Iter [91/100] Loss: 0.4052\n",
      "Epoch [21], Iter [91/100] Loss: 0.4145\n",
      "Test MSE: 0.5961763858795166\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9f8e0",
   "metadata": {},
   "source": [
    "## 4.2.3 COMPARING RESULTS FOR DIFFERENT PREDICTOR COMBINATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4b0c3",
   "metadata": {},
   "source": [
    "### tas only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ff3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bacf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c422-0x4bb546e1f722d58b_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8545\n",
      "Epoch [2], Iter [91/100] Loss: 0.8283\n",
      "Epoch [3], Iter [91/100] Loss: 0.7116\n",
      "Epoch [4], Iter [91/100] Loss: 0.7889\n",
      "Epoch [5], Iter [91/100] Loss: 0.7034\n",
      "Epoch [6], Iter [91/100] Loss: 0.6899\n",
      "Epoch [7], Iter [91/100] Loss: 0.7414\n",
      "Epoch [8], Iter [91/100] Loss: 0.7019\n",
      "Epoch [9], Iter [91/100] Loss: 0.6896\n",
      "Epoch [10], Iter [91/100] Loss: 0.6510\n",
      "Epoch [11], Iter [91/100] Loss: 0.6369\n",
      "Epoch [12], Iter [91/100] Loss: 0.6217\n",
      "Epoch [13], Iter [91/100] Loss: 0.6367\n",
      "Epoch [14], Iter [91/100] Loss: 0.6702\n",
      "Epoch [15], Iter [91/100] Loss: 0.6054\n",
      "Epoch [16], Iter [91/100] Loss: 0.5786\n",
      "Epoch [17], Iter [91/100] Loss: 0.5885\n",
      "Epoch [18], Iter [91/100] Loss: 0.5876\n",
      "Epoch [19], Iter [91/100] Loss: 0.5990\n",
      "Epoch [20], Iter [91/100] Loss: 0.5799\n",
      "Test MSE: 0.7301462292671204\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c422-0x50d06c735a7775d3_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7812\n",
      "Epoch [2], Iter [91/100] Loss: 0.7958\n",
      "Epoch [3], Iter [91/100] Loss: 0.7233\n",
      "Epoch [4], Iter [91/100] Loss: 0.7695\n",
      "Epoch [5], Iter [91/100] Loss: 0.7236\n",
      "Epoch [6], Iter [91/100] Loss: 0.6619\n",
      "Epoch [7], Iter [91/100] Loss: 0.7011\n",
      "Epoch [8], Iter [91/100] Loss: 0.6719\n",
      "Epoch [9], Iter [91/100] Loss: 0.6808\n",
      "Epoch [10], Iter [91/100] Loss: 0.6939\n",
      "Epoch [11], Iter [91/100] Loss: 0.6591\n",
      "Epoch [12], Iter [91/100] Loss: 0.6607\n",
      "Epoch [13], Iter [91/100] Loss: 0.6194\n",
      "Epoch [14], Iter [91/100] Loss: 0.6253\n",
      "Epoch [15], Iter [91/100] Loss: 0.6225\n",
      "Epoch [16], Iter [91/100] Loss: 0.6475\n",
      "Epoch [17], Iter [91/100] Loss: 0.6215\n",
      "Epoch [18], Iter [91/100] Loss: 0.5882\n",
      "Epoch [19], Iter [91/100] Loss: 0.5866\n",
      "Epoch [20], Iter [91/100] Loss: 0.5760\n",
      "Epoch [21], Iter [91/100] Loss: 0.5681\n",
      "Test MSE: 0.7219374775886536\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c422-0x18d645a6b8e0d970_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7771\n",
      "Epoch [2], Iter [91/100] Loss: 0.8552\n",
      "Epoch [3], Iter [91/100] Loss: 0.7523\n",
      "Epoch [4], Iter [91/100] Loss: 0.7744\n",
      "Epoch [5], Iter [91/100] Loss: 0.7160\n",
      "Epoch [6], Iter [91/100] Loss: 0.7067\n",
      "Epoch [7], Iter [91/100] Loss: 0.7150\n",
      "Epoch [8], Iter [91/100] Loss: 0.6798\n",
      "Epoch [9], Iter [91/100] Loss: 0.6796\n",
      "Epoch [10], Iter [91/100] Loss: 0.6416\n",
      "Epoch [11], Iter [91/100] Loss: 0.6938\n",
      "Epoch [12], Iter [91/100] Loss: 0.6600\n",
      "Epoch [13], Iter [91/100] Loss: 0.6356\n",
      "Epoch [14], Iter [91/100] Loss: 0.6196\n",
      "Epoch [15], Iter [91/100] Loss: 0.5934\n",
      "Test MSE: 0.740757405757904\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c422-0x569c12d1107540e1_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7908\n",
      "Epoch [2], Iter [91/100] Loss: 0.7599\n",
      "Epoch [3], Iter [91/100] Loss: 0.7433\n",
      "Epoch [4], Iter [91/100] Loss: 0.7369\n",
      "Epoch [5], Iter [91/100] Loss: 0.7382\n",
      "Epoch [6], Iter [91/100] Loss: 0.7067\n",
      "Epoch [7], Iter [91/100] Loss: 0.7008\n",
      "Epoch [8], Iter [91/100] Loss: 0.6675\n",
      "Epoch [9], Iter [91/100] Loss: 0.7020\n",
      "Epoch [10], Iter [91/100] Loss: 0.6622\n",
      "Epoch [11], Iter [91/100] Loss: 0.6358\n",
      "Epoch [12], Iter [91/100] Loss: 0.6597\n",
      "Epoch [13], Iter [91/100] Loss: 0.6371\n",
      "Epoch [14], Iter [91/100] Loss: 0.6279\n",
      "Epoch [15], Iter [91/100] Loss: 0.6165\n",
      "Epoch [16], Iter [91/100] Loss: 0.6365\n",
      "Epoch [17], Iter [91/100] Loss: 0.6219\n",
      "Epoch [18], Iter [91/100] Loss: 0.6159\n",
      "Epoch [19], Iter [91/100] Loss: 0.6048\n",
      "Epoch [20], Iter [91/100] Loss: 0.5823\n",
      "Epoch [21], Iter [91/100] Loss: 0.5559\n",
      "Epoch [22], Iter [91/100] Loss: 0.5523\n",
      "Test MSE: 0.7244753837585449\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c422-0x7e523672a25ebe0e_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8263\n",
      "Epoch [2], Iter [91/100] Loss: 0.8501\n",
      "Epoch [3], Iter [91/100] Loss: 0.7400\n",
      "Epoch [4], Iter [91/100] Loss: 0.7546\n",
      "Epoch [5], Iter [91/100] Loss: 0.7140\n",
      "Epoch [6], Iter [91/100] Loss: 0.7908\n",
      "Epoch [7], Iter [91/100] Loss: 0.6910\n",
      "Epoch [8], Iter [91/100] Loss: 0.6803\n",
      "Epoch [9], Iter [91/100] Loss: 0.6912\n",
      "Epoch [10], Iter [91/100] Loss: 0.7175\n",
      "Epoch [11], Iter [91/100] Loss: 0.6907\n",
      "Epoch [12], Iter [91/100] Loss: 0.6747\n",
      "Epoch [13], Iter [91/100] Loss: 0.6349\n",
      "Epoch [14], Iter [91/100] Loss: 0.6245\n",
      "Epoch [15], Iter [91/100] Loss: 0.6427\n",
      "Epoch [16], Iter [91/100] Loss: 0.6139\n",
      "Epoch [17], Iter [91/100] Loss: 0.6176\n",
      "Epoch [18], Iter [91/100] Loss: 0.6268\n",
      "Test MSE: 0.720170259475708\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c4220x1aa2c36fa46b7092_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8556\n",
      "Epoch [2], Iter [91/100] Loss: 0.8104\n",
      "Epoch [3], Iter [91/100] Loss: 0.7294\n",
      "Epoch [4], Iter [91/100] Loss: 0.7352\n",
      "Epoch [5], Iter [91/100] Loss: 0.7020\n",
      "Epoch [6], Iter [91/100] Loss: 0.7079\n",
      "Epoch [7], Iter [91/100] Loss: 0.7173\n",
      "Epoch [8], Iter [91/100] Loss: 0.6989\n",
      "Epoch [9], Iter [91/100] Loss: 0.6613\n",
      "Epoch [10], Iter [91/100] Loss: 0.6771\n",
      "Epoch [11], Iter [91/100] Loss: 0.6827\n",
      "Epoch [12], Iter [91/100] Loss: 0.6348\n",
      "Epoch [13], Iter [91/100] Loss: 0.6382\n",
      "Epoch [14], Iter [91/100] Loss: 0.6282\n",
      "Test MSE: 0.7275064587593079\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c4220xc46eec262af81f4_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8726\n",
      "Epoch [2], Iter [91/100] Loss: 0.7213\n",
      "Epoch [3], Iter [91/100] Loss: 0.7825\n",
      "Epoch [4], Iter [91/100] Loss: 0.7645\n",
      "Epoch [5], Iter [91/100] Loss: 0.6834\n",
      "Epoch [6], Iter [91/100] Loss: 0.7051\n",
      "Epoch [7], Iter [91/100] Loss: 0.6750\n",
      "Epoch [8], Iter [91/100] Loss: 0.7019\n",
      "Epoch [9], Iter [91/100] Loss: 0.7208\n",
      "Epoch [10], Iter [91/100] Loss: 0.6718\n",
      "Epoch [11], Iter [91/100] Loss: 0.6491\n",
      "Epoch [12], Iter [91/100] Loss: 0.6375\n",
      "Epoch [13], Iter [91/100] Loss: 0.6632\n",
      "Epoch [14], Iter [91/100] Loss: 0.6395\n",
      "Epoch [15], Iter [91/100] Loss: 0.6480\n",
      "Epoch [16], Iter [91/100] Loss: 0.6238\n",
      "Epoch [17], Iter [91/100] Loss: 0.6184\n",
      "Epoch [18], Iter [91/100] Loss: 0.6123\n",
      "Epoch [19], Iter [91/100] Loss: 0.5903\n",
      "Epoch [20], Iter [91/100] Loss: 0.5641\n",
      "Test MSE: 0.7169363498687744\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c4220x36c7242442056bf9_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7665\n",
      "Epoch [2], Iter [91/100] Loss: 0.8504\n",
      "Epoch [3], Iter [91/100] Loss: 0.7602\n",
      "Epoch [4], Iter [91/100] Loss: 0.7312\n",
      "Epoch [5], Iter [91/100] Loss: 0.7163\n",
      "Epoch [6], Iter [91/100] Loss: 0.7089\n",
      "Epoch [7], Iter [91/100] Loss: 0.7078\n",
      "Epoch [8], Iter [91/100] Loss: 0.6894\n",
      "Epoch [9], Iter [91/100] Loss: 0.6594\n",
      "Epoch [10], Iter [91/100] Loss: 0.6793\n",
      "Epoch [11], Iter [91/100] Loss: 0.6552\n",
      "Epoch [12], Iter [91/100] Loss: 0.6477\n",
      "Epoch [13], Iter [91/100] Loss: 0.6361\n",
      "Epoch [14], Iter [91/100] Loss: 0.6016\n",
      "Epoch [15], Iter [91/100] Loss: 0.6068\n",
      "Epoch [16], Iter [91/100] Loss: 0.6007\n",
      "Epoch [17], Iter [91/100] Loss: 0.5977\n",
      "Epoch [18], Iter [91/100] Loss: 0.5736\n",
      "Epoch [19], Iter [91/100] Loss: 0.5658\n",
      "Epoch [20], Iter [91/100] Loss: 0.5618\n",
      "Test MSE: 0.736452043056488\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c422-0x54df69ca9c3e68de_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7855\n",
      "Epoch [2], Iter [91/100] Loss: 0.7765\n",
      "Epoch [3], Iter [91/100] Loss: 0.7443\n",
      "Epoch [4], Iter [91/100] Loss: 0.6903\n",
      "Epoch [5], Iter [91/100] Loss: 0.6880\n",
      "Epoch [6], Iter [91/100] Loss: 0.7382\n",
      "Epoch [7], Iter [91/100] Loss: 0.7234\n",
      "Epoch [8], Iter [91/100] Loss: 0.6533\n",
      "Epoch [9], Iter [91/100] Loss: 0.6512\n",
      "Epoch [10], Iter [91/100] Loss: 0.6398\n",
      "Epoch [11], Iter [91/100] Loss: 0.6705\n",
      "Epoch [12], Iter [91/100] Loss: 0.6469\n",
      "Epoch [13], Iter [91/100] Loss: 0.6264\n",
      "Epoch [14], Iter [91/100] Loss: 0.6242\n",
      "Epoch [15], Iter [91/100] Loss: 0.6161\n",
      "Epoch [16], Iter [91/100] Loss: 0.6036\n",
      "Epoch [17], Iter [91/100] Loss: 0.5704\n",
      "Epoch [18], Iter [91/100] Loss: 0.5575\n",
      "Epoch [19], Iter [91/100] Loss: 0.5946\n",
      "Test MSE: 0.7249408960342407\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x440ee194b908c4220x6f9658270b87b93c_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8587\n",
      "Epoch [2], Iter [91/100] Loss: 0.7710\n",
      "Epoch [3], Iter [91/100] Loss: 0.7323\n",
      "Epoch [4], Iter [91/100] Loss: 0.7154\n",
      "Epoch [5], Iter [91/100] Loss: 0.7288\n",
      "Epoch [6], Iter [91/100] Loss: 0.6825\n",
      "Epoch [7], Iter [91/100] Loss: 0.6928\n",
      "Epoch [8], Iter [91/100] Loss: 0.6637\n",
      "Epoch [9], Iter [91/100] Loss: 0.7155\n",
      "Epoch [10], Iter [91/100] Loss: 0.6755\n",
      "Epoch [11], Iter [91/100] Loss: 0.6642\n",
      "Epoch [12], Iter [91/100] Loss: 0.6719\n",
      "Epoch [13], Iter [91/100] Loss: 0.6219\n",
      "Epoch [14], Iter [91/100] Loss: 0.6555\n",
      "Epoch [15], Iter [91/100] Loss: 0.6770\n",
      "Epoch [16], Iter [91/100] Loss: 0.6104\n",
      "Epoch [17], Iter [91/100] Loss: 0.5971\n",
      "Epoch [18], Iter [91/100] Loss: 0.5933\n",
      "Test MSE: 0.7173461318016052\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder, use_tensorboard=True)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f174e",
   "metadata": {},
   "source": [
    "### precip only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82472aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f23f2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c74774-0x4bb546e1f722d58b_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6932\n",
      "Epoch [2], Iter [91/100] Loss: 0.6829\n",
      "Epoch [3], Iter [91/100] Loss: 0.6520\n",
      "Epoch [4], Iter [91/100] Loss: 0.6165\n",
      "Epoch [5], Iter [91/100] Loss: 0.6050\n",
      "Epoch [6], Iter [91/100] Loss: 0.6299\n",
      "Epoch [7], Iter [91/100] Loss: 0.6006\n",
      "Epoch [8], Iter [91/100] Loss: 0.5894\n",
      "Epoch [9], Iter [91/100] Loss: 0.5622\n",
      "Epoch [10], Iter [91/100] Loss: 0.6122\n",
      "Epoch [11], Iter [91/100] Loss: 0.5724\n",
      "Epoch [12], Iter [91/100] Loss: 0.5686\n",
      "Epoch [13], Iter [91/100] Loss: 0.5721\n",
      "Epoch [14], Iter [91/100] Loss: 0.5493\n",
      "Epoch [15], Iter [91/100] Loss: 0.5256\n",
      "Epoch [16], Iter [91/100] Loss: 0.5103\n",
      "Epoch [17], Iter [91/100] Loss: 0.5574\n",
      "Epoch [18], Iter [91/100] Loss: 0.5405\n",
      "Epoch [19], Iter [91/100] Loss: 0.5119\n",
      "Epoch [20], Iter [91/100] Loss: 0.5300\n",
      "Epoch [21], Iter [91/100] Loss: 0.5130\n",
      "Test MSE: 0.641785204410553\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c74774-0x50d06c735a7775d3_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6829\n",
      "Epoch [2], Iter [91/100] Loss: 0.6249\n",
      "Epoch [3], Iter [91/100] Loss: 0.6331\n",
      "Epoch [4], Iter [91/100] Loss: 0.6435\n",
      "Epoch [5], Iter [91/100] Loss: 0.6288\n",
      "Epoch [6], Iter [91/100] Loss: 0.6196\n",
      "Epoch [7], Iter [91/100] Loss: 0.5878\n",
      "Epoch [8], Iter [91/100] Loss: 0.6106\n",
      "Epoch [9], Iter [91/100] Loss: 0.5843\n",
      "Epoch [10], Iter [91/100] Loss: 0.5644\n",
      "Epoch [11], Iter [91/100] Loss: 0.5527\n",
      "Epoch [12], Iter [91/100] Loss: 0.5883\n",
      "Epoch [13], Iter [91/100] Loss: 0.5439\n",
      "Epoch [14], Iter [91/100] Loss: 0.5373\n",
      "Epoch [15], Iter [91/100] Loss: 0.5420\n",
      "Epoch [16], Iter [91/100] Loss: 0.5243\n",
      "Epoch [17], Iter [91/100] Loss: 0.5216\n",
      "Epoch [18], Iter [91/100] Loss: 0.5054\n",
      "Epoch [19], Iter [91/100] Loss: 0.4968\n",
      "Test MSE: 0.6341349482536316\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c74774-0x18d645a6b8e0d970_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6867\n",
      "Epoch [2], Iter [91/100] Loss: 0.6625\n",
      "Epoch [3], Iter [91/100] Loss: 0.6518\n",
      "Epoch [4], Iter [91/100] Loss: 0.6176\n",
      "Epoch [5], Iter [91/100] Loss: 0.6208\n",
      "Epoch [6], Iter [91/100] Loss: 0.6062\n",
      "Epoch [7], Iter [91/100] Loss: 0.5716\n",
      "Epoch [8], Iter [91/100] Loss: 0.5888\n",
      "Epoch [9], Iter [91/100] Loss: 0.5953\n",
      "Epoch [10], Iter [91/100] Loss: 0.5971\n",
      "Epoch [11], Iter [91/100] Loss: 0.5695\n",
      "Epoch [12], Iter [91/100] Loss: 0.5828\n",
      "Epoch [13], Iter [91/100] Loss: 0.5756\n",
      "Epoch [14], Iter [91/100] Loss: 0.5583\n",
      "Epoch [15], Iter [91/100] Loss: 0.5459\n",
      "Epoch [16], Iter [91/100] Loss: 0.5310\n",
      "Epoch [17], Iter [91/100] Loss: 0.5420\n",
      "Epoch [18], Iter [91/100] Loss: 0.5451\n",
      "Epoch [19], Iter [91/100] Loss: 0.5302\n",
      "Epoch [20], Iter [91/100] Loss: 0.5036\n",
      "Epoch [21], Iter [91/100] Loss: 0.4887\n",
      "Test MSE: 0.6470401883125305\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c74774-0x569c12d1107540e1_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7675\n",
      "Epoch [2], Iter [91/100] Loss: 0.6716\n",
      "Epoch [3], Iter [91/100] Loss: 0.6483\n",
      "Epoch [4], Iter [91/100] Loss: 0.6798\n",
      "Epoch [5], Iter [91/100] Loss: 0.6872\n",
      "Epoch [6], Iter [91/100] Loss: 0.6320\n",
      "Epoch [7], Iter [91/100] Loss: 0.7174\n",
      "Epoch [8], Iter [91/100] Loss: 0.5928\n",
      "Epoch [9], Iter [91/100] Loss: 0.5832\n",
      "Epoch [10], Iter [91/100] Loss: 0.5506\n",
      "Epoch [11], Iter [91/100] Loss: 0.5542\n",
      "Epoch [12], Iter [91/100] Loss: 0.5569\n",
      "Epoch [13], Iter [91/100] Loss: 0.5618\n",
      "Epoch [14], Iter [91/100] Loss: 0.5952\n",
      "Epoch [15], Iter [91/100] Loss: 0.5529\n",
      "Epoch [16], Iter [91/100] Loss: 0.5483\n",
      "Epoch [17], Iter [91/100] Loss: 0.5297\n",
      "Epoch [18], Iter [91/100] Loss: 0.5143\n",
      "Epoch [19], Iter [91/100] Loss: 0.5142\n",
      "Epoch [20], Iter [91/100] Loss: 0.5077\n",
      "Test MSE: 0.6281326413154602\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c74774-0x7e523672a25ebe0e_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7242\n",
      "Epoch [2], Iter [91/100] Loss: 0.7126\n",
      "Epoch [3], Iter [91/100] Loss: 0.6336\n",
      "Epoch [4], Iter [91/100] Loss: 0.6497\n",
      "Epoch [5], Iter [91/100] Loss: 0.6508\n",
      "Epoch [6], Iter [91/100] Loss: 0.6123\n",
      "Epoch [7], Iter [91/100] Loss: 0.5995\n",
      "Epoch [8], Iter [91/100] Loss: 0.6366\n",
      "Epoch [9], Iter [91/100] Loss: 0.6058\n",
      "Epoch [10], Iter [91/100] Loss: 0.5717\n",
      "Epoch [11], Iter [91/100] Loss: 0.5620\n",
      "Epoch [12], Iter [91/100] Loss: 0.5678\n",
      "Epoch [13], Iter [91/100] Loss: 0.5441\n",
      "Epoch [14], Iter [91/100] Loss: 0.5467\n",
      "Epoch [15], Iter [91/100] Loss: 0.5493\n",
      "Epoch [16], Iter [91/100] Loss: 0.5589\n",
      "Epoch [17], Iter [91/100] Loss: 0.5361\n",
      "Epoch [18], Iter [91/100] Loss: 0.5484\n",
      "Epoch [19], Iter [91/100] Loss: 0.5128\n",
      "Epoch [20], Iter [91/100] Loss: 0.4998\n",
      "Test MSE: 0.6336514353752136\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c747740x1aa2c36fa46b7092_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7071\n",
      "Epoch [2], Iter [91/100] Loss: 0.6411\n",
      "Epoch [3], Iter [91/100] Loss: 0.6449\n",
      "Epoch [4], Iter [91/100] Loss: 0.6353\n",
      "Epoch [5], Iter [91/100] Loss: 0.6078\n",
      "Epoch [6], Iter [91/100] Loss: 0.6260\n",
      "Epoch [7], Iter [91/100] Loss: 0.6426\n",
      "Epoch [8], Iter [91/100] Loss: 0.5914\n",
      "Epoch [9], Iter [91/100] Loss: 0.6071\n",
      "Epoch [10], Iter [91/100] Loss: 0.5519\n",
      "Epoch [11], Iter [91/100] Loss: 0.5440\n",
      "Epoch [12], Iter [91/100] Loss: 0.5403\n",
      "Epoch [13], Iter [91/100] Loss: 0.5491\n",
      "Epoch [14], Iter [91/100] Loss: 0.5581\n",
      "Epoch [15], Iter [91/100] Loss: 0.5547\n",
      "Epoch [16], Iter [91/100] Loss: 0.5297\n",
      "Test MSE: 0.6380564570426941\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c747740xc46eec262af81f4_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6931\n",
      "Epoch [2], Iter [91/100] Loss: 0.6399\n",
      "Epoch [3], Iter [91/100] Loss: 0.6295\n",
      "Epoch [4], Iter [91/100] Loss: 0.6599\n",
      "Epoch [5], Iter [91/100] Loss: 0.6292\n",
      "Epoch [6], Iter [91/100] Loss: 0.6363\n",
      "Epoch [7], Iter [91/100] Loss: 0.6072\n",
      "Epoch [8], Iter [91/100] Loss: 0.5979\n",
      "Epoch [9], Iter [91/100] Loss: 0.5449\n",
      "Epoch [10], Iter [91/100] Loss: 0.6014\n",
      "Epoch [11], Iter [91/100] Loss: 0.5710\n",
      "Epoch [12], Iter [91/100] Loss: 0.6514\n",
      "Epoch [13], Iter [91/100] Loss: 0.5591\n",
      "Epoch [14], Iter [91/100] Loss: 0.5427\n",
      "Epoch [15], Iter [91/100] Loss: 0.5642\n",
      "Epoch [16], Iter [91/100] Loss: 0.5487\n",
      "Epoch [17], Iter [91/100] Loss: 0.5214\n",
      "Epoch [18], Iter [91/100] Loss: 0.5056\n",
      "Epoch [19], Iter [91/100] Loss: 0.5051\n",
      "Epoch [20], Iter [91/100] Loss: 0.4881\n",
      "Test MSE: 0.6342288255691528\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c747740x36c7242442056bf9_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6923\n",
      "Epoch [2], Iter [91/100] Loss: 0.7417\n",
      "Epoch [3], Iter [91/100] Loss: 0.6824\n",
      "Epoch [4], Iter [91/100] Loss: 0.6264\n",
      "Epoch [5], Iter [91/100] Loss: 0.6650\n",
      "Epoch [6], Iter [91/100] Loss: 0.6299\n",
      "Epoch [7], Iter [91/100] Loss: 0.6115\n",
      "Epoch [8], Iter [91/100] Loss: 0.5718\n",
      "Epoch [9], Iter [91/100] Loss: 0.5809\n",
      "Epoch [10], Iter [91/100] Loss: 0.6106\n",
      "Epoch [11], Iter [91/100] Loss: 0.5773\n",
      "Epoch [12], Iter [91/100] Loss: 0.5590\n",
      "Epoch [13], Iter [91/100] Loss: 0.5519\n",
      "Epoch [14], Iter [91/100] Loss: 0.5709\n",
      "Epoch [15], Iter [91/100] Loss: 0.5410\n",
      "Epoch [16], Iter [91/100] Loss: 0.5294\n",
      "Epoch [17], Iter [91/100] Loss: 0.5124\n",
      "Test MSE: 0.6416376829147339\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c74774-0x54df69ca9c3e68de_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7062\n",
      "Epoch [2], Iter [91/100] Loss: 0.6560\n",
      "Epoch [3], Iter [91/100] Loss: 0.6403\n",
      "Epoch [4], Iter [91/100] Loss: 0.6484\n",
      "Epoch [5], Iter [91/100] Loss: 0.6215\n",
      "Epoch [6], Iter [91/100] Loss: 0.6069\n",
      "Epoch [7], Iter [91/100] Loss: 0.6632\n",
      "Epoch [8], Iter [91/100] Loss: 0.5965\n",
      "Epoch [9], Iter [91/100] Loss: 0.5528\n",
      "Epoch [10], Iter [91/100] Loss: 0.5783\n",
      "Epoch [11], Iter [91/100] Loss: 0.5927\n",
      "Epoch [12], Iter [91/100] Loss: 0.5624\n",
      "Epoch [13], Iter [91/100] Loss: 0.5347\n",
      "Epoch [14], Iter [91/100] Loss: 0.5604\n",
      "Epoch [15], Iter [91/100] Loss: 0.5208\n",
      "Epoch [16], Iter [91/100] Loss: 0.5429\n",
      "Epoch [17], Iter [91/100] Loss: 0.5422\n",
      "Test MSE: 0.6362040638923645\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\0x2301de6f28c747740x6f9658270b87b93c_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6561\n",
      "Epoch [2], Iter [91/100] Loss: 0.6612\n",
      "Epoch [3], Iter [91/100] Loss: 0.6395\n",
      "Epoch [4], Iter [91/100] Loss: 0.6275\n",
      "Epoch [5], Iter [91/100] Loss: 0.6332\n",
      "Epoch [6], Iter [91/100] Loss: 0.6287\n",
      "Epoch [7], Iter [91/100] Loss: 0.6026\n",
      "Epoch [8], Iter [91/100] Loss: 0.5916\n",
      "Epoch [9], Iter [91/100] Loss: 0.6020\n",
      "Epoch [10], Iter [91/100] Loss: 0.5788\n",
      "Epoch [11], Iter [91/100] Loss: 0.6064\n",
      "Epoch [12], Iter [91/100] Loss: 0.5894\n",
      "Epoch [13], Iter [91/100] Loss: 0.5664\n",
      "Epoch [14], Iter [91/100] Loss: 0.5531\n",
      "Epoch [15], Iter [91/100] Loss: 0.5456\n",
      "Epoch [16], Iter [91/100] Loss: 0.5554\n",
      "Epoch [17], Iter [91/100] Loss: 0.5262\n",
      "Test MSE: 0.6360149383544922\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder, use_tensorboard=True)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd59a25",
   "metadata": {},
   "source": [
    "### slp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c9d6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "312560f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7972\n",
      "Epoch [2], Iter [91/100] Loss: 0.7728\n",
      "Epoch [3], Iter [91/100] Loss: 0.6955\n",
      "Epoch [4], Iter [91/100] Loss: 0.7176\n",
      "Epoch [5], Iter [91/100] Loss: 0.7006\n",
      "Epoch [6], Iter [91/100] Loss: 0.6821\n",
      "Epoch [7], Iter [91/100] Loss: 0.6410\n",
      "Epoch [8], Iter [91/100] Loss: 0.6709\n",
      "Epoch [9], Iter [91/100] Loss: 0.6200\n",
      "Epoch [10], Iter [91/100] Loss: 0.6229\n",
      "Epoch [11], Iter [91/100] Loss: 0.6269\n",
      "Epoch [12], Iter [91/100] Loss: 0.6125\n",
      "Epoch [13], Iter [91/100] Loss: 0.5970\n",
      "Epoch [14], Iter [91/100] Loss: 0.6353\n",
      "Test MSE: 0.690142810344696\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8662\n",
      "Epoch [2], Iter [91/100] Loss: 0.7569\n",
      "Epoch [3], Iter [91/100] Loss: 0.7135\n",
      "Epoch [4], Iter [91/100] Loss: 0.6846\n",
      "Epoch [5], Iter [91/100] Loss: 0.6563\n",
      "Epoch [6], Iter [91/100] Loss: 0.6420\n",
      "Epoch [7], Iter [91/100] Loss: 0.6691\n",
      "Epoch [8], Iter [91/100] Loss: 0.6710\n",
      "Epoch [9], Iter [91/100] Loss: 0.6190\n",
      "Epoch [10], Iter [91/100] Loss: 0.6374\n",
      "Epoch [11], Iter [91/100] Loss: 0.6205\n",
      "Epoch [12], Iter [91/100] Loss: 0.6337\n",
      "Epoch [13], Iter [91/100] Loss: 0.6281\n",
      "Epoch [14], Iter [91/100] Loss: 0.6040\n",
      "Epoch [15], Iter [91/100] Loss: 0.6127\n",
      "Epoch [16], Iter [91/100] Loss: 0.5712\n",
      "Epoch [17], Iter [91/100] Loss: 0.5845\n",
      "Epoch [18], Iter [91/100] Loss: 0.5606\n",
      "Test MSE: 0.6912035942077637\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8405\n",
      "Epoch [2], Iter [91/100] Loss: 0.7357\n",
      "Epoch [3], Iter [91/100] Loss: 0.6831\n",
      "Epoch [4], Iter [91/100] Loss: 0.6911\n",
      "Epoch [5], Iter [91/100] Loss: 0.6974\n",
      "Epoch [6], Iter [91/100] Loss: 0.6750\n",
      "Epoch [7], Iter [91/100] Loss: 0.6319\n",
      "Epoch [8], Iter [91/100] Loss: 0.7064\n",
      "Epoch [9], Iter [91/100] Loss: 0.6333\n",
      "Epoch [10], Iter [91/100] Loss: 0.7226\n",
      "Epoch [11], Iter [91/100] Loss: 0.6092\n",
      "Epoch [12], Iter [91/100] Loss: 0.6471\n",
      "Epoch [13], Iter [91/100] Loss: 0.7023\n",
      "Epoch [14], Iter [91/100] Loss: 0.6106\n",
      "Epoch [15], Iter [91/100] Loss: 0.5871\n",
      "Epoch [16], Iter [91/100] Loss: 0.6032\n",
      "Epoch [17], Iter [91/100] Loss: 0.5572\n",
      "Epoch [18], Iter [91/100] Loss: 0.5637\n",
      "Epoch [19], Iter [91/100] Loss: 0.5673\n",
      "Epoch [20], Iter [91/100] Loss: 0.5176\n",
      "Epoch [21], Iter [91/100] Loss: 0.5522\n",
      "Test MSE: 0.6819645166397095\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7825\n",
      "Epoch [2], Iter [91/100] Loss: 0.7232\n",
      "Epoch [3], Iter [91/100] Loss: 0.7368\n",
      "Epoch [4], Iter [91/100] Loss: 0.7059\n",
      "Epoch [5], Iter [91/100] Loss: 0.6776\n",
      "Epoch [6], Iter [91/100] Loss: 0.6782\n",
      "Epoch [7], Iter [91/100] Loss: 0.7976\n",
      "Epoch [8], Iter [91/100] Loss: 0.6243\n",
      "Epoch [9], Iter [91/100] Loss: 0.6679\n",
      "Epoch [10], Iter [91/100] Loss: 0.6697\n",
      "Epoch [11], Iter [91/100] Loss: 0.5904\n",
      "Epoch [12], Iter [91/100] Loss: 0.6351\n",
      "Epoch [13], Iter [91/100] Loss: 0.6166\n",
      "Epoch [14], Iter [91/100] Loss: 0.6065\n",
      "Epoch [15], Iter [91/100] Loss: 0.5946\n",
      "Epoch [16], Iter [91/100] Loss: 0.5868\n",
      "Epoch [17], Iter [91/100] Loss: 0.5611\n",
      "Epoch [18], Iter [91/100] Loss: 0.5406\n",
      "Epoch [19], Iter [91/100] Loss: 0.5519\n",
      "Test MSE: 0.6839588284492493\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7549\n",
      "Epoch [2], Iter [91/100] Loss: 0.7949\n",
      "Epoch [3], Iter [91/100] Loss: 0.7131\n",
      "Epoch [4], Iter [91/100] Loss: 0.6758\n",
      "Epoch [5], Iter [91/100] Loss: 0.6688\n",
      "Epoch [6], Iter [91/100] Loss: 0.6835\n",
      "Epoch [7], Iter [91/100] Loss: 0.6141\n",
      "Epoch [8], Iter [91/100] Loss: 0.6574\n",
      "Epoch [9], Iter [91/100] Loss: 0.6297\n",
      "Epoch [10], Iter [91/100] Loss: 0.6103\n",
      "Epoch [11], Iter [91/100] Loss: 0.6192\n",
      "Epoch [12], Iter [91/100] Loss: 0.6416\n",
      "Epoch [13], Iter [91/100] Loss: 0.6030\n",
      "Epoch [14], Iter [91/100] Loss: 0.5878\n",
      "Epoch [15], Iter [91/100] Loss: 0.5795\n",
      "Epoch [16], Iter [91/100] Loss: 0.5961\n",
      "Epoch [17], Iter [91/100] Loss: 0.5707\n",
      "Epoch [18], Iter [91/100] Loss: 0.5631\n",
      "Epoch [19], Iter [91/100] Loss: 0.5517\n",
      "Epoch [20], Iter [91/100] Loss: 0.5459\n",
      "Test MSE: 0.6793809533119202\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8185\n",
      "Epoch [2], Iter [91/100] Loss: 0.6933\n",
      "Epoch [3], Iter [91/100] Loss: 0.7450\n",
      "Epoch [4], Iter [91/100] Loss: 0.7048\n",
      "Epoch [5], Iter [91/100] Loss: 0.7405\n",
      "Epoch [6], Iter [91/100] Loss: 0.6391\n",
      "Epoch [7], Iter [91/100] Loss: 0.6482\n",
      "Epoch [8], Iter [91/100] Loss: 0.6606\n",
      "Epoch [9], Iter [91/100] Loss: 0.6148\n",
      "Epoch [10], Iter [91/100] Loss: 0.6413\n",
      "Epoch [11], Iter [91/100] Loss: 0.5936\n",
      "Epoch [12], Iter [91/100] Loss: 0.6316\n",
      "Epoch [13], Iter [91/100] Loss: 0.6011\n",
      "Epoch [14], Iter [91/100] Loss: 0.5901\n",
      "Epoch [15], Iter [91/100] Loss: 0.5923\n",
      "Epoch [16], Iter [91/100] Loss: 0.5954\n",
      "Test MSE: 0.6914368867874146\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7893\n",
      "Epoch [2], Iter [91/100] Loss: 0.7676\n",
      "Epoch [3], Iter [91/100] Loss: 0.7535\n",
      "Epoch [4], Iter [91/100] Loss: 0.7267\n",
      "Epoch [5], Iter [91/100] Loss: 0.7845\n",
      "Epoch [6], Iter [91/100] Loss: 0.7341\n",
      "Epoch [7], Iter [91/100] Loss: 0.6809\n",
      "Epoch [8], Iter [91/100] Loss: 0.6393\n",
      "Epoch [9], Iter [91/100] Loss: 0.6548\n",
      "Epoch [10], Iter [91/100] Loss: 0.6533\n",
      "Epoch [11], Iter [91/100] Loss: 0.6120\n",
      "Epoch [12], Iter [91/100] Loss: 0.6061\n",
      "Epoch [13], Iter [91/100] Loss: 0.5846\n",
      "Epoch [14], Iter [91/100] Loss: 0.5996\n",
      "Epoch [15], Iter [91/100] Loss: 0.6106\n",
      "Epoch [16], Iter [91/100] Loss: 0.5878\n",
      "Epoch [17], Iter [91/100] Loss: 0.5854\n",
      "Epoch [18], Iter [91/100] Loss: 0.6102\n",
      "Test MSE: 0.6803104281425476\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8262\n",
      "Epoch [2], Iter [91/100] Loss: 0.7383\n",
      "Epoch [3], Iter [91/100] Loss: 0.7074\n",
      "Epoch [4], Iter [91/100] Loss: 0.6757\n",
      "Epoch [5], Iter [91/100] Loss: 0.7063\n",
      "Epoch [6], Iter [91/100] Loss: 0.6708\n",
      "Epoch [7], Iter [91/100] Loss: 0.7013\n",
      "Epoch [8], Iter [91/100] Loss: 0.6365\n",
      "Epoch [9], Iter [91/100] Loss: 0.7095\n",
      "Epoch [10], Iter [91/100] Loss: 0.6580\n",
      "Epoch [11], Iter [91/100] Loss: 0.6520\n",
      "Epoch [12], Iter [91/100] Loss: 0.6346\n",
      "Epoch [13], Iter [91/100] Loss: 0.6035\n",
      "Epoch [14], Iter [91/100] Loss: 0.6002\n",
      "Epoch [15], Iter [91/100] Loss: 0.6054\n",
      "Epoch [16], Iter [91/100] Loss: 0.5797\n",
      "Epoch [17], Iter [91/100] Loss: 0.5795\n",
      "Epoch [18], Iter [91/100] Loss: 0.5798\n",
      "Epoch [19], Iter [91/100] Loss: 0.5671\n",
      "Epoch [20], Iter [91/100] Loss: 0.5484\n",
      "Epoch [21], Iter [91/100] Loss: 0.5662\n",
      "Epoch [22], Iter [91/100] Loss: 0.5533\n",
      "Epoch [23], Iter [91/100] Loss: 0.5465\n",
      "Epoch [24], Iter [91/100] Loss: 0.5390\n",
      "Test MSE: 0.6763361692428589\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7899\n",
      "Epoch [2], Iter [91/100] Loss: 0.7357\n",
      "Epoch [3], Iter [91/100] Loss: 0.6920\n",
      "Epoch [4], Iter [91/100] Loss: 0.6816\n",
      "Epoch [5], Iter [91/100] Loss: 0.7201\n",
      "Epoch [6], Iter [91/100] Loss: 0.6475\n",
      "Epoch [7], Iter [91/100] Loss: 0.6860\n",
      "Epoch [8], Iter [91/100] Loss: 0.6596\n",
      "Epoch [9], Iter [91/100] Loss: 0.6425\n",
      "Epoch [10], Iter [91/100] Loss: 0.6282\n",
      "Epoch [11], Iter [91/100] Loss: 0.6741\n",
      "Epoch [12], Iter [91/100] Loss: 0.6294\n",
      "Epoch [13], Iter [91/100] Loss: 0.6045\n",
      "Epoch [14], Iter [91/100] Loss: 0.6328\n",
      "Epoch [15], Iter [91/100] Loss: 0.5898\n",
      "Epoch [16], Iter [91/100] Loss: 0.5911\n",
      "Epoch [17], Iter [91/100] Loss: 0.5796\n",
      "Epoch [18], Iter [91/100] Loss: 0.5758\n",
      "Epoch [19], Iter [91/100] Loss: 0.5858\n",
      "Test MSE: 0.6773972511291504\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8298\n",
      "Epoch [2], Iter [91/100] Loss: 0.7285\n",
      "Epoch [3], Iter [91/100] Loss: 0.7077\n",
      "Epoch [4], Iter [91/100] Loss: 0.7057\n",
      "Epoch [5], Iter [91/100] Loss: 0.6664\n",
      "Epoch [6], Iter [91/100] Loss: 0.6332\n",
      "Epoch [7], Iter [91/100] Loss: 0.6483\n",
      "Epoch [8], Iter [91/100] Loss: 0.6224\n",
      "Epoch [9], Iter [91/100] Loss: 0.5888\n",
      "Epoch [10], Iter [91/100] Loss: 0.6284\n",
      "Epoch [11], Iter [91/100] Loss: 0.6235\n",
      "Epoch [12], Iter [91/100] Loss: 0.5982\n",
      "Epoch [13], Iter [91/100] Loss: 0.5986\n",
      "Epoch [14], Iter [91/100] Loss: 0.5948\n",
      "Epoch [15], Iter [91/100] Loss: 0.6021\n",
      "Epoch [16], Iter [91/100] Loss: 0.6106\n",
      "Test MSE: 0.6925275325775146\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c2dba",
   "metadata": {},
   "source": [
    "### slp, tas, precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6767544",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46c45f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6385\n",
      "Epoch [2], Iter [91/100] Loss: 0.5652\n",
      "Epoch [3], Iter [91/100] Loss: 0.5906\n",
      "Epoch [4], Iter [91/100] Loss: 0.5811\n",
      "Epoch [5], Iter [91/100] Loss: 0.5684\n",
      "Epoch [6], Iter [91/100] Loss: 0.5476\n",
      "Epoch [7], Iter [91/100] Loss: 0.5452\n",
      "Epoch [8], Iter [91/100] Loss: 0.5355\n",
      "Epoch [9], Iter [91/100] Loss: 0.5087\n",
      "Epoch [10], Iter [91/100] Loss: 0.5002\n",
      "Epoch [11], Iter [91/100] Loss: 0.4978\n",
      "Epoch [12], Iter [91/100] Loss: 0.4712\n",
      "Epoch [13], Iter [91/100] Loss: 0.5864\n",
      "Epoch [14], Iter [91/100] Loss: 0.4889\n",
      "Epoch [15], Iter [91/100] Loss: 0.4828\n",
      "Epoch [16], Iter [91/100] Loss: 0.4637\n",
      "Epoch [17], Iter [91/100] Loss: 0.4671\n",
      "Test MSE: 0.5627509951591492\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6374\n",
      "Epoch [2], Iter [91/100] Loss: 0.5831\n",
      "Epoch [3], Iter [91/100] Loss: 0.5820\n",
      "Epoch [4], Iter [91/100] Loss: 0.5484\n",
      "Epoch [5], Iter [91/100] Loss: 0.5488\n",
      "Epoch [6], Iter [91/100] Loss: 0.5402\n",
      "Epoch [7], Iter [91/100] Loss: 0.5474\n",
      "Epoch [8], Iter [91/100] Loss: 0.5518\n",
      "Epoch [9], Iter [91/100] Loss: 0.5387\n",
      "Epoch [10], Iter [91/100] Loss: 0.5134\n",
      "Epoch [11], Iter [91/100] Loss: 0.5184\n",
      "Epoch [12], Iter [91/100] Loss: 0.5096\n",
      "Epoch [13], Iter [91/100] Loss: 0.5648\n",
      "Epoch [14], Iter [91/100] Loss: 0.5031\n",
      "Epoch [15], Iter [91/100] Loss: 0.4840\n",
      "Epoch [16], Iter [91/100] Loss: 0.4833\n",
      "Epoch [17], Iter [91/100] Loss: 0.4896\n",
      "Epoch [18], Iter [91/100] Loss: 0.4550\n",
      "Epoch [19], Iter [91/100] Loss: 0.4573\n",
      "Test MSE: 0.5617765784263611\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6523\n",
      "Epoch [2], Iter [91/100] Loss: 0.5948\n",
      "Epoch [3], Iter [91/100] Loss: 0.6060\n",
      "Epoch [4], Iter [91/100] Loss: 0.6014\n",
      "Epoch [5], Iter [91/100] Loss: 0.5577\n",
      "Epoch [6], Iter [91/100] Loss: 0.5070\n",
      "Epoch [7], Iter [91/100] Loss: 0.5360\n",
      "Epoch [8], Iter [91/100] Loss: 0.5292\n",
      "Epoch [9], Iter [91/100] Loss: 0.5279\n",
      "Epoch [10], Iter [91/100] Loss: 0.5190\n",
      "Epoch [11], Iter [91/100] Loss: 0.5248\n",
      "Epoch [12], Iter [91/100] Loss: 0.4779\n",
      "Epoch [13], Iter [91/100] Loss: 0.4954\n",
      "Epoch [14], Iter [91/100] Loss: 0.4927\n",
      "Epoch [15], Iter [91/100] Loss: 0.4723\n",
      "Epoch [16], Iter [91/100] Loss: 0.4699\n",
      "Epoch [17], Iter [91/100] Loss: 0.4768\n",
      "Test MSE: 0.5538816452026367\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6415\n",
      "Epoch [2], Iter [91/100] Loss: 0.5895\n",
      "Epoch [3], Iter [91/100] Loss: 0.5802\n",
      "Epoch [4], Iter [91/100] Loss: 0.5604\n",
      "Epoch [5], Iter [91/100] Loss: 0.5894\n",
      "Epoch [6], Iter [91/100] Loss: 0.5578\n",
      "Epoch [7], Iter [91/100] Loss: 0.5291\n",
      "Epoch [8], Iter [91/100] Loss: 0.5411\n",
      "Epoch [9], Iter [91/100] Loss: 0.5213\n",
      "Epoch [10], Iter [91/100] Loss: 0.5141\n",
      "Epoch [11], Iter [91/100] Loss: 0.5404\n",
      "Epoch [12], Iter [91/100] Loss: 0.5271\n",
      "Epoch [13], Iter [91/100] Loss: 0.5707\n",
      "Epoch [14], Iter [91/100] Loss: 0.4883\n",
      "Epoch [15], Iter [91/100] Loss: 0.4848\n",
      "Epoch [16], Iter [91/100] Loss: 0.4667\n",
      "Epoch [17], Iter [91/100] Loss: 0.4705\n",
      "Test MSE: 0.5518606901168823\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6624\n",
      "Epoch [2], Iter [91/100] Loss: 0.5961\n",
      "Epoch [3], Iter [91/100] Loss: 0.6417\n",
      "Epoch [4], Iter [91/100] Loss: 0.5565\n",
      "Epoch [5], Iter [91/100] Loss: 0.5532\n",
      "Epoch [6], Iter [91/100] Loss: 0.5871\n",
      "Epoch [7], Iter [91/100] Loss: 0.5220\n",
      "Epoch [8], Iter [91/100] Loss: 0.5299\n",
      "Epoch [9], Iter [91/100] Loss: 0.5044\n",
      "Epoch [10], Iter [91/100] Loss: 0.4945\n",
      "Epoch [11], Iter [91/100] Loss: 0.5033\n",
      "Epoch [12], Iter [91/100] Loss: 0.5294\n",
      "Epoch [13], Iter [91/100] Loss: 0.5033\n",
      "Epoch [14], Iter [91/100] Loss: 0.5006\n",
      "Epoch [15], Iter [91/100] Loss: 0.4823\n",
      "Epoch [16], Iter [91/100] Loss: 0.5024\n",
      "Epoch [17], Iter [91/100] Loss: 0.5035\n",
      "Epoch [18], Iter [91/100] Loss: 0.4715\n",
      "Epoch [19], Iter [91/100] Loss: 0.4694\n",
      "Epoch [20], Iter [91/100] Loss: 0.4738\n",
      "Epoch [21], Iter [91/100] Loss: 0.4572\n",
      "Epoch [22], Iter [91/100] Loss: 0.4461\n",
      "Epoch [23], Iter [91/100] Loss: 0.4349\n",
      "Test MSE: 0.5553189516067505\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6185\n",
      "Epoch [2], Iter [91/100] Loss: 0.6398\n",
      "Epoch [3], Iter [91/100] Loss: 0.5719\n",
      "Epoch [4], Iter [91/100] Loss: 0.5692\n",
      "Epoch [5], Iter [91/100] Loss: 0.5740\n",
      "Epoch [6], Iter [91/100] Loss: 0.5539\n",
      "Epoch [7], Iter [91/100] Loss: 0.5385\n",
      "Epoch [8], Iter [91/100] Loss: 0.5160\n",
      "Epoch [9], Iter [91/100] Loss: 0.5219\n",
      "Epoch [10], Iter [91/100] Loss: 0.4927\n",
      "Epoch [11], Iter [91/100] Loss: 0.5169\n",
      "Epoch [12], Iter [91/100] Loss: 0.5010\n",
      "Epoch [13], Iter [91/100] Loss: 0.5039\n",
      "Epoch [14], Iter [91/100] Loss: 0.4836\n",
      "Epoch [15], Iter [91/100] Loss: 0.4940\n",
      "Epoch [16], Iter [91/100] Loss: 0.4863\n",
      "Epoch [17], Iter [91/100] Loss: 0.4531\n",
      "Epoch [18], Iter [91/100] Loss: 0.4600\n",
      "Epoch [19], Iter [91/100] Loss: 0.4665\n",
      "Test MSE: 0.5559583902359009\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7014\n",
      "Epoch [2], Iter [91/100] Loss: 0.5982\n",
      "Epoch [3], Iter [91/100] Loss: 0.5827\n",
      "Epoch [4], Iter [91/100] Loss: 0.5588\n",
      "Epoch [5], Iter [91/100] Loss: 0.5589\n",
      "Epoch [6], Iter [91/100] Loss: 0.5395\n",
      "Epoch [7], Iter [91/100] Loss: 0.5423\n",
      "Epoch [8], Iter [91/100] Loss: 0.5439\n",
      "Epoch [9], Iter [91/100] Loss: 0.5232\n",
      "Epoch [10], Iter [91/100] Loss: 0.5083\n",
      "Epoch [11], Iter [91/100] Loss: 0.5145\n",
      "Epoch [12], Iter [91/100] Loss: 0.4886\n",
      "Epoch [13], Iter [91/100] Loss: 0.4895\n",
      "Epoch [14], Iter [91/100] Loss: 0.4923\n",
      "Epoch [15], Iter [91/100] Loss: 0.4657\n",
      "Epoch [16], Iter [91/100] Loss: 0.4660\n",
      "Test MSE: 0.5596743822097778\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6405\n",
      "Epoch [2], Iter [91/100] Loss: 0.5703\n",
      "Epoch [3], Iter [91/100] Loss: 0.5980\n",
      "Epoch [4], Iter [91/100] Loss: 0.6209\n",
      "Epoch [5], Iter [91/100] Loss: 0.5654\n",
      "Epoch [6], Iter [91/100] Loss: 0.5463\n",
      "Epoch [7], Iter [91/100] Loss: 0.5062\n",
      "Epoch [8], Iter [91/100] Loss: 0.5622\n",
      "Epoch [9], Iter [91/100] Loss: 0.5278\n",
      "Epoch [10], Iter [91/100] Loss: 0.5000\n",
      "Epoch [11], Iter [91/100] Loss: 0.4742\n",
      "Epoch [12], Iter [91/100] Loss: 0.4913\n",
      "Epoch [13], Iter [91/100] Loss: 0.5391\n",
      "Epoch [14], Iter [91/100] Loss: 0.4933\n",
      "Epoch [15], Iter [91/100] Loss: 0.4586\n",
      "Epoch [16], Iter [91/100] Loss: 0.4661\n",
      "Epoch [17], Iter [91/100] Loss: 0.4839\n",
      "Epoch [18], Iter [91/100] Loss: 0.4751\n",
      "Test MSE: 0.5492035150527954\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6188\n",
      "Epoch [2], Iter [91/100] Loss: 0.6041\n",
      "Epoch [3], Iter [91/100] Loss: 0.5922\n",
      "Epoch [4], Iter [91/100] Loss: 0.5872\n",
      "Epoch [5], Iter [91/100] Loss: 0.5307\n",
      "Epoch [6], Iter [91/100] Loss: 0.5341\n",
      "Epoch [7], Iter [91/100] Loss: 0.5605\n",
      "Epoch [8], Iter [91/100] Loss: 0.5399\n",
      "Epoch [9], Iter [91/100] Loss: 0.5338\n",
      "Epoch [10], Iter [91/100] Loss: 0.5242\n",
      "Epoch [11], Iter [91/100] Loss: 0.5124\n",
      "Epoch [12], Iter [91/100] Loss: 0.5049\n",
      "Epoch [13], Iter [91/100] Loss: 0.5059\n",
      "Epoch [14], Iter [91/100] Loss: 0.4950\n",
      "Epoch [15], Iter [91/100] Loss: 0.4892\n",
      "Epoch [16], Iter [91/100] Loss: 0.4743\n",
      "Epoch [17], Iter [91/100] Loss: 0.4980\n",
      "Epoch [18], Iter [91/100] Loss: 0.4802\n",
      "Test MSE: 0.5523160696029663\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6524\n",
      "Epoch [2], Iter [91/100] Loss: 0.5741\n",
      "Epoch [3], Iter [91/100] Loss: 0.5744\n",
      "Epoch [4], Iter [91/100] Loss: 0.5734\n",
      "Epoch [5], Iter [91/100] Loss: 0.5294\n",
      "Epoch [6], Iter [91/100] Loss: 0.5344\n",
      "Epoch [7], Iter [91/100] Loss: 0.5361\n",
      "Epoch [8], Iter [91/100] Loss: 0.5184\n",
      "Epoch [9], Iter [91/100] Loss: 0.5230\n",
      "Epoch [10], Iter [91/100] Loss: 0.5765\n",
      "Epoch [11], Iter [91/100] Loss: 0.5099\n",
      "Epoch [12], Iter [91/100] Loss: 0.5256\n",
      "Epoch [13], Iter [91/100] Loss: 0.5045\n",
      "Epoch [14], Iter [91/100] Loss: 0.5109\n",
      "Epoch [15], Iter [91/100] Loss: 0.4810\n",
      "Epoch [16], Iter [91/100] Loss: 0.4705\n",
      "Epoch [17], Iter [91/100] Loss: 0.4441\n",
      "Epoch [18], Iter [91/100] Loss: 0.4606\n",
      "Test MSE: 0.5629997253417969\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e44dc4",
   "metadata": {},
   "source": [
    "### precip, tas, orogrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44f10ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\",\n",
    "                                \"oro\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"oro\": [\"ht\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Global\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30178260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 1.0857\n",
      "Epoch [2], Iter [91/100] Loss: 1.0446\n",
      "Epoch [3], Iter [91/100] Loss: 0.8483\n",
      "Epoch [4], Iter [91/100] Loss: 0.7063\n",
      "Epoch [5], Iter [91/100] Loss: 0.6621\n",
      "Epoch [6], Iter [91/100] Loss: 0.6080\n",
      "Epoch [7], Iter [91/100] Loss: 0.5939\n",
      "Epoch [8], Iter [91/100] Loss: 0.5764\n",
      "Epoch [9], Iter [91/100] Loss: 0.5906\n",
      "Epoch [10], Iter [91/100] Loss: 0.5518\n",
      "Epoch [11], Iter [91/100] Loss: 0.5380\n",
      "Epoch [12], Iter [91/100] Loss: 0.5152\n",
      "Epoch [13], Iter [91/100] Loss: 0.5178\n",
      "Epoch [14], Iter [91/100] Loss: 0.4964\n",
      "Epoch [15], Iter [91/100] Loss: 0.4938\n",
      "Epoch [16], Iter [91/100] Loss: 0.5103\n",
      "Epoch [17], Iter [91/100] Loss: 0.4864\n",
      "Epoch [18], Iter [91/100] Loss: 0.5003\n",
      "Epoch [19], Iter [91/100] Loss: 0.4710\n",
      "Epoch [20], Iter [91/100] Loss: 0.4784\n",
      "Epoch [21], Iter [91/100] Loss: 0.4588\n",
      "Epoch [22], Iter [91/100] Loss: 0.4453\n",
      "Epoch [23], Iter [91/100] Loss: 0.4682\n",
      "Epoch [24], Iter [91/100] Loss: 0.4506\n",
      "Epoch [25], Iter [91/100] Loss: 0.4333\n",
      "Epoch [26], Iter [91/100] Loss: 0.4425\n",
      "Test MSE: 0.5749168992042542\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.9501\n",
      "Epoch [2], Iter [91/100] Loss: 0.9624\n",
      "Epoch [3], Iter [91/100] Loss: 0.7173\n",
      "Epoch [4], Iter [91/100] Loss: 0.7324\n",
      "Epoch [5], Iter [91/100] Loss: 0.6709\n",
      "Epoch [6], Iter [91/100] Loss: 0.5831\n",
      "Epoch [7], Iter [91/100] Loss: 0.5864\n",
      "Epoch [8], Iter [91/100] Loss: 0.5548\n",
      "Epoch [9], Iter [91/100] Loss: 0.5282\n",
      "Epoch [10], Iter [91/100] Loss: 0.5476\n",
      "Epoch [11], Iter [91/100] Loss: 0.5356\n",
      "Epoch [12], Iter [91/100] Loss: 0.5225\n",
      "Epoch [13], Iter [91/100] Loss: 0.5205\n",
      "Epoch [14], Iter [91/100] Loss: 0.5019\n",
      "Epoch [15], Iter [91/100] Loss: 0.5081\n",
      "Epoch [16], Iter [91/100] Loss: 0.4930\n",
      "Epoch [17], Iter [91/100] Loss: 0.4908\n",
      "Epoch [18], Iter [91/100] Loss: 0.5165\n",
      "Epoch [19], Iter [91/100] Loss: 0.4526\n",
      "Test MSE: 0.5676613450050354\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 1.0723\n",
      "Epoch [2], Iter [91/100] Loss: 1.0397\n",
      "Epoch [3], Iter [91/100] Loss: 0.7435\n",
      "Epoch [4], Iter [91/100] Loss: 0.7335\n",
      "Epoch [5], Iter [91/100] Loss: 0.6617\n",
      "Epoch [6], Iter [91/100] Loss: 0.6118\n",
      "Epoch [7], Iter [91/100] Loss: 0.5841\n",
      "Epoch [8], Iter [91/100] Loss: 0.5605\n",
      "Epoch [9], Iter [91/100] Loss: 0.5860\n",
      "Epoch [10], Iter [91/100] Loss: 0.5481\n",
      "Epoch [11], Iter [91/100] Loss: 0.5563\n",
      "Epoch [12], Iter [91/100] Loss: 0.5106\n",
      "Epoch [13], Iter [91/100] Loss: 0.5922\n",
      "Epoch [14], Iter [91/100] Loss: 0.5008\n",
      "Epoch [15], Iter [91/100] Loss: 0.5235\n",
      "Epoch [16], Iter [91/100] Loss: 0.5036\n",
      "Epoch [17], Iter [91/100] Loss: 0.5113\n",
      "Epoch [18], Iter [91/100] Loss: 0.4737\n",
      "Epoch [19], Iter [91/100] Loss: 0.4562\n",
      "Test MSE: 0.5775488615036011\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.9861\n",
      "Epoch [2], Iter [91/100] Loss: 0.9528\n",
      "Epoch [3], Iter [91/100] Loss: 0.9532\n",
      "Epoch [4], Iter [91/100] Loss: 0.8159\n",
      "Epoch [5], Iter [91/100] Loss: 0.7070\n",
      "Epoch [6], Iter [91/100] Loss: 0.7168\n",
      "Epoch [7], Iter [91/100] Loss: 0.6302\n",
      "Epoch [8], Iter [91/100] Loss: 0.6565\n",
      "Epoch [9], Iter [91/100] Loss: 0.6012\n",
      "Epoch [10], Iter [91/100] Loss: 0.5587\n",
      "Epoch [11], Iter [91/100] Loss: 0.5262\n",
      "Epoch [12], Iter [91/100] Loss: 0.5462\n",
      "Epoch [13], Iter [91/100] Loss: 0.5393\n",
      "Epoch [14], Iter [91/100] Loss: 0.5295\n",
      "Epoch [15], Iter [91/100] Loss: 0.4943\n",
      "Epoch [16], Iter [91/100] Loss: 0.5063\n",
      "Epoch [17], Iter [91/100] Loss: 0.4863\n",
      "Epoch [18], Iter [91/100] Loss: 0.4914\n",
      "Epoch [19], Iter [91/100] Loss: 0.4787\n",
      "Epoch [20], Iter [91/100] Loss: 0.4701\n",
      "Epoch [21], Iter [91/100] Loss: 0.4591\n",
      "Epoch [22], Iter [91/100] Loss: 0.4712\n",
      "Epoch [23], Iter [91/100] Loss: 0.4594\n",
      "Epoch [24], Iter [91/100] Loss: 0.4471\n",
      "Test MSE: 0.5686340928077698\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 1.1013\n",
      "Epoch [2], Iter [91/100] Loss: 0.9900\n",
      "Epoch [3], Iter [91/100] Loss: 0.9839\n",
      "Epoch [4], Iter [91/100] Loss: 0.8823\n",
      "Epoch [5], Iter [91/100] Loss: 0.9615\n",
      "Epoch [6], Iter [91/100] Loss: 0.9562\n",
      "Epoch [7], Iter [91/100] Loss: 0.7331\n",
      "Epoch [8], Iter [91/100] Loss: 0.6787\n",
      "Epoch [9], Iter [91/100] Loss: 0.6207\n",
      "Epoch [10], Iter [91/100] Loss: 0.6245\n",
      "Epoch [11], Iter [91/100] Loss: 0.5736\n",
      "Epoch [12], Iter [91/100] Loss: 0.5547\n",
      "Epoch [13], Iter [91/100] Loss: 0.5434\n",
      "Epoch [14], Iter [91/100] Loss: 0.5483\n",
      "Epoch [15], Iter [91/100] Loss: 0.5491\n",
      "Epoch [16], Iter [91/100] Loss: 0.5183\n",
      "Epoch [17], Iter [91/100] Loss: 0.5225\n",
      "Epoch [18], Iter [91/100] Loss: 0.5095\n",
      "Epoch [19], Iter [91/100] Loss: 0.4828\n",
      "Epoch [20], Iter [91/100] Loss: 0.4807\n",
      "Epoch [21], Iter [91/100] Loss: 0.4721\n",
      "Epoch [22], Iter [91/100] Loss: 0.4930\n",
      "Test MSE: 0.5707063674926758\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.9061\n",
      "Epoch [2], Iter [91/100] Loss: 0.9465\n",
      "Epoch [3], Iter [91/100] Loss: 0.7396\n",
      "Epoch [4], Iter [91/100] Loss: 0.7023\n",
      "Epoch [5], Iter [91/100] Loss: 0.6676\n",
      "Epoch [6], Iter [91/100] Loss: 0.6091\n",
      "Epoch [7], Iter [91/100] Loss: 0.5914\n",
      "Epoch [8], Iter [91/100] Loss: 0.5690\n",
      "Epoch [9], Iter [91/100] Loss: 0.5977\n",
      "Epoch [10], Iter [91/100] Loss: 0.5620\n",
      "Epoch [11], Iter [91/100] Loss: 0.5360\n",
      "Epoch [12], Iter [91/100] Loss: 0.5446\n",
      "Epoch [13], Iter [91/100] Loss: 0.5462\n",
      "Epoch [14], Iter [91/100] Loss: 0.5517\n",
      "Epoch [15], Iter [91/100] Loss: 0.5078\n",
      "Epoch [16], Iter [91/100] Loss: 0.4999\n",
      "Epoch [17], Iter [91/100] Loss: 0.5006\n",
      "Epoch [18], Iter [91/100] Loss: 0.4705\n",
      "Epoch [19], Iter [91/100] Loss: 0.4770\n",
      "Epoch [20], Iter [91/100] Loss: 0.4957\n",
      "Epoch [21], Iter [91/100] Loss: 0.4780\n",
      "Epoch [22], Iter [91/100] Loss: 0.4705\n",
      "Epoch [23], Iter [91/100] Loss: 0.4715\n",
      "Test MSE: 0.5760701298713684\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 1.0055\n",
      "Epoch [2], Iter [91/100] Loss: 0.8437\n",
      "Epoch [3], Iter [91/100] Loss: 0.7081\n",
      "Epoch [4], Iter [91/100] Loss: 0.6930\n",
      "Epoch [5], Iter [91/100] Loss: 0.7288\n",
      "Epoch [6], Iter [91/100] Loss: 0.6006\n",
      "Epoch [7], Iter [91/100] Loss: 0.6019\n",
      "Epoch [8], Iter [91/100] Loss: 0.5687\n",
      "Epoch [9], Iter [91/100] Loss: 0.5633\n",
      "Epoch [10], Iter [91/100] Loss: 0.5913\n",
      "Epoch [11], Iter [91/100] Loss: 0.5563\n",
      "Epoch [12], Iter [91/100] Loss: 0.5508\n",
      "Epoch [13], Iter [91/100] Loss: 0.5334\n",
      "Epoch [14], Iter [91/100] Loss: 0.5090\n",
      "Epoch [15], Iter [91/100] Loss: 0.5048\n",
      "Epoch [16], Iter [91/100] Loss: 0.5112\n",
      "Epoch [17], Iter [91/100] Loss: 0.4994\n",
      "Epoch [18], Iter [91/100] Loss: 0.4871\n",
      "Epoch [19], Iter [91/100] Loss: 0.4486\n",
      "Test MSE: 0.5836153030395508\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 1.0213\n",
      "Epoch [2], Iter [91/100] Loss: 0.8354\n",
      "Epoch [3], Iter [91/100] Loss: 0.6931\n",
      "Epoch [4], Iter [91/100] Loss: 0.6523\n",
      "Epoch [5], Iter [91/100] Loss: 0.6143\n",
      "Epoch [6], Iter [91/100] Loss: 0.6541\n",
      "Epoch [7], Iter [91/100] Loss: 0.5690\n",
      "Epoch [8], Iter [91/100] Loss: 0.5783\n",
      "Epoch [9], Iter [91/100] Loss: 0.5479\n",
      "Epoch [10], Iter [91/100] Loss: 0.5259\n",
      "Epoch [11], Iter [91/100] Loss: 0.5156\n",
      "Epoch [12], Iter [91/100] Loss: 0.5319\n",
      "Epoch [13], Iter [91/100] Loss: 0.5408\n",
      "Epoch [14], Iter [91/100] Loss: 0.5176\n",
      "Epoch [15], Iter [91/100] Loss: 0.5120\n",
      "Epoch [16], Iter [91/100] Loss: 0.4802\n",
      "Epoch [17], Iter [91/100] Loss: 0.4830\n",
      "Epoch [18], Iter [91/100] Loss: 0.4888\n",
      "Epoch [19], Iter [91/100] Loss: 0.4761\n",
      "Epoch [20], Iter [91/100] Loss: 0.4538\n",
      "Epoch [21], Iter [91/100] Loss: 0.4704\n",
      "Test MSE: 0.5678178668022156\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.9849\n",
      "Epoch [2], Iter [91/100] Loss: 0.8186\n",
      "Epoch [3], Iter [91/100] Loss: 0.7010\n",
      "Epoch [4], Iter [91/100] Loss: 0.6997\n",
      "Epoch [5], Iter [91/100] Loss: 0.6339\n",
      "Epoch [6], Iter [91/100] Loss: 0.6253\n",
      "Epoch [7], Iter [91/100] Loss: 0.6114\n",
      "Epoch [8], Iter [91/100] Loss: 0.5754\n",
      "Epoch [9], Iter [91/100] Loss: 0.5913\n",
      "Epoch [10], Iter [91/100] Loss: 0.5355\n",
      "Epoch [11], Iter [91/100] Loss: 0.5513\n",
      "Epoch [12], Iter [91/100] Loss: 0.5038\n",
      "Epoch [13], Iter [91/100] Loss: 0.4992\n",
      "Epoch [14], Iter [91/100] Loss: 0.5073\n",
      "Epoch [15], Iter [91/100] Loss: 0.4907\n",
      "Epoch [16], Iter [91/100] Loss: 0.4870\n",
      "Epoch [17], Iter [91/100] Loss: 0.4944\n",
      "Epoch [18], Iter [91/100] Loss: 0.4918\n",
      "Epoch [19], Iter [91/100] Loss: 0.4661\n",
      "Epoch [20], Iter [91/100] Loss: 0.4653\n",
      "Epoch [21], Iter [91/100] Loss: 0.4214\n",
      "Epoch [22], Iter [91/100] Loss: 0.4411\n",
      "Test MSE: 0.5757251381874084\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.9316\n",
      "Epoch [2], Iter [91/100] Loss: 1.0927\n",
      "Epoch [3], Iter [91/100] Loss: 0.9801\n",
      "Epoch [4], Iter [91/100] Loss: 0.8853\n",
      "Epoch [5], Iter [91/100] Loss: 0.7249\n",
      "Epoch [6], Iter [91/100] Loss: 0.7487\n",
      "Epoch [7], Iter [91/100] Loss: 0.6627\n",
      "Epoch [8], Iter [91/100] Loss: 0.6448\n",
      "Epoch [9], Iter [91/100] Loss: 0.6141\n",
      "Epoch [10], Iter [91/100] Loss: 0.5724\n",
      "Epoch [11], Iter [91/100] Loss: 0.5528\n",
      "Epoch [12], Iter [91/100] Loss: 0.5517\n",
      "Epoch [13], Iter [91/100] Loss: 0.5615\n",
      "Epoch [14], Iter [91/100] Loss: 0.5038\n",
      "Epoch [15], Iter [91/100] Loss: 0.5203\n",
      "Epoch [16], Iter [91/100] Loss: 0.5022\n",
      "Epoch [17], Iter [91/100] Loss: 0.4995\n",
      "Epoch [18], Iter [91/100] Loss: 0.4980\n",
      "Epoch [19], Iter [91/100] Loss: 0.5057\n",
      "Epoch [20], Iter [91/100] Loss: 0.4969\n",
      "Epoch [21], Iter [91/100] Loss: 0.5080\n",
      "Epoch [22], Iter [91/100] Loss: 0.4677\n",
      "Epoch [23], Iter [91/100] Loss: 0.4620\n",
      "Epoch [24], Iter [91/100] Loss: 0.4480\n",
      "Test MSE: 0.580020546913147\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0c94d",
   "metadata": {},
   "source": [
    "## 4.2.4 Hyperparameter tuning\n",
    "\n",
    "Tune the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3397989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.logspace(-4,-1,20)\n",
    "runs_per_lr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44f62766",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b668e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6733\n",
      "Epoch [2], Iter [91/100] Loss: 0.6083\n",
      "Epoch [3], Iter [91/100] Loss: 0.5710\n",
      "Epoch [4], Iter [91/100] Loss: 0.5774\n",
      "Epoch [5], Iter [91/100] Loss: 0.5792\n",
      "Epoch [6], Iter [91/100] Loss: 0.5449\n",
      "Epoch [7], Iter [91/100] Loss: 0.5203\n",
      "Epoch [8], Iter [91/100] Loss: 0.5288\n",
      "Epoch [9], Iter [91/100] Loss: 0.5496\n",
      "Epoch [10], Iter [91/100] Loss: 0.5131\n",
      "Epoch [11], Iter [91/100] Loss: 0.5089\n",
      "Epoch [12], Iter [91/100] Loss: 0.5366\n",
      "Epoch [13], Iter [91/100] Loss: 0.5038\n",
      "Epoch [14], Iter [91/100] Loss: 0.5246\n",
      "Epoch [15], Iter [91/100] Loss: 0.5058\n",
      "Epoch [16], Iter [91/100] Loss: 0.4598\n",
      "Epoch [17], Iter [91/100] Loss: 0.4683\n",
      "Epoch [18], Iter [91/100] Loss: 0.4814\n",
      "Epoch [19], Iter [91/100] Loss: 0.4539\n",
      "Epoch [20], Iter [91/100] Loss: 0.4557\n",
      "Epoch [21], Iter [91/100] Loss: 0.4475\n",
      "Test MSE: 0.565453290939331\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6597\n",
      "Epoch [2], Iter [91/100] Loss: 0.5884\n",
      "Epoch [3], Iter [91/100] Loss: 0.6044\n",
      "Epoch [4], Iter [91/100] Loss: 0.5589\n",
      "Epoch [5], Iter [91/100] Loss: 0.5812\n",
      "Epoch [6], Iter [91/100] Loss: 0.5574\n",
      "Epoch [7], Iter [91/100] Loss: 0.5460\n",
      "Epoch [8], Iter [91/100] Loss: 0.5134\n",
      "Epoch [9], Iter [91/100] Loss: 0.5209\n",
      "Epoch [10], Iter [91/100] Loss: 0.5163\n",
      "Epoch [11], Iter [91/100] Loss: 0.5346\n",
      "Epoch [12], Iter [91/100] Loss: 0.4980\n",
      "Epoch [13], Iter [91/100] Loss: 0.5143\n",
      "Epoch [14], Iter [91/100] Loss: 0.4945\n",
      "Epoch [15], Iter [91/100] Loss: 0.4819\n",
      "Epoch [16], Iter [91/100] Loss: 0.4871\n",
      "Epoch [17], Iter [91/100] Loss: 0.5399\n",
      "Epoch [18], Iter [91/100] Loss: 0.4618\n",
      "Epoch [19], Iter [91/100] Loss: 0.4623\n",
      "Test MSE: 0.5594737529754639\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6193\n",
      "Epoch [2], Iter [91/100] Loss: 0.5979\n",
      "Epoch [3], Iter [91/100] Loss: 0.5896\n",
      "Epoch [4], Iter [91/100] Loss: 0.6056\n",
      "Epoch [5], Iter [91/100] Loss: 0.6616\n",
      "Epoch [6], Iter [91/100] Loss: 0.5426\n",
      "Epoch [7], Iter [91/100] Loss: 0.5031\n",
      "Epoch [8], Iter [91/100] Loss: 0.5276\n",
      "Epoch [9], Iter [91/100] Loss: 0.5144\n",
      "Epoch [10], Iter [91/100] Loss: 0.4980\n",
      "Epoch [11], Iter [91/100] Loss: 0.5073\n",
      "Epoch [12], Iter [91/100] Loss: 0.5006\n",
      "Epoch [13], Iter [91/100] Loss: 0.5153\n",
      "Epoch [14], Iter [91/100] Loss: 0.4880\n",
      "Epoch [15], Iter [91/100] Loss: 0.4893\n",
      "Epoch [16], Iter [91/100] Loss: 0.4817\n",
      "Epoch [17], Iter [91/100] Loss: 0.4666\n",
      "Epoch [18], Iter [91/100] Loss: 0.4531\n",
      "Epoch [19], Iter [91/100] Loss: 0.4434\n",
      "Epoch [20], Iter [91/100] Loss: 0.4446\n",
      "Test MSE: 0.5496870875358582\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6405\n",
      "Epoch [2], Iter [91/100] Loss: 0.6168\n",
      "Epoch [3], Iter [91/100] Loss: 0.5999\n",
      "Epoch [4], Iter [91/100] Loss: 0.5313\n",
      "Epoch [5], Iter [91/100] Loss: 0.5829\n",
      "Epoch [6], Iter [91/100] Loss: 0.5305\n",
      "Epoch [7], Iter [91/100] Loss: 0.5436\n",
      "Epoch [8], Iter [91/100] Loss: 0.5102\n",
      "Epoch [9], Iter [91/100] Loss: 0.5201\n",
      "Epoch [10], Iter [91/100] Loss: 0.5126\n",
      "Epoch [11], Iter [91/100] Loss: 0.5009\n",
      "Epoch [12], Iter [91/100] Loss: 0.4856\n",
      "Epoch [13], Iter [91/100] Loss: 0.4827\n",
      "Epoch [14], Iter [91/100] Loss: 0.4822\n",
      "Epoch [15], Iter [91/100] Loss: 0.4757\n",
      "Epoch [16], Iter [91/100] Loss: 0.4970\n",
      "Epoch [17], Iter [91/100] Loss: 0.4842\n",
      "Epoch [18], Iter [91/100] Loss: 0.4864\n",
      "Epoch [19], Iter [91/100] Loss: 0.5045\n",
      "Test MSE: 0.554512083530426\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6571\n",
      "Epoch [2], Iter [91/100] Loss: 0.5549\n",
      "Epoch [3], Iter [91/100] Loss: 0.5720\n",
      "Epoch [4], Iter [91/100] Loss: 0.5784\n",
      "Epoch [5], Iter [91/100] Loss: 0.5583\n",
      "Epoch [6], Iter [91/100] Loss: 0.5550\n",
      "Epoch [7], Iter [91/100] Loss: 0.6143\n",
      "Epoch [8], Iter [91/100] Loss: 0.5340\n",
      "Epoch [9], Iter [91/100] Loss: 0.5144\n",
      "Epoch [10], Iter [91/100] Loss: 0.5262\n",
      "Epoch [11], Iter [91/100] Loss: 0.4964\n",
      "Epoch [12], Iter [91/100] Loss: 0.5264\n",
      "Epoch [13], Iter [91/100] Loss: 0.4944\n",
      "Epoch [14], Iter [91/100] Loss: 0.5005\n",
      "Epoch [15], Iter [91/100] Loss: 0.5104\n",
      "Epoch [16], Iter [91/100] Loss: 0.5162\n",
      "Epoch [17], Iter [91/100] Loss: 0.4564\n",
      "Test MSE: 0.5592573285102844\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6254\n",
      "Epoch [2], Iter [91/100] Loss: 0.5862\n",
      "Epoch [3], Iter [91/100] Loss: 0.5828\n",
      "Epoch [4], Iter [91/100] Loss: 0.5970\n",
      "Epoch [5], Iter [91/100] Loss: 0.5683\n",
      "Epoch [6], Iter [91/100] Loss: 0.5682\n",
      "Epoch [7], Iter [91/100] Loss: 0.5219\n",
      "Epoch [8], Iter [91/100] Loss: 0.5192\n",
      "Epoch [9], Iter [91/100] Loss: 0.5072\n",
      "Epoch [10], Iter [91/100] Loss: 0.5365\n",
      "Epoch [11], Iter [91/100] Loss: 0.5155\n",
      "Epoch [12], Iter [91/100] Loss: 0.5150\n",
      "Epoch [13], Iter [91/100] Loss: 0.5145\n",
      "Epoch [14], Iter [91/100] Loss: 0.4641\n",
      "Epoch [15], Iter [91/100] Loss: 0.4690\n",
      "Epoch [16], Iter [91/100] Loss: 0.4826\n",
      "Epoch [17], Iter [91/100] Loss: 0.4960\n",
      "Epoch [18], Iter [91/100] Loss: 0.4691\n",
      "Epoch [19], Iter [91/100] Loss: 0.4500\n",
      "Epoch [20], Iter [91/100] Loss: 0.4405\n",
      "Epoch [21], Iter [91/100] Loss: 0.4603\n",
      "Epoch [22], Iter [91/100] Loss: 0.4688\n",
      "Test MSE: 0.5593933463096619\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6243\n",
      "Epoch [2], Iter [91/100] Loss: 0.6223\n",
      "Epoch [3], Iter [91/100] Loss: 0.5830\n",
      "Epoch [4], Iter [91/100] Loss: 0.5719\n",
      "Epoch [5], Iter [91/100] Loss: 0.5448\n",
      "Epoch [6], Iter [91/100] Loss: 0.5627\n",
      "Epoch [7], Iter [91/100] Loss: 0.5323\n",
      "Epoch [8], Iter [91/100] Loss: 0.5366\n",
      "Epoch [9], Iter [91/100] Loss: 0.5067\n",
      "Epoch [10], Iter [91/100] Loss: 0.5570\n",
      "Epoch [11], Iter [91/100] Loss: 0.5243\n",
      "Epoch [12], Iter [91/100] Loss: 0.4886\n",
      "Epoch [13], Iter [91/100] Loss: 0.4982\n",
      "Epoch [14], Iter [91/100] Loss: 0.4944\n",
      "Epoch [15], Iter [91/100] Loss: 0.4874\n",
      "Epoch [16], Iter [91/100] Loss: 0.5165\n",
      "Epoch [17], Iter [91/100] Loss: 0.4599\n",
      "Epoch [18], Iter [91/100] Loss: 0.5206\n",
      "Epoch [19], Iter [91/100] Loss: 0.4472\n",
      "Epoch [20], Iter [91/100] Loss: 0.4802\n",
      "Epoch [21], Iter [91/100] Loss: 0.4597\n",
      "Epoch [22], Iter [91/100] Loss: 0.4488\n",
      "Test MSE: 0.552040696144104\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6375\n",
      "Epoch [2], Iter [91/100] Loss: 0.5776\n",
      "Epoch [3], Iter [91/100] Loss: 0.5511\n",
      "Epoch [4], Iter [91/100] Loss: 0.5726\n",
      "Epoch [5], Iter [91/100] Loss: 0.5498\n",
      "Epoch [6], Iter [91/100] Loss: 0.5745\n",
      "Epoch [7], Iter [91/100] Loss: 0.5468\n",
      "Epoch [8], Iter [91/100] Loss: 0.5267\n",
      "Epoch [9], Iter [91/100] Loss: 0.5284\n",
      "Epoch [10], Iter [91/100] Loss: 0.5383\n",
      "Epoch [11], Iter [91/100] Loss: 0.5070\n",
      "Epoch [12], Iter [91/100] Loss: 0.5119\n",
      "Epoch [13], Iter [91/100] Loss: 0.5133\n",
      "Epoch [14], Iter [91/100] Loss: 0.4924\n",
      "Epoch [15], Iter [91/100] Loss: 0.4714\n",
      "Epoch [16], Iter [91/100] Loss: 0.4567\n",
      "Epoch [17], Iter [91/100] Loss: 0.4708\n",
      "Epoch [18], Iter [91/100] Loss: 0.4753\n",
      "Epoch [19], Iter [91/100] Loss: 0.4685\n",
      "Epoch [20], Iter [91/100] Loss: 0.4462\n",
      "Test MSE: 0.5527199506759644\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6363\n",
      "Epoch [2], Iter [91/100] Loss: 0.5692\n",
      "Epoch [3], Iter [91/100] Loss: 0.5778\n",
      "Epoch [4], Iter [91/100] Loss: 0.5472\n",
      "Epoch [5], Iter [91/100] Loss: 0.5532\n",
      "Epoch [6], Iter [91/100] Loss: 0.5288\n",
      "Epoch [7], Iter [91/100] Loss: 0.5397\n",
      "Epoch [8], Iter [91/100] Loss: 0.5313\n",
      "Epoch [9], Iter [91/100] Loss: 0.5124\n",
      "Epoch [10], Iter [91/100] Loss: 0.5286\n",
      "Epoch [11], Iter [91/100] Loss: 0.5101\n",
      "Epoch [12], Iter [91/100] Loss: 0.4797\n",
      "Epoch [13], Iter [91/100] Loss: 0.4850\n",
      "Epoch [14], Iter [91/100] Loss: 0.5123\n",
      "Epoch [15], Iter [91/100] Loss: 0.4820\n",
      "Epoch [16], Iter [91/100] Loss: 0.4733\n",
      "Epoch [17], Iter [91/100] Loss: 0.4918\n",
      "Epoch [18], Iter [91/100] Loss: 0.4557\n",
      "Epoch [19], Iter [91/100] Loss: 0.4605\n",
      "Epoch [20], Iter [91/100] Loss: 0.4737\n",
      "Epoch [21], Iter [91/100] Loss: 0.4399\n",
      "Epoch [22], Iter [91/100] Loss: 0.4506\n",
      "Epoch [23], Iter [91/100] Loss: 0.4314\n",
      "Epoch [24], Iter [91/100] Loss: 0.4389\n",
      "Test MSE: 0.56341552734375\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6492\n",
      "Epoch [2], Iter [91/100] Loss: 0.6448\n",
      "Epoch [3], Iter [91/100] Loss: 0.5959\n",
      "Epoch [4], Iter [91/100] Loss: 0.5534\n",
      "Epoch [5], Iter [91/100] Loss: 0.5486\n",
      "Epoch [6], Iter [91/100] Loss: 0.5355\n",
      "Epoch [7], Iter [91/100] Loss: 0.5280\n",
      "Epoch [8], Iter [91/100] Loss: 0.5423\n",
      "Epoch [9], Iter [91/100] Loss: 0.5214\n",
      "Epoch [10], Iter [91/100] Loss: 0.5219\n",
      "Epoch [11], Iter [91/100] Loss: 0.5099\n",
      "Epoch [12], Iter [91/100] Loss: 0.5333\n",
      "Epoch [13], Iter [91/100] Loss: 0.5106\n",
      "Epoch [14], Iter [91/100] Loss: 0.4963\n",
      "Epoch [15], Iter [91/100] Loss: 0.4974\n",
      "Epoch [16], Iter [91/100] Loss: 0.4851\n",
      "Epoch [17], Iter [91/100] Loss: 0.5087\n",
      "Epoch [18], Iter [91/100] Loss: 0.4544\n",
      "Epoch [19], Iter [91/100] Loss: 0.4686\n",
      "Epoch [20], Iter [91/100] Loss: 0.4422\n",
      "Epoch [21], Iter [91/100] Loss: 0.4324\n",
      "Test MSE: 0.5647112131118774\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6077\n",
      "Epoch [2], Iter [91/100] Loss: 0.5827\n",
      "Epoch [3], Iter [91/100] Loss: 0.5852\n",
      "Epoch [4], Iter [91/100] Loss: 0.5415\n",
      "Epoch [5], Iter [91/100] Loss: 0.5360\n",
      "Epoch [6], Iter [91/100] Loss: 0.5515\n",
      "Epoch [7], Iter [91/100] Loss: 0.5680\n",
      "Epoch [8], Iter [91/100] Loss: 0.5030\n",
      "Epoch [9], Iter [91/100] Loss: 0.5369\n",
      "Epoch [10], Iter [91/100] Loss: 0.5182\n",
      "Epoch [11], Iter [91/100] Loss: 0.4799\n",
      "Epoch [12], Iter [91/100] Loss: 0.4958\n",
      "Epoch [13], Iter [91/100] Loss: 0.5150\n",
      "Epoch [14], Iter [91/100] Loss: 0.4957\n",
      "Epoch [15], Iter [91/100] Loss: 0.4756\n",
      "Epoch [16], Iter [91/100] Loss: 0.4617\n",
      "Epoch [17], Iter [91/100] Loss: 0.4579\n",
      "Test MSE: 0.5576013922691345\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6403\n",
      "Epoch [2], Iter [91/100] Loss: 0.5936\n",
      "Epoch [3], Iter [91/100] Loss: 0.5420\n",
      "Epoch [4], Iter [91/100] Loss: 0.5834\n",
      "Epoch [5], Iter [91/100] Loss: 0.5887\n",
      "Epoch [6], Iter [91/100] Loss: 0.5257\n",
      "Epoch [7], Iter [91/100] Loss: 0.5357\n",
      "Epoch [8], Iter [91/100] Loss: 0.5272\n",
      "Epoch [9], Iter [91/100] Loss: 0.5350\n",
      "Epoch [10], Iter [91/100] Loss: 0.5389\n",
      "Epoch [11], Iter [91/100] Loss: 0.5070\n",
      "Epoch [12], Iter [91/100] Loss: 0.4837\n",
      "Epoch [13], Iter [91/100] Loss: 0.5032\n",
      "Epoch [14], Iter [91/100] Loss: 0.4934\n",
      "Epoch [15], Iter [91/100] Loss: 0.4671\n",
      "Epoch [16], Iter [91/100] Loss: 0.4737\n",
      "Epoch [17], Iter [91/100] Loss: 0.5014\n",
      "Epoch [18], Iter [91/100] Loss: 0.4720\n",
      "Test MSE: 0.5606204271316528\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6429\n",
      "Epoch [2], Iter [91/100] Loss: 0.6057\n",
      "Epoch [3], Iter [91/100] Loss: 0.6037\n",
      "Epoch [4], Iter [91/100] Loss: 0.5683\n",
      "Epoch [5], Iter [91/100] Loss: 0.5410\n",
      "Epoch [6], Iter [91/100] Loss: 0.5559\n",
      "Epoch [7], Iter [91/100] Loss: 0.5354\n",
      "Epoch [8], Iter [91/100] Loss: 0.5591\n",
      "Epoch [9], Iter [91/100] Loss: 0.5035\n",
      "Epoch [10], Iter [91/100] Loss: 0.5067\n",
      "Epoch [11], Iter [91/100] Loss: 0.5140\n",
      "Epoch [12], Iter [91/100] Loss: 0.4902\n",
      "Epoch [13], Iter [91/100] Loss: 0.4946\n",
      "Epoch [14], Iter [91/100] Loss: 0.5073\n",
      "Epoch [15], Iter [91/100] Loss: 0.4881\n",
      "Epoch [16], Iter [91/100] Loss: 0.4974\n",
      "Epoch [17], Iter [91/100] Loss: 0.5134\n",
      "Epoch [18], Iter [91/100] Loss: 0.4623\n",
      "Epoch [19], Iter [91/100] Loss: 0.4666\n",
      "Epoch [20], Iter [91/100] Loss: 0.4682\n",
      "Test MSE: 0.5556620955467224\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6211\n",
      "Epoch [2], Iter [91/100] Loss: 0.5977\n",
      "Epoch [3], Iter [91/100] Loss: 0.6384\n",
      "Epoch [4], Iter [91/100] Loss: 0.5563\n",
      "Epoch [5], Iter [91/100] Loss: 0.5625\n",
      "Epoch [6], Iter [91/100] Loss: 0.5270\n",
      "Epoch [7], Iter [91/100] Loss: 0.5387\n",
      "Epoch [8], Iter [91/100] Loss: 0.5338\n",
      "Epoch [9], Iter [91/100] Loss: 0.5376\n",
      "Epoch [10], Iter [91/100] Loss: 0.5260\n",
      "Epoch [11], Iter [91/100] Loss: 0.5341\n",
      "Epoch [12], Iter [91/100] Loss: 0.4803\n",
      "Epoch [13], Iter [91/100] Loss: 0.5087\n",
      "Epoch [14], Iter [91/100] Loss: 0.4668\n",
      "Epoch [15], Iter [91/100] Loss: 0.4752\n",
      "Epoch [16], Iter [91/100] Loss: 0.4620\n",
      "Epoch [17], Iter [91/100] Loss: 0.4568\n",
      "Test MSE: 0.5509065985679626\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6126\n",
      "Epoch [2], Iter [91/100] Loss: 0.6108\n",
      "Epoch [3], Iter [91/100] Loss: 0.6064\n",
      "Epoch [4], Iter [91/100] Loss: 0.5710\n",
      "Epoch [5], Iter [91/100] Loss: 0.5555\n",
      "Epoch [6], Iter [91/100] Loss: 0.5455\n",
      "Epoch [7], Iter [91/100] Loss: 0.5260\n",
      "Epoch [8], Iter [91/100] Loss: 0.5237\n",
      "Epoch [9], Iter [91/100] Loss: 0.5006\n",
      "Epoch [10], Iter [91/100] Loss: 0.5238\n",
      "Epoch [11], Iter [91/100] Loss: 0.5076\n",
      "Epoch [12], Iter [91/100] Loss: 0.5108\n",
      "Epoch [13], Iter [91/100] Loss: 0.5187\n",
      "Epoch [14], Iter [91/100] Loss: 0.4892\n",
      "Epoch [15], Iter [91/100] Loss: 0.4860\n",
      "Epoch [16], Iter [91/100] Loss: 0.4763\n",
      "Epoch [17], Iter [91/100] Loss: 0.4771\n",
      "Test MSE: 0.554915726184845\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.5989\n",
      "Epoch [2], Iter [91/100] Loss: 0.6260\n",
      "Epoch [3], Iter [91/100] Loss: 0.5927\n",
      "Epoch [4], Iter [91/100] Loss: 0.5565\n",
      "Epoch [5], Iter [91/100] Loss: 0.5382\n",
      "Epoch [6], Iter [91/100] Loss: 0.5345\n",
      "Epoch [7], Iter [91/100] Loss: 0.5401\n",
      "Epoch [8], Iter [91/100] Loss: 0.5383\n",
      "Epoch [9], Iter [91/100] Loss: 0.5040\n",
      "Epoch [10], Iter [91/100] Loss: 0.5112\n",
      "Epoch [11], Iter [91/100] Loss: 0.5233\n",
      "Epoch [12], Iter [91/100] Loss: 0.5028\n",
      "Epoch [13], Iter [91/100] Loss: 0.5119\n",
      "Epoch [14], Iter [91/100] Loss: 0.4788\n",
      "Epoch [15], Iter [91/100] Loss: 0.4871\n",
      "Epoch [16], Iter [91/100] Loss: 0.4807\n",
      "Epoch [17], Iter [91/100] Loss: 0.4655\n",
      "Epoch [18], Iter [91/100] Loss: 0.5249\n",
      "Epoch [19], Iter [91/100] Loss: 0.4509\n",
      "Epoch [20], Iter [91/100] Loss: 0.4592\n",
      "Epoch [21], Iter [91/100] Loss: 0.4636\n",
      "Epoch [22], Iter [91/100] Loss: 0.4218\n",
      "Test MSE: 0.5651748776435852\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6356\n",
      "Epoch [2], Iter [91/100] Loss: 0.6152\n",
      "Epoch [3], Iter [91/100] Loss: 0.6511\n",
      "Epoch [4], Iter [91/100] Loss: 0.5756\n",
      "Epoch [5], Iter [91/100] Loss: 0.5619\n",
      "Epoch [6], Iter [91/100] Loss: 0.5499\n",
      "Epoch [7], Iter [91/100] Loss: 0.5899\n",
      "Epoch [8], Iter [91/100] Loss: 0.5364\n",
      "Epoch [9], Iter [91/100] Loss: 0.5172\n",
      "Epoch [10], Iter [91/100] Loss: 0.5045\n",
      "Epoch [11], Iter [91/100] Loss: 0.5010\n",
      "Epoch [12], Iter [91/100] Loss: 0.4939\n",
      "Epoch [13], Iter [91/100] Loss: 0.4820\n",
      "Epoch [14], Iter [91/100] Loss: 0.5100\n",
      "Epoch [15], Iter [91/100] Loss: 0.4687\n",
      "Epoch [16], Iter [91/100] Loss: 0.4770\n",
      "Epoch [17], Iter [91/100] Loss: 0.4728\n",
      "Epoch [18], Iter [91/100] Loss: 0.4571\n",
      "Test MSE: 0.5620817542076111\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6119\n",
      "Epoch [2], Iter [91/100] Loss: 0.5960\n",
      "Epoch [3], Iter [91/100] Loss: 0.5984\n",
      "Epoch [4], Iter [91/100] Loss: 0.6025\n",
      "Epoch [5], Iter [91/100] Loss: 0.5495\n",
      "Epoch [6], Iter [91/100] Loss: 0.5254\n",
      "Epoch [7], Iter [91/100] Loss: 0.5373\n",
      "Epoch [8], Iter [91/100] Loss: 0.5306\n",
      "Epoch [9], Iter [91/100] Loss: 0.5162\n",
      "Epoch [10], Iter [91/100] Loss: 0.5136\n",
      "Epoch [11], Iter [91/100] Loss: 0.5113\n",
      "Epoch [12], Iter [91/100] Loss: 0.5108\n",
      "Epoch [13], Iter [91/100] Loss: 0.4934\n",
      "Epoch [14], Iter [91/100] Loss: 0.4888\n",
      "Epoch [15], Iter [91/100] Loss: 0.4669\n",
      "Epoch [16], Iter [91/100] Loss: 0.5014\n",
      "Epoch [17], Iter [91/100] Loss: 0.4738\n",
      "Epoch [18], Iter [91/100] Loss: 0.4720\n",
      "Epoch [19], Iter [91/100] Loss: 0.4814\n",
      "Test MSE: 0.5573922991752625\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6104\n",
      "Epoch [2], Iter [91/100] Loss: 0.5988\n",
      "Epoch [3], Iter [91/100] Loss: 0.5866\n",
      "Epoch [4], Iter [91/100] Loss: 0.5758\n",
      "Epoch [5], Iter [91/100] Loss: 0.5354\n",
      "Epoch [6], Iter [91/100] Loss: 0.5703\n",
      "Epoch [7], Iter [91/100] Loss: 0.5200\n",
      "Epoch [8], Iter [91/100] Loss: 0.5535\n",
      "Epoch [9], Iter [91/100] Loss: 0.5091\n",
      "Epoch [10], Iter [91/100] Loss: 0.5263\n",
      "Epoch [11], Iter [91/100] Loss: 0.5242\n",
      "Epoch [12], Iter [91/100] Loss: 0.5096\n",
      "Epoch [13], Iter [91/100] Loss: 0.5001\n",
      "Epoch [14], Iter [91/100] Loss: 0.5083\n",
      "Epoch [15], Iter [91/100] Loss: 0.5022\n",
      "Epoch [16], Iter [91/100] Loss: 0.4821\n",
      "Epoch [17], Iter [91/100] Loss: 0.4634\n",
      "Epoch [18], Iter [91/100] Loss: 0.5620\n",
      "Epoch [19], Iter [91/100] Loss: 0.4536\n",
      "Test MSE: 0.5477484464645386\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6571\n",
      "Epoch [2], Iter [91/100] Loss: 0.5865\n",
      "Epoch [3], Iter [91/100] Loss: 0.5928\n",
      "Epoch [4], Iter [91/100] Loss: 0.5486\n",
      "Epoch [5], Iter [91/100] Loss: 0.5309\n",
      "Epoch [6], Iter [91/100] Loss: 0.5584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Iter [91/100] Loss: 0.5339\n",
      "Epoch [8], Iter [91/100] Loss: 0.5190\n",
      "Epoch [9], Iter [91/100] Loss: 0.5342\n",
      "Epoch [10], Iter [91/100] Loss: 0.5064\n",
      "Epoch [11], Iter [91/100] Loss: 0.5110\n",
      "Epoch [12], Iter [91/100] Loss: 0.4908\n",
      "Epoch [13], Iter [91/100] Loss: 0.4837\n",
      "Epoch [14], Iter [91/100] Loss: 0.4988\n",
      "Epoch [15], Iter [91/100] Loss: 0.5106\n",
      "Epoch [16], Iter [91/100] Loss: 0.4688\n",
      "Epoch [17], Iter [91/100] Loss: 0.4683\n",
      "Epoch [18], Iter [91/100] Loss: 0.4492\n",
      "Epoch [19], Iter [91/100] Loss: 0.4535\n",
      "Epoch [20], Iter [91/100] Loss: 0.4653\n",
      "Test MSE: 0.5542435646057129\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6157\n",
      "Epoch [2], Iter [91/100] Loss: 0.6211\n",
      "Epoch [3], Iter [91/100] Loss: 0.5937\n",
      "Epoch [4], Iter [91/100] Loss: 0.5923\n",
      "Epoch [5], Iter [91/100] Loss: 0.5245\n",
      "Epoch [6], Iter [91/100] Loss: 0.5797\n",
      "Epoch [7], Iter [91/100] Loss: 0.5331\n",
      "Epoch [8], Iter [91/100] Loss: 0.5305\n",
      "Epoch [9], Iter [91/100] Loss: 0.5407\n",
      "Epoch [10], Iter [91/100] Loss: 0.5194\n",
      "Epoch [11], Iter [91/100] Loss: 0.4944\n",
      "Epoch [12], Iter [91/100] Loss: 0.4979\n",
      "Epoch [13], Iter [91/100] Loss: 0.5252\n",
      "Epoch [14], Iter [91/100] Loss: 0.4977\n",
      "Epoch [15], Iter [91/100] Loss: 0.5039\n",
      "Epoch [16], Iter [91/100] Loss: 0.4716\n",
      "Epoch [17], Iter [91/100] Loss: 0.4743\n",
      "Epoch [18], Iter [91/100] Loss: 0.4804\n",
      "Epoch [19], Iter [91/100] Loss: 0.4859\n",
      "Epoch [20], Iter [91/100] Loss: 0.4556\n",
      "Epoch [21], Iter [91/100] Loss: 0.4457\n",
      "Test MSE: 0.5523959994316101\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6697\n",
      "Epoch [2], Iter [91/100] Loss: 0.6136\n",
      "Epoch [3], Iter [91/100] Loss: 0.5696\n",
      "Epoch [4], Iter [91/100] Loss: 0.5355\n",
      "Epoch [5], Iter [91/100] Loss: 0.5597\n",
      "Epoch [6], Iter [91/100] Loss: 0.5671\n",
      "Epoch [7], Iter [91/100] Loss: 0.5231\n",
      "Epoch [8], Iter [91/100] Loss: 0.5282\n",
      "Epoch [9], Iter [91/100] Loss: 0.5302\n",
      "Epoch [10], Iter [91/100] Loss: 0.4991\n",
      "Epoch [11], Iter [91/100] Loss: 0.4897\n",
      "Epoch [12], Iter [91/100] Loss: 0.4988\n",
      "Epoch [13], Iter [91/100] Loss: 0.4931\n",
      "Epoch [14], Iter [91/100] Loss: 0.4955\n",
      "Epoch [15], Iter [91/100] Loss: 0.4916\n",
      "Epoch [16], Iter [91/100] Loss: 0.4789\n",
      "Test MSE: 0.5767528414726257\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6331\n",
      "Epoch [2], Iter [91/100] Loss: 0.5964\n",
      "Epoch [3], Iter [91/100] Loss: 0.5883\n",
      "Epoch [4], Iter [91/100] Loss: 0.5466\n",
      "Epoch [5], Iter [91/100] Loss: 0.5252\n",
      "Epoch [6], Iter [91/100] Loss: 0.5431\n",
      "Epoch [7], Iter [91/100] Loss: 0.5524\n",
      "Epoch [8], Iter [91/100] Loss: 0.5047\n",
      "Epoch [9], Iter [91/100] Loss: 0.5207\n",
      "Epoch [10], Iter [91/100] Loss: 0.5137\n",
      "Epoch [11], Iter [91/100] Loss: 0.5262\n",
      "Epoch [12], Iter [91/100] Loss: 0.5001\n",
      "Epoch [13], Iter [91/100] Loss: 0.4884\n",
      "Epoch [14], Iter [91/100] Loss: 0.5070\n",
      "Epoch [15], Iter [91/100] Loss: 0.5011\n",
      "Epoch [16], Iter [91/100] Loss: 0.4743\n",
      "Epoch [17], Iter [91/100] Loss: 0.4637\n",
      "Epoch [18], Iter [91/100] Loss: 0.4828\n",
      "Epoch [19], Iter [91/100] Loss: 0.4601\n",
      "Epoch [20], Iter [91/100] Loss: 0.4582\n",
      "Test MSE: 0.5629423260688782\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6487\n",
      "Epoch [2], Iter [91/100] Loss: 0.6079\n",
      "Epoch [3], Iter [91/100] Loss: 0.5751\n",
      "Epoch [4], Iter [91/100] Loss: 0.5578\n",
      "Epoch [5], Iter [91/100] Loss: 0.5732\n",
      "Epoch [6], Iter [91/100] Loss: 0.5255\n",
      "Epoch [7], Iter [91/100] Loss: 0.5363\n",
      "Epoch [8], Iter [91/100] Loss: 0.5273\n",
      "Epoch [9], Iter [91/100] Loss: 0.5189\n",
      "Epoch [10], Iter [91/100] Loss: 0.5061\n",
      "Epoch [11], Iter [91/100] Loss: 0.5786\n",
      "Epoch [12], Iter [91/100] Loss: 0.5066\n",
      "Epoch [13], Iter [91/100] Loss: 0.4955\n",
      "Epoch [14], Iter [91/100] Loss: 0.4766\n",
      "Epoch [15], Iter [91/100] Loss: 0.4805\n",
      "Epoch [16], Iter [91/100] Loss: 0.4808\n",
      "Epoch [17], Iter [91/100] Loss: 0.4752\n",
      "Epoch [18], Iter [91/100] Loss: 0.4556\n",
      "Epoch [19], Iter [91/100] Loss: 0.4356\n",
      "Epoch [20], Iter [91/100] Loss: 0.4504\n",
      "Epoch [21], Iter [91/100] Loss: 0.4279\n",
      "Test MSE: 0.5649915933609009\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6583\n",
      "Epoch [2], Iter [91/100] Loss: 0.6289\n",
      "Epoch [3], Iter [91/100] Loss: 0.5947\n",
      "Epoch [4], Iter [91/100] Loss: 0.5691\n",
      "Epoch [5], Iter [91/100] Loss: 0.5480\n",
      "Epoch [6], Iter [91/100] Loss: 0.5571\n",
      "Epoch [7], Iter [91/100] Loss: 0.5486\n",
      "Epoch [8], Iter [91/100] Loss: 0.5240\n",
      "Epoch [9], Iter [91/100] Loss: 0.5409\n",
      "Epoch [10], Iter [91/100] Loss: 0.5067\n",
      "Epoch [11], Iter [91/100] Loss: 0.5268\n",
      "Epoch [12], Iter [91/100] Loss: 0.5331\n",
      "Epoch [13], Iter [91/100] Loss: 0.5276\n",
      "Epoch [14], Iter [91/100] Loss: 0.5096\n",
      "Epoch [15], Iter [91/100] Loss: 0.4767\n",
      "Epoch [16], Iter [91/100] Loss: 0.5112\n",
      "Epoch [17], Iter [91/100] Loss: 0.4722\n",
      "Epoch [18], Iter [91/100] Loss: 0.4674\n",
      "Epoch [19], Iter [91/100] Loss: 0.4785\n",
      "Test MSE: 0.5566821098327637\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6331\n",
      "Epoch [2], Iter [91/100] Loss: 0.6810\n",
      "Epoch [3], Iter [91/100] Loss: 0.5998\n",
      "Epoch [4], Iter [91/100] Loss: 0.6104\n",
      "Epoch [5], Iter [91/100] Loss: 0.5569\n",
      "Epoch [6], Iter [91/100] Loss: 0.5323\n",
      "Epoch [7], Iter [91/100] Loss: 0.5283\n",
      "Epoch [8], Iter [91/100] Loss: 0.5288\n",
      "Epoch [9], Iter [91/100] Loss: 0.5214\n",
      "Epoch [10], Iter [91/100] Loss: 0.4926\n",
      "Epoch [11], Iter [91/100] Loss: 0.5051\n",
      "Epoch [12], Iter [91/100] Loss: 0.4916\n",
      "Epoch [13], Iter [91/100] Loss: 0.4849\n",
      "Epoch [14], Iter [91/100] Loss: 0.5107\n",
      "Epoch [15], Iter [91/100] Loss: 0.4815\n",
      "Epoch [16], Iter [91/100] Loss: 0.4566\n",
      "Epoch [17], Iter [91/100] Loss: 0.4709\n",
      "Epoch [18], Iter [91/100] Loss: 0.4916\n",
      "Epoch [19], Iter [91/100] Loss: 0.4677\n",
      "Epoch [20], Iter [91/100] Loss: 0.4592\n",
      "Test MSE: 0.5522646307945251\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6398\n",
      "Epoch [2], Iter [91/100] Loss: 0.5915\n",
      "Epoch [3], Iter [91/100] Loss: 0.5666\n",
      "Epoch [4], Iter [91/100] Loss: 0.5681\n",
      "Epoch [5], Iter [91/100] Loss: 0.5842\n",
      "Epoch [6], Iter [91/100] Loss: 0.5244\n",
      "Epoch [7], Iter [91/100] Loss: 0.5391\n",
      "Epoch [8], Iter [91/100] Loss: 0.5537\n",
      "Epoch [9], Iter [91/100] Loss: 0.5319\n",
      "Epoch [10], Iter [91/100] Loss: 0.4915\n",
      "Epoch [11], Iter [91/100] Loss: 0.5204\n",
      "Epoch [12], Iter [91/100] Loss: 0.5068\n",
      "Epoch [13], Iter [91/100] Loss: 0.4949\n",
      "Epoch [14], Iter [91/100] Loss: 0.5006\n",
      "Epoch [15], Iter [91/100] Loss: 0.4962\n",
      "Epoch [16], Iter [91/100] Loss: 0.4911\n",
      "Epoch [17], Iter [91/100] Loss: 0.4578\n",
      "Epoch [18], Iter [91/100] Loss: 0.4619\n",
      "Epoch [19], Iter [91/100] Loss: 0.4516\n",
      "Epoch [20], Iter [91/100] Loss: 0.4680\n",
      "Epoch [21], Iter [91/100] Loss: 0.4467\n",
      "Epoch [22], Iter [91/100] Loss: 0.4389\n",
      "Epoch [23], Iter [91/100] Loss: 0.4299\n",
      "Epoch [24], Iter [91/100] Loss: 0.4294\n",
      "Epoch [25], Iter [91/100] Loss: 0.4176\n",
      "Epoch [26], Iter [91/100] Loss: 0.4185\n",
      "Test MSE: 0.559958279132843\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6253\n",
      "Epoch [2], Iter [91/100] Loss: 0.5890\n",
      "Epoch [3], Iter [91/100] Loss: 0.5598\n",
      "Epoch [4], Iter [91/100] Loss: 0.5611\n",
      "Epoch [5], Iter [91/100] Loss: 0.5428\n",
      "Epoch [6], Iter [91/100] Loss: 0.5389\n",
      "Epoch [7], Iter [91/100] Loss: 0.5545\n",
      "Epoch [8], Iter [91/100] Loss: 0.5489\n",
      "Epoch [9], Iter [91/100] Loss: 0.5296\n",
      "Epoch [10], Iter [91/100] Loss: 0.5119\n",
      "Epoch [11], Iter [91/100] Loss: 0.4871\n",
      "Epoch [12], Iter [91/100] Loss: 0.4951\n",
      "Epoch [13], Iter [91/100] Loss: 0.4773\n",
      "Epoch [14], Iter [91/100] Loss: 0.4962\n",
      "Epoch [15], Iter [91/100] Loss: 0.4793\n",
      "Epoch [16], Iter [91/100] Loss: 0.4693\n",
      "Epoch [17], Iter [91/100] Loss: 0.4582\n",
      "Test MSE: 0.555109441280365\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.5997\n",
      "Epoch [2], Iter [91/100] Loss: 0.5993\n",
      "Epoch [3], Iter [91/100] Loss: 0.5778\n",
      "Epoch [4], Iter [91/100] Loss: 0.5819\n",
      "Epoch [5], Iter [91/100] Loss: 0.5700\n",
      "Epoch [6], Iter [91/100] Loss: 0.5463\n",
      "Epoch [7], Iter [91/100] Loss: 0.5418\n",
      "Epoch [8], Iter [91/100] Loss: 0.5491\n",
      "Epoch [9], Iter [91/100] Loss: 0.5300\n",
      "Epoch [10], Iter [91/100] Loss: 0.5234\n",
      "Epoch [11], Iter [91/100] Loss: 0.5278\n",
      "Epoch [12], Iter [91/100] Loss: 0.5279\n",
      "Epoch [13], Iter [91/100] Loss: 0.4947\n",
      "Epoch [14], Iter [91/100] Loss: 0.4972\n",
      "Epoch [15], Iter [91/100] Loss: 0.4842\n",
      "Epoch [16], Iter [91/100] Loss: 0.4844\n",
      "Epoch [17], Iter [91/100] Loss: 0.4762\n",
      "Epoch [18], Iter [91/100] Loss: 0.4717\n",
      "Epoch [19], Iter [91/100] Loss: 0.4680\n",
      "Epoch [20], Iter [91/100] Loss: 0.4717\n",
      "Epoch [21], Iter [91/100] Loss: 0.4759\n",
      "Test MSE: 0.5540973544120789\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6959\n",
      "Epoch [2], Iter [91/100] Loss: 0.6458\n",
      "Epoch [3], Iter [91/100] Loss: 0.5646\n",
      "Epoch [4], Iter [91/100] Loss: 0.5670\n",
      "Epoch [5], Iter [91/100] Loss: 0.5370\n",
      "Epoch [6], Iter [91/100] Loss: 0.5328\n",
      "Epoch [7], Iter [91/100] Loss: 0.5460\n",
      "Epoch [8], Iter [91/100] Loss: 0.5389\n",
      "Epoch [9], Iter [91/100] Loss: 0.5340\n",
      "Epoch [10], Iter [91/100] Loss: 0.5236\n",
      "Epoch [11], Iter [91/100] Loss: 0.5455\n",
      "Epoch [12], Iter [91/100] Loss: 0.5136\n",
      "Epoch [13], Iter [91/100] Loss: 0.4884\n",
      "Epoch [14], Iter [91/100] Loss: 0.4813\n",
      "Epoch [15], Iter [91/100] Loss: 0.4884\n",
      "Epoch [16], Iter [91/100] Loss: 0.5346\n",
      "Epoch [17], Iter [91/100] Loss: 0.4668\n",
      "Epoch [18], Iter [91/100] Loss: 0.4727\n",
      "Epoch [19], Iter [91/100] Loss: 0.4818\n",
      "Epoch [20], Iter [91/100] Loss: 0.4596\n",
      "Epoch [21], Iter [91/100] Loss: 0.4394\n",
      "Test MSE: 0.552590548992157\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6320\n",
      "Epoch [2], Iter [91/100] Loss: 0.5940\n",
      "Epoch [3], Iter [91/100] Loss: 0.5866\n",
      "Epoch [4], Iter [91/100] Loss: 0.5651\n",
      "Epoch [5], Iter [91/100] Loss: 0.5495\n",
      "Epoch [6], Iter [91/100] Loss: 0.5455\n",
      "Epoch [7], Iter [91/100] Loss: 0.5111\n",
      "Epoch [8], Iter [91/100] Loss: 0.5229\n",
      "Epoch [9], Iter [91/100] Loss: 0.4901\n",
      "Epoch [10], Iter [91/100] Loss: 0.5133\n",
      "Epoch [11], Iter [91/100] Loss: 0.5250\n",
      "Epoch [12], Iter [91/100] Loss: 0.4859\n",
      "Epoch [13], Iter [91/100] Loss: 0.5162\n",
      "Epoch [14], Iter [91/100] Loss: 0.5049\n",
      "Epoch [15], Iter [91/100] Loss: 0.4626\n",
      "Epoch [16], Iter [91/100] Loss: 0.4969\n",
      "Epoch [17], Iter [91/100] Loss: 0.4738\n",
      "Epoch [18], Iter [91/100] Loss: 0.4643\n",
      "Epoch [19], Iter [91/100] Loss: 0.4409\n",
      "Epoch [20], Iter [91/100] Loss: 0.4547\n",
      "Epoch [21], Iter [91/100] Loss: 0.4517\n",
      "Epoch [22], Iter [91/100] Loss: 0.4428\n",
      "Epoch [23], Iter [91/100] Loss: 0.4347\n",
      "Test MSE: 0.5577567219734192\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6470\n",
      "Epoch [2], Iter [91/100] Loss: 0.5938\n",
      "Epoch [3], Iter [91/100] Loss: 0.5617\n",
      "Epoch [4], Iter [91/100] Loss: 0.5377\n",
      "Epoch [5], Iter [91/100] Loss: 0.5612\n",
      "Epoch [6], Iter [91/100] Loss: 0.5234\n",
      "Epoch [7], Iter [91/100] Loss: 0.5379\n",
      "Epoch [8], Iter [91/100] Loss: 0.5282\n",
      "Epoch [9], Iter [91/100] Loss: 0.5282\n",
      "Epoch [10], Iter [91/100] Loss: 0.5356\n",
      "Epoch [11], Iter [91/100] Loss: 0.5065\n",
      "Epoch [12], Iter [91/100] Loss: 0.4912\n",
      "Epoch [13], Iter [91/100] Loss: 0.4940\n",
      "Epoch [14], Iter [91/100] Loss: 0.4652\n",
      "Epoch [15], Iter [91/100] Loss: 0.4722\n",
      "Epoch [16], Iter [91/100] Loss: 0.5105\n",
      "Epoch [17], Iter [91/100] Loss: 0.4875\n",
      "Test MSE: 0.5498136878013611\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6523\n",
      "Epoch [2], Iter [91/100] Loss: 0.5676\n",
      "Epoch [3], Iter [91/100] Loss: 0.5813\n",
      "Epoch [4], Iter [91/100] Loss: 0.5849\n",
      "Epoch [5], Iter [91/100] Loss: 0.5549\n",
      "Epoch [6], Iter [91/100] Loss: 0.5424\n",
      "Epoch [7], Iter [91/100] Loss: 0.5295\n",
      "Epoch [8], Iter [91/100] Loss: 0.5467\n",
      "Epoch [9], Iter [91/100] Loss: 0.5182\n",
      "Epoch [10], Iter [91/100] Loss: 0.5090\n",
      "Epoch [11], Iter [91/100] Loss: 0.5134\n",
      "Epoch [12], Iter [91/100] Loss: 0.5130\n",
      "Epoch [13], Iter [91/100] Loss: 0.4934\n",
      "Epoch [14], Iter [91/100] Loss: 0.5231\n",
      "Epoch [15], Iter [91/100] Loss: 0.4838\n",
      "Epoch [16], Iter [91/100] Loss: 0.4795\n",
      "Epoch [17], Iter [91/100] Loss: 0.4454\n",
      "Epoch [18], Iter [91/100] Loss: 0.4804\n",
      "Epoch [19], Iter [91/100] Loss: 0.4342\n",
      "Epoch [20], Iter [91/100] Loss: 0.4308\n",
      "Epoch [21], Iter [91/100] Loss: 0.4414\n",
      "Epoch [22], Iter [91/100] Loss: 0.4467\n",
      "Epoch [23], Iter [91/100] Loss: 0.4396\n",
      "Epoch [24], Iter [91/100] Loss: 0.4343\n",
      "Epoch [25], Iter [91/100] Loss: 0.4277\n",
      "Test MSE: 0.5623528361320496\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6416\n",
      "Epoch [2], Iter [91/100] Loss: 0.5885\n",
      "Epoch [3], Iter [91/100] Loss: 0.5786\n",
      "Epoch [4], Iter [91/100] Loss: 0.5733\n",
      "Epoch [5], Iter [91/100] Loss: 0.5593\n",
      "Epoch [6], Iter [91/100] Loss: 0.5291\n",
      "Epoch [7], Iter [91/100] Loss: 0.5259\n",
      "Epoch [8], Iter [91/100] Loss: 0.5539\n",
      "Epoch [9], Iter [91/100] Loss: 0.5109\n",
      "Epoch [10], Iter [91/100] Loss: 0.5209\n",
      "Epoch [11], Iter [91/100] Loss: 0.5059\n",
      "Epoch [12], Iter [91/100] Loss: 0.4940\n",
      "Epoch [13], Iter [91/100] Loss: 0.4950\n",
      "Epoch [14], Iter [91/100] Loss: 0.4832\n",
      "Epoch [15], Iter [91/100] Loss: 0.4958\n",
      "Epoch [16], Iter [91/100] Loss: 0.4868\n",
      "Epoch [17], Iter [91/100] Loss: 0.4849\n",
      "Epoch [18], Iter [91/100] Loss: 0.4609\n",
      "Test MSE: 0.5593506097793579\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6282\n",
      "Epoch [2], Iter [91/100] Loss: 0.5984\n",
      "Epoch [3], Iter [91/100] Loss: 0.5733\n",
      "Epoch [4], Iter [91/100] Loss: 0.6119\n",
      "Epoch [5], Iter [91/100] Loss: 0.6109\n",
      "Epoch [6], Iter [91/100] Loss: 0.5668\n",
      "Epoch [7], Iter [91/100] Loss: 0.5338\n",
      "Epoch [8], Iter [91/100] Loss: 0.5288\n",
      "Epoch [9], Iter [91/100] Loss: 0.5055\n",
      "Epoch [10], Iter [91/100] Loss: 0.5078\n",
      "Epoch [11], Iter [91/100] Loss: 0.5109\n",
      "Epoch [12], Iter [91/100] Loss: 0.4957\n",
      "Epoch [13], Iter [91/100] Loss: 0.4941\n",
      "Epoch [14], Iter [91/100] Loss: 0.5107\n",
      "Epoch [15], Iter [91/100] Loss: 0.4849\n",
      "Epoch [16], Iter [91/100] Loss: 0.4792\n",
      "Epoch [17], Iter [91/100] Loss: 0.5231\n",
      "Epoch [18], Iter [91/100] Loss: 0.5524\n",
      "Epoch [19], Iter [91/100] Loss: 0.4971\n",
      "Epoch [20], Iter [91/100] Loss: 0.4619\n",
      "Test MSE: 0.5516701936721802\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6913\n",
      "Epoch [2], Iter [91/100] Loss: 0.5889\n",
      "Epoch [3], Iter [91/100] Loss: 0.6048\n",
      "Epoch [4], Iter [91/100] Loss: 0.5559\n",
      "Epoch [5], Iter [91/100] Loss: 0.5526\n",
      "Epoch [6], Iter [91/100] Loss: 0.5316\n",
      "Epoch [7], Iter [91/100] Loss: 0.5695\n",
      "Epoch [8], Iter [91/100] Loss: 0.5387\n",
      "Epoch [9], Iter [91/100] Loss: 0.5425\n",
      "Epoch [10], Iter [91/100] Loss: 0.4942\n",
      "Epoch [11], Iter [91/100] Loss: 0.5191\n",
      "Epoch [12], Iter [91/100] Loss: 0.4942\n",
      "Epoch [13], Iter [91/100] Loss: 0.4726\n",
      "Epoch [14], Iter [91/100] Loss: 0.4874\n",
      "Epoch [15], Iter [91/100] Loss: 0.4678\n",
      "Epoch [16], Iter [91/100] Loss: 0.4676\n",
      "Epoch [17], Iter [91/100] Loss: 0.4726\n",
      "Epoch [18], Iter [91/100] Loss: 0.4616\n",
      "Test MSE: 0.5593897700309753\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6098\n",
      "Epoch [2], Iter [91/100] Loss: 0.7198\n",
      "Epoch [3], Iter [91/100] Loss: 0.5664\n",
      "Epoch [4], Iter [91/100] Loss: 0.5929\n",
      "Epoch [5], Iter [91/100] Loss: 0.5599\n",
      "Epoch [6], Iter [91/100] Loss: 0.5406\n",
      "Epoch [7], Iter [91/100] Loss: 0.5257\n",
      "Epoch [8], Iter [91/100] Loss: 0.5358\n",
      "Epoch [9], Iter [91/100] Loss: 0.5190\n",
      "Epoch [10], Iter [91/100] Loss: 0.5107\n",
      "Epoch [11], Iter [91/100] Loss: 0.4970\n",
      "Epoch [12], Iter [91/100] Loss: 0.5063\n",
      "Epoch [13], Iter [91/100] Loss: 0.5281\n",
      "Epoch [14], Iter [91/100] Loss: 0.5008\n",
      "Epoch [15], Iter [91/100] Loss: 0.4750\n",
      "Epoch [16], Iter [91/100] Loss: 0.5138\n",
      "Epoch [17], Iter [91/100] Loss: 0.4637\n",
      "Epoch [18], Iter [91/100] Loss: 0.4773\n",
      "Test MSE: 0.55133455991745\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6365\n",
      "Epoch [2], Iter [91/100] Loss: 0.6073\n",
      "Epoch [3], Iter [91/100] Loss: 0.5825\n",
      "Epoch [4], Iter [91/100] Loss: 0.6102\n",
      "Epoch [5], Iter [91/100] Loss: 0.5513\n",
      "Epoch [6], Iter [91/100] Loss: 0.5447\n",
      "Epoch [7], Iter [91/100] Loss: 0.5372\n",
      "Epoch [8], Iter [91/100] Loss: 0.5076\n",
      "Epoch [9], Iter [91/100] Loss: 0.5356\n",
      "Epoch [10], Iter [91/100] Loss: 0.5333\n",
      "Epoch [11], Iter [91/100] Loss: 0.5169\n",
      "Epoch [12], Iter [91/100] Loss: 0.5273\n",
      "Epoch [13], Iter [91/100] Loss: 0.4937\n",
      "Epoch [14], Iter [91/100] Loss: 0.4862\n",
      "Epoch [15], Iter [91/100] Loss: 0.5018\n",
      "Epoch [16], Iter [91/100] Loss: 0.4864\n",
      "Epoch [17], Iter [91/100] Loss: 0.4549\n",
      "Epoch [18], Iter [91/100] Loss: 0.4609\n",
      "Epoch [19], Iter [91/100] Loss: 0.4818\n",
      "Epoch [20], Iter [91/100] Loss: 0.4424\n",
      "Test MSE: 0.5565373301506042\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6442\n",
      "Epoch [2], Iter [91/100] Loss: 0.6102\n",
      "Epoch [3], Iter [91/100] Loss: 0.5432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Iter [91/100] Loss: 0.5823\n",
      "Epoch [5], Iter [91/100] Loss: 0.5733\n",
      "Epoch [6], Iter [91/100] Loss: 0.5734\n",
      "Epoch [7], Iter [91/100] Loss: 0.5055\n",
      "Epoch [8], Iter [91/100] Loss: 0.5294\n",
      "Epoch [9], Iter [91/100] Loss: 0.5042\n",
      "Epoch [10], Iter [91/100] Loss: 0.5231\n",
      "Epoch [11], Iter [91/100] Loss: 0.5044\n",
      "Epoch [12], Iter [91/100] Loss: 0.5119\n",
      "Epoch [13], Iter [91/100] Loss: 0.5008\n",
      "Epoch [14], Iter [91/100] Loss: 0.5509\n",
      "Epoch [15], Iter [91/100] Loss: 0.4845\n",
      "Epoch [16], Iter [91/100] Loss: 0.4795\n",
      "Epoch [17], Iter [91/100] Loss: 0.4810\n",
      "Epoch [18], Iter [91/100] Loss: 0.4533\n",
      "Test MSE: 0.5529162287712097\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6436\n",
      "Epoch [2], Iter [91/100] Loss: 0.6130\n",
      "Epoch [3], Iter [91/100] Loss: 0.5974\n",
      "Epoch [4], Iter [91/100] Loss: 0.5852\n",
      "Epoch [5], Iter [91/100] Loss: 0.5534\n",
      "Epoch [6], Iter [91/100] Loss: 0.5763\n",
      "Epoch [7], Iter [91/100] Loss: 0.5172\n",
      "Epoch [8], Iter [91/100] Loss: 0.5260\n",
      "Epoch [9], Iter [91/100] Loss: 0.5278\n",
      "Epoch [10], Iter [91/100] Loss: 0.5126\n",
      "Epoch [11], Iter [91/100] Loss: 0.5068\n",
      "Epoch [12], Iter [91/100] Loss: 0.5203\n",
      "Epoch [13], Iter [91/100] Loss: 0.5086\n",
      "Epoch [14], Iter [91/100] Loss: 0.4866\n",
      "Epoch [15], Iter [91/100] Loss: 0.4869\n",
      "Epoch [16], Iter [91/100] Loss: 0.4771\n",
      "Epoch [17], Iter [91/100] Loss: 0.4661\n",
      "Epoch [18], Iter [91/100] Loss: 0.4806\n",
      "Epoch [19], Iter [91/100] Loss: 0.4589\n",
      "Epoch [20], Iter [91/100] Loss: 0.4664\n",
      "Epoch [21], Iter [91/100] Loss: 0.5200\n",
      "Test MSE: 0.5596680045127869\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6507\n",
      "Epoch [2], Iter [91/100] Loss: 0.6029\n",
      "Epoch [3], Iter [91/100] Loss: 0.6148\n",
      "Epoch [4], Iter [91/100] Loss: 0.5721\n",
      "Epoch [5], Iter [91/100] Loss: 0.5500\n",
      "Epoch [6], Iter [91/100] Loss: 0.5586\n",
      "Epoch [7], Iter [91/100] Loss: 0.5438\n",
      "Epoch [8], Iter [91/100] Loss: 0.5352\n",
      "Epoch [9], Iter [91/100] Loss: 0.5558\n",
      "Epoch [10], Iter [91/100] Loss: 0.5392\n",
      "Epoch [11], Iter [91/100] Loss: 0.5315\n",
      "Epoch [12], Iter [91/100] Loss: 0.4657\n",
      "Epoch [13], Iter [91/100] Loss: 0.4960\n",
      "Epoch [14], Iter [91/100] Loss: 0.5116\n",
      "Epoch [15], Iter [91/100] Loss: 0.4779\n",
      "Epoch [16], Iter [91/100] Loss: 0.4650\n",
      "Epoch [17], Iter [91/100] Loss: 0.4905\n",
      "Epoch [18], Iter [91/100] Loss: 0.4796\n",
      "Test MSE: 0.5559499859809875\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.5865\n",
      "Epoch [2], Iter [91/100] Loss: 0.5927\n",
      "Epoch [3], Iter [91/100] Loss: 0.6357\n",
      "Epoch [4], Iter [91/100] Loss: 0.5355\n",
      "Epoch [5], Iter [91/100] Loss: 0.5524\n",
      "Epoch [6], Iter [91/100] Loss: 0.5165\n",
      "Epoch [7], Iter [91/100] Loss: 0.5386\n",
      "Epoch [8], Iter [91/100] Loss: 0.5219\n",
      "Epoch [9], Iter [91/100] Loss: 0.5237\n",
      "Epoch [10], Iter [91/100] Loss: 0.5261\n",
      "Epoch [11], Iter [91/100] Loss: 0.4986\n",
      "Epoch [12], Iter [91/100] Loss: 0.5075\n",
      "Epoch [13], Iter [91/100] Loss: 0.5014\n",
      "Epoch [14], Iter [91/100] Loss: 0.5211\n",
      "Epoch [15], Iter [91/100] Loss: 0.4847\n",
      "Epoch [16], Iter [91/100] Loss: 0.4624\n",
      "Epoch [17], Iter [91/100] Loss: 0.4884\n",
      "Epoch [18], Iter [91/100] Loss: 0.4691\n",
      "Epoch [19], Iter [91/100] Loss: 0.4660\n",
      "Test MSE: 0.55732262134552\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7319\n",
      "Epoch [2], Iter [91/100] Loss: 0.6518\n",
      "Epoch [3], Iter [91/100] Loss: 0.6876\n",
      "Epoch [4], Iter [91/100] Loss: 0.5826\n",
      "Epoch [5], Iter [91/100] Loss: 0.5410\n",
      "Epoch [6], Iter [91/100] Loss: 0.5784\n",
      "Epoch [7], Iter [91/100] Loss: 0.5745\n",
      "Epoch [8], Iter [91/100] Loss: 0.5412\n",
      "Epoch [9], Iter [91/100] Loss: 0.5051\n",
      "Epoch [10], Iter [91/100] Loss: 0.5283\n",
      "Epoch [11], Iter [91/100] Loss: 0.4861\n",
      "Epoch [12], Iter [91/100] Loss: 0.4962\n",
      "Epoch [13], Iter [91/100] Loss: 0.4773\n",
      "Epoch [14], Iter [91/100] Loss: 0.4987\n",
      "Epoch [15], Iter [91/100] Loss: 0.4815\n",
      "Epoch [16], Iter [91/100] Loss: 0.4815\n",
      "Epoch [17], Iter [91/100] Loss: 0.4622\n",
      "Epoch [18], Iter [91/100] Loss: 0.4763\n",
      "Test MSE: 0.5571560859680176\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6680\n",
      "Epoch [2], Iter [91/100] Loss: 0.6179\n",
      "Epoch [3], Iter [91/100] Loss: 0.6049\n",
      "Epoch [4], Iter [91/100] Loss: 0.5389\n",
      "Epoch [5], Iter [91/100] Loss: 0.5593\n",
      "Epoch [6], Iter [91/100] Loss: 0.5279\n",
      "Epoch [7], Iter [91/100] Loss: 0.5329\n",
      "Epoch [8], Iter [91/100] Loss: 0.5224\n",
      "Epoch [9], Iter [91/100] Loss: 0.5372\n",
      "Epoch [10], Iter [91/100] Loss: 0.5353\n",
      "Epoch [11], Iter [91/100] Loss: 0.5521\n",
      "Epoch [12], Iter [91/100] Loss: 0.4892\n",
      "Epoch [13], Iter [91/100] Loss: 0.4959\n",
      "Epoch [14], Iter [91/100] Loss: 0.5162\n",
      "Epoch [15], Iter [91/100] Loss: 0.5092\n",
      "Epoch [16], Iter [91/100] Loss: 0.4865\n",
      "Epoch [17], Iter [91/100] Loss: 0.4872\n",
      "Epoch [18], Iter [91/100] Loss: 0.4762\n",
      "Epoch [19], Iter [91/100] Loss: 0.4586\n",
      "Epoch [20], Iter [91/100] Loss: 0.4738\n",
      "Test MSE: 0.5470114946365356\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.5979\n",
      "Epoch [2], Iter [91/100] Loss: 0.6036\n",
      "Epoch [3], Iter [91/100] Loss: 0.5844\n",
      "Epoch [4], Iter [91/100] Loss: 0.5841\n",
      "Epoch [5], Iter [91/100] Loss: 0.5898\n",
      "Epoch [6], Iter [91/100] Loss: 0.5528\n",
      "Epoch [7], Iter [91/100] Loss: 0.5383\n",
      "Epoch [8], Iter [91/100] Loss: 0.5332\n",
      "Epoch [9], Iter [91/100] Loss: 0.5420\n",
      "Epoch [10], Iter [91/100] Loss: 0.5095\n",
      "Epoch [11], Iter [91/100] Loss: 0.5307\n",
      "Epoch [12], Iter [91/100] Loss: 0.4940\n",
      "Epoch [13], Iter [91/100] Loss: 0.5022\n",
      "Epoch [14], Iter [91/100] Loss: 0.4950\n",
      "Epoch [15], Iter [91/100] Loss: 0.4718\n",
      "Epoch [16], Iter [91/100] Loss: 0.4489\n",
      "Epoch [17], Iter [91/100] Loss: 0.4854\n",
      "Epoch [18], Iter [91/100] Loss: 0.4739\n",
      "Epoch [19], Iter [91/100] Loss: 0.4557\n",
      "Epoch [20], Iter [91/100] Loss: 0.4534\n",
      "Test MSE: 0.5503865480422974\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6313\n",
      "Epoch [2], Iter [91/100] Loss: 0.5792\n",
      "Epoch [3], Iter [91/100] Loss: 0.5499\n",
      "Epoch [4], Iter [91/100] Loss: 0.5947\n",
      "Epoch [5], Iter [91/100] Loss: 0.6583\n",
      "Epoch [6], Iter [91/100] Loss: 0.5323\n",
      "Epoch [7], Iter [91/100] Loss: 0.5501\n",
      "Epoch [8], Iter [91/100] Loss: 0.5125\n",
      "Epoch [9], Iter [91/100] Loss: 0.5167\n",
      "Epoch [10], Iter [91/100] Loss: 0.5223\n",
      "Epoch [11], Iter [91/100] Loss: 0.4951\n",
      "Epoch [12], Iter [91/100] Loss: 0.4971\n",
      "Epoch [13], Iter [91/100] Loss: 0.4889\n",
      "Epoch [14], Iter [91/100] Loss: 0.4904\n",
      "Epoch [15], Iter [91/100] Loss: 0.4930\n",
      "Epoch [16], Iter [91/100] Loss: 0.4965\n",
      "Epoch [17], Iter [91/100] Loss: 0.4741\n",
      "Epoch [18], Iter [91/100] Loss: 0.4556\n",
      "Epoch [19], Iter [91/100] Loss: 0.4572\n",
      "Epoch [20], Iter [91/100] Loss: 0.4755\n",
      "Epoch [21], Iter [91/100] Loss: 0.4409\n",
      "Test MSE: 0.5560072064399719\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6237\n",
      "Epoch [2], Iter [91/100] Loss: 0.5884\n",
      "Epoch [3], Iter [91/100] Loss: 0.5948\n",
      "Epoch [4], Iter [91/100] Loss: 0.5694\n",
      "Epoch [5], Iter [91/100] Loss: 0.5290\n",
      "Epoch [6], Iter [91/100] Loss: 0.5211\n",
      "Epoch [7], Iter [91/100] Loss: 0.5358\n",
      "Epoch [8], Iter [91/100] Loss: 0.5559\n",
      "Epoch [9], Iter [91/100] Loss: 0.5136\n",
      "Epoch [10], Iter [91/100] Loss: 0.5072\n",
      "Epoch [11], Iter [91/100] Loss: 0.5008\n",
      "Epoch [12], Iter [91/100] Loss: 0.4945\n",
      "Epoch [13], Iter [91/100] Loss: 0.5097\n",
      "Epoch [14], Iter [91/100] Loss: 0.4884\n",
      "Epoch [15], Iter [91/100] Loss: 0.4748\n",
      "Epoch [16], Iter [91/100] Loss: 0.4814\n",
      "Epoch [17], Iter [91/100] Loss: 0.4787\n",
      "Epoch [18], Iter [91/100] Loss: 0.4490\n",
      "Epoch [19], Iter [91/100] Loss: 0.4717\n",
      "Epoch [20], Iter [91/100] Loss: 0.4504\n",
      "Test MSE: 0.5519316792488098\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6629\n",
      "Epoch [2], Iter [91/100] Loss: 0.6063\n",
      "Epoch [3], Iter [91/100] Loss: 0.5989\n",
      "Epoch [4], Iter [91/100] Loss: 0.5632\n",
      "Epoch [5], Iter [91/100] Loss: 0.5549\n",
      "Epoch [6], Iter [91/100] Loss: 0.5391\n",
      "Epoch [7], Iter [91/100] Loss: 0.5540\n",
      "Epoch [8], Iter [91/100] Loss: 0.5385\n",
      "Epoch [9], Iter [91/100] Loss: 0.5240\n",
      "Epoch [10], Iter [91/100] Loss: 0.5223\n",
      "Epoch [11], Iter [91/100] Loss: 0.5250\n",
      "Epoch [12], Iter [91/100] Loss: 0.5090\n",
      "Epoch [13], Iter [91/100] Loss: 0.4771\n",
      "Epoch [14], Iter [91/100] Loss: 0.5069\n",
      "Epoch [15], Iter [91/100] Loss: 0.4640\n",
      "Epoch [16], Iter [91/100] Loss: 0.4731\n",
      "Epoch [17], Iter [91/100] Loss: 0.4762\n",
      "Test MSE: 0.5560715794563293\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6456\n",
      "Epoch [2], Iter [91/100] Loss: 0.6018\n",
      "Epoch [3], Iter [91/100] Loss: 0.5605\n",
      "Epoch [4], Iter [91/100] Loss: 0.6034\n",
      "Epoch [5], Iter [91/100] Loss: 0.5730\n",
      "Epoch [6], Iter [91/100] Loss: 0.5600\n",
      "Epoch [7], Iter [91/100] Loss: 0.5897\n",
      "Epoch [8], Iter [91/100] Loss: 0.5257\n",
      "Epoch [9], Iter [91/100] Loss: 0.5314\n",
      "Epoch [10], Iter [91/100] Loss: 0.5166\n",
      "Epoch [11], Iter [91/100] Loss: 0.4978\n",
      "Epoch [12], Iter [91/100] Loss: 0.5033\n",
      "Epoch [13], Iter [91/100] Loss: 0.5129\n",
      "Epoch [14], Iter [91/100] Loss: 0.4907\n",
      "Epoch [15], Iter [91/100] Loss: 0.4985\n",
      "Epoch [16], Iter [91/100] Loss: 0.4721\n",
      "Epoch [17], Iter [91/100] Loss: 0.4906\n",
      "Epoch [18], Iter [91/100] Loss: 0.4610\n",
      "Epoch [19], Iter [91/100] Loss: 0.4677\n",
      "Epoch [20], Iter [91/100] Loss: 0.4507\n",
      "Test MSE: 0.5633128881454468\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6103\n",
      "Epoch [2], Iter [91/100] Loss: 0.6016\n",
      "Epoch [3], Iter [91/100] Loss: 0.6088\n",
      "Epoch [4], Iter [91/100] Loss: 0.5830\n",
      "Epoch [5], Iter [91/100] Loss: 0.5379\n",
      "Epoch [6], Iter [91/100] Loss: 0.5201\n",
      "Epoch [7], Iter [91/100] Loss: 0.5151\n",
      "Epoch [8], Iter [91/100] Loss: 0.5330\n",
      "Epoch [9], Iter [91/100] Loss: 0.5291\n",
      "Epoch [10], Iter [91/100] Loss: 0.5381\n",
      "Epoch [11], Iter [91/100] Loss: 0.4841\n",
      "Epoch [12], Iter [91/100] Loss: 0.4957\n",
      "Epoch [13], Iter [91/100] Loss: 0.4910\n",
      "Epoch [14], Iter [91/100] Loss: 0.4878\n",
      "Epoch [15], Iter [91/100] Loss: 0.4857\n",
      "Epoch [16], Iter [91/100] Loss: 0.4804\n",
      "Epoch [17], Iter [91/100] Loss: 0.4771\n",
      "Epoch [18], Iter [91/100] Loss: 0.4677\n",
      "Epoch [19], Iter [91/100] Loss: 0.4499\n",
      "Epoch [20], Iter [91/100] Loss: 0.4645\n",
      "Epoch [21], Iter [91/100] Loss: 0.4692\n",
      "Epoch [22], Iter [91/100] Loss: 0.4573\n",
      "Test MSE: 0.560589075088501\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6421\n",
      "Epoch [2], Iter [91/100] Loss: 0.6216\n",
      "Epoch [3], Iter [91/100] Loss: 0.6114\n",
      "Epoch [4], Iter [91/100] Loss: 0.5759\n",
      "Epoch [5], Iter [91/100] Loss: 0.5479\n",
      "Epoch [6], Iter [91/100] Loss: 0.5493\n",
      "Epoch [7], Iter [91/100] Loss: 0.5673\n",
      "Epoch [8], Iter [91/100] Loss: 0.5267\n",
      "Epoch [9], Iter [91/100] Loss: 0.5186\n",
      "Epoch [10], Iter [91/100] Loss: 0.5107\n",
      "Epoch [11], Iter [91/100] Loss: 0.5164\n",
      "Epoch [12], Iter [91/100] Loss: 0.4871\n",
      "Epoch [13], Iter [91/100] Loss: 0.5047\n",
      "Epoch [14], Iter [91/100] Loss: 0.5186\n",
      "Epoch [15], Iter [91/100] Loss: 0.4644\n",
      "Epoch [16], Iter [91/100] Loss: 0.4629\n",
      "Epoch [17], Iter [91/100] Loss: 0.4573\n",
      "Epoch [18], Iter [91/100] Loss: 0.4600\n",
      "Epoch [19], Iter [91/100] Loss: 0.4555\n",
      "Test MSE: 0.5611512064933777\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6468\n",
      "Epoch [2], Iter [91/100] Loss: 0.6264\n",
      "Epoch [3], Iter [91/100] Loss: 0.5882\n",
      "Epoch [4], Iter [91/100] Loss: 0.5588\n",
      "Epoch [5], Iter [91/100] Loss: 0.5676\n",
      "Epoch [6], Iter [91/100] Loss: 0.5681\n",
      "Epoch [7], Iter [91/100] Loss: 0.5329\n",
      "Epoch [8], Iter [91/100] Loss: 0.5235\n",
      "Epoch [9], Iter [91/100] Loss: 0.5348\n",
      "Epoch [10], Iter [91/100] Loss: 0.5354\n",
      "Epoch [11], Iter [91/100] Loss: 0.5079\n",
      "Epoch [12], Iter [91/100] Loss: 0.5137\n",
      "Epoch [13], Iter [91/100] Loss: 0.5222\n",
      "Epoch [14], Iter [91/100] Loss: 0.4894\n",
      "Epoch [15], Iter [91/100] Loss: 0.4647\n",
      "Epoch [16], Iter [91/100] Loss: 0.4573\n",
      "Epoch [17], Iter [91/100] Loss: 0.4625\n",
      "Epoch [18], Iter [91/100] Loss: 0.4574\n",
      "Epoch [19], Iter [91/100] Loss: 0.4711\n",
      "Epoch [20], Iter [91/100] Loss: 0.4522\n",
      "Epoch [21], Iter [91/100] Loss: 0.4443\n",
      "Epoch [22], Iter [91/100] Loss: 0.4332\n",
      "Test MSE: 0.5666587352752686\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7106\n",
      "Epoch [2], Iter [91/100] Loss: 0.6031\n",
      "Epoch [3], Iter [91/100] Loss: 0.5749\n",
      "Epoch [4], Iter [91/100] Loss: 0.5889\n",
      "Epoch [5], Iter [91/100] Loss: 0.5442\n",
      "Epoch [6], Iter [91/100] Loss: 0.5223\n",
      "Epoch [7], Iter [91/100] Loss: 0.5281\n",
      "Epoch [8], Iter [91/100] Loss: 0.5433\n",
      "Epoch [9], Iter [91/100] Loss: 0.5248\n",
      "Epoch [10], Iter [91/100] Loss: 0.5219\n",
      "Epoch [11], Iter [91/100] Loss: 0.4964\n",
      "Epoch [12], Iter [91/100] Loss: 0.5099\n",
      "Epoch [13], Iter [91/100] Loss: 0.4897\n",
      "Epoch [14], Iter [91/100] Loss: 0.4834\n",
      "Epoch [15], Iter [91/100] Loss: 0.4947\n",
      "Epoch [16], Iter [91/100] Loss: 0.4731\n",
      "Epoch [17], Iter [91/100] Loss: 0.4630\n",
      "Epoch [18], Iter [91/100] Loss: 0.4577\n",
      "Epoch [19], Iter [91/100] Loss: 0.4416\n",
      "Epoch [20], Iter [91/100] Loss: 0.4547\n",
      "Epoch [21], Iter [91/100] Loss: 0.4329\n",
      "Epoch [22], Iter [91/100] Loss: 0.4296\n",
      "Epoch [23], Iter [91/100] Loss: 0.4367\n",
      "Epoch [24], Iter [91/100] Loss: 0.4242\n",
      "Test MSE: 0.5633735060691833\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6081\n",
      "Epoch [2], Iter [91/100] Loss: 0.5906\n",
      "Epoch [3], Iter [91/100] Loss: 0.5652\n",
      "Epoch [4], Iter [91/100] Loss: 0.5700\n",
      "Epoch [5], Iter [91/100] Loss: 0.5305\n",
      "Epoch [6], Iter [91/100] Loss: 0.5444\n",
      "Epoch [7], Iter [91/100] Loss: 0.5212\n",
      "Epoch [8], Iter [91/100] Loss: 0.5317\n",
      "Epoch [9], Iter [91/100] Loss: 0.5214\n",
      "Epoch [10], Iter [91/100] Loss: 0.5339\n",
      "Epoch [11], Iter [91/100] Loss: 0.5059\n",
      "Epoch [12], Iter [91/100] Loss: 0.5041\n",
      "Epoch [13], Iter [91/100] Loss: 0.4869\n",
      "Epoch [14], Iter [91/100] Loss: 0.5131\n",
      "Epoch [15], Iter [91/100] Loss: 0.5065\n",
      "Epoch [16], Iter [91/100] Loss: 0.4698\n",
      "Epoch [17], Iter [91/100] Loss: 0.4707\n",
      "Epoch [18], Iter [91/100] Loss: 0.4607\n",
      "Epoch [19], Iter [91/100] Loss: 0.4463\n",
      "Test MSE: 0.5587846040725708\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6204\n",
      "Epoch [2], Iter [91/100] Loss: 0.5708\n",
      "Epoch [3], Iter [91/100] Loss: 0.6119\n",
      "Epoch [4], Iter [91/100] Loss: 0.5601\n",
      "Epoch [5], Iter [91/100] Loss: 0.5533\n",
      "Epoch [6], Iter [91/100] Loss: 0.5593\n",
      "Epoch [7], Iter [91/100] Loss: 0.5488\n",
      "Epoch [8], Iter [91/100] Loss: 0.5572\n",
      "Epoch [9], Iter [91/100] Loss: 0.5113\n",
      "Epoch [10], Iter [91/100] Loss: 0.5336\n",
      "Epoch [11], Iter [91/100] Loss: 0.5320\n",
      "Epoch [12], Iter [91/100] Loss: 0.5066\n",
      "Epoch [13], Iter [91/100] Loss: 0.5032\n",
      "Epoch [14], Iter [91/100] Loss: 0.5092\n",
      "Epoch [15], Iter [91/100] Loss: 0.4967\n",
      "Epoch [16], Iter [91/100] Loss: 0.5075\n",
      "Epoch [17], Iter [91/100] Loss: 0.4997\n",
      "Epoch [18], Iter [91/100] Loss: 0.4682\n",
      "Epoch [19], Iter [91/100] Loss: 0.4796\n",
      "Test MSE: 0.5532228350639343\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6475\n",
      "Epoch [2], Iter [91/100] Loss: 0.6187\n",
      "Epoch [3], Iter [91/100] Loss: 0.5901\n",
      "Epoch [4], Iter [91/100] Loss: 0.5869\n",
      "Epoch [5], Iter [91/100] Loss: 0.5802\n",
      "Epoch [6], Iter [91/100] Loss: 0.5522\n",
      "Epoch [7], Iter [91/100] Loss: 0.5097\n",
      "Epoch [8], Iter [91/100] Loss: 0.5432\n",
      "Epoch [9], Iter [91/100] Loss: 0.5112\n",
      "Epoch [10], Iter [91/100] Loss: 0.5188\n",
      "Epoch [11], Iter [91/100] Loss: 0.5140\n",
      "Epoch [12], Iter [91/100] Loss: 0.4990\n",
      "Epoch [13], Iter [91/100] Loss: 0.5085\n",
      "Epoch [14], Iter [91/100] Loss: 0.4914\n",
      "Epoch [15], Iter [91/100] Loss: 0.5088\n",
      "Epoch [16], Iter [91/100] Loss: 0.4848\n",
      "Epoch [17], Iter [91/100] Loss: 0.4827\n",
      "Epoch [18], Iter [91/100] Loss: 0.4798\n",
      "Epoch [19], Iter [91/100] Loss: 0.4714\n",
      "Epoch [20], Iter [91/100] Loss: 0.4632\n",
      "Epoch [21], Iter [91/100] Loss: 0.4453\n",
      "Epoch [22], Iter [91/100] Loss: 0.4258\n",
      "Test MSE: 0.5544171333312988\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6505\n",
      "Epoch [2], Iter [91/100] Loss: 0.6073\n",
      "Epoch [3], Iter [91/100] Loss: 0.5483\n",
      "Epoch [4], Iter [91/100] Loss: 0.5484\n",
      "Epoch [5], Iter [91/100] Loss: 0.5550\n",
      "Epoch [6], Iter [91/100] Loss: 0.5998\n",
      "Epoch [7], Iter [91/100] Loss: 0.5421\n",
      "Epoch [8], Iter [91/100] Loss: 0.5082\n",
      "Epoch [9], Iter [91/100] Loss: 0.5014\n",
      "Epoch [10], Iter [91/100] Loss: 0.5087\n",
      "Epoch [11], Iter [91/100] Loss: 0.5316\n",
      "Epoch [12], Iter [91/100] Loss: 0.5237\n",
      "Epoch [13], Iter [91/100] Loss: 0.4880\n",
      "Epoch [14], Iter [91/100] Loss: 0.4944\n",
      "Epoch [15], Iter [91/100] Loss: 0.4753\n",
      "Epoch [16], Iter [91/100] Loss: 0.4984\n",
      "Test MSE: 0.5628792643547058\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6138\n",
      "Epoch [2], Iter [91/100] Loss: 0.6060\n",
      "Epoch [3], Iter [91/100] Loss: 0.5706\n",
      "Epoch [4], Iter [91/100] Loss: 0.5869\n",
      "Epoch [5], Iter [91/100] Loss: 0.6091\n",
      "Epoch [6], Iter [91/100] Loss: 0.5504\n",
      "Epoch [7], Iter [91/100] Loss: 0.5614\n",
      "Epoch [8], Iter [91/100] Loss: 0.5489\n",
      "Epoch [9], Iter [91/100] Loss: 0.5215\n",
      "Epoch [10], Iter [91/100] Loss: 0.5162\n",
      "Epoch [11], Iter [91/100] Loss: 0.5630\n",
      "Epoch [12], Iter [91/100] Loss: 0.4939\n",
      "Epoch [13], Iter [91/100] Loss: 0.4937\n",
      "Epoch [14], Iter [91/100] Loss: 0.4873\n",
      "Epoch [15], Iter [91/100] Loss: 0.4704\n",
      "Epoch [16], Iter [91/100] Loss: 0.4787\n",
      "Epoch [17], Iter [91/100] Loss: 0.4433\n",
      "Epoch [18], Iter [91/100] Loss: 0.4683\n",
      "Epoch [19], Iter [91/100] Loss: 0.4506\n",
      "Epoch [20], Iter [91/100] Loss: 0.4453\n",
      "Epoch [21], Iter [91/100] Loss: 0.4471\n",
      "Epoch [22], Iter [91/100] Loss: 0.4241\n",
      "Epoch [23], Iter [91/100] Loss: 0.4324\n",
      "Epoch [24], Iter [91/100] Loss: 0.4291\n",
      "Epoch [25], Iter [91/100] Loss: 0.4243\n",
      "Epoch [26], Iter [91/100] Loss: 0.4243\n",
      "Test MSE: 0.5573015213012695\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6748\n",
      "Epoch [2], Iter [91/100] Loss: 0.6141\n",
      "Epoch [3], Iter [91/100] Loss: 0.6008\n",
      "Epoch [4], Iter [91/100] Loss: 0.5569\n",
      "Epoch [5], Iter [91/100] Loss: 0.5410\n",
      "Epoch [6], Iter [91/100] Loss: 0.5299\n",
      "Epoch [7], Iter [91/100] Loss: 0.5260\n",
      "Epoch [8], Iter [91/100] Loss: 0.5528\n",
      "Epoch [9], Iter [91/100] Loss: 0.5067\n",
      "Epoch [10], Iter [91/100] Loss: 0.5177\n",
      "Epoch [11], Iter [91/100] Loss: 0.5395\n",
      "Epoch [12], Iter [91/100] Loss: 0.5065\n",
      "Epoch [13], Iter [91/100] Loss: 0.4891\n",
      "Epoch [14], Iter [91/100] Loss: 0.4857\n",
      "Epoch [15], Iter [91/100] Loss: 0.4969\n",
      "Epoch [16], Iter [91/100] Loss: 0.4773\n",
      "Epoch [17], Iter [91/100] Loss: 0.5099\n",
      "Epoch [18], Iter [91/100] Loss: 0.4572\n",
      "Epoch [19], Iter [91/100] Loss: 0.4899\n",
      "Test MSE: 0.5644382834434509\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6397\n",
      "Epoch [2], Iter [91/100] Loss: 0.6286\n",
      "Epoch [3], Iter [91/100] Loss: 0.5684\n",
      "Epoch [4], Iter [91/100] Loss: 0.5646\n",
      "Epoch [5], Iter [91/100] Loss: 0.5746\n",
      "Epoch [6], Iter [91/100] Loss: 0.5384\n",
      "Epoch [7], Iter [91/100] Loss: 0.5491\n",
      "Epoch [8], Iter [91/100] Loss: 0.5543\n",
      "Epoch [9], Iter [91/100] Loss: 0.5418\n",
      "Epoch [10], Iter [91/100] Loss: 0.5370\n",
      "Epoch [11], Iter [91/100] Loss: 0.5055\n",
      "Epoch [12], Iter [91/100] Loss: 0.5256\n",
      "Epoch [13], Iter [91/100] Loss: 0.5117\n",
      "Epoch [14], Iter [91/100] Loss: 0.4970\n",
      "Epoch [15], Iter [91/100] Loss: 0.5098\n",
      "Epoch [16], Iter [91/100] Loss: 0.4864\n",
      "Epoch [17], Iter [91/100] Loss: 0.4640\n",
      "Epoch [18], Iter [91/100] Loss: 0.4795\n",
      "Epoch [19], Iter [91/100] Loss: 0.4651\n",
      "Epoch [20], Iter [91/100] Loss: 0.4578\n",
      "Epoch [21], Iter [91/100] Loss: 0.4512\n",
      "Epoch [22], Iter [91/100] Loss: 0.4514\n",
      "Test MSE: 0.5532360076904297\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    for i in range(runs_per_lr):\n",
    "        model_training_description[\"RUN_NR\"] = i\n",
    "        model_training_description[\"LEARNING_RATE\"] = lr  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "        unet = train_unet(description, model_training_description, output_folder)\n",
    "        predict_save_unet(description, model_training_description, output_folder, unet, output_folder)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e07ce6",
   "metadata": {},
   "source": [
    "### UNet wider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66ca1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Global\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 64\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (64,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a096e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6113\n",
      "Epoch [2], Iter [91/100] Loss: 0.5959\n",
      "Epoch [3], Iter [91/100] Loss: 0.5784\n",
      "Epoch [4], Iter [91/100] Loss: 0.5680\n",
      "Epoch [5], Iter [91/100] Loss: 0.5567\n",
      "Epoch [6], Iter [91/100] Loss: 0.5529\n",
      "Epoch [7], Iter [91/100] Loss: 0.5313\n",
      "Epoch [8], Iter [91/100] Loss: 0.5067\n",
      "Epoch [9], Iter [91/100] Loss: 0.5419\n",
      "Epoch [10], Iter [91/100] Loss: 0.5192\n",
      "Epoch [11], Iter [91/100] Loss: 0.5040\n",
      "Epoch [12], Iter [91/100] Loss: 0.5010\n",
      "Epoch [13], Iter [91/100] Loss: 0.4901\n",
      "Epoch [14], Iter [91/100] Loss: 0.4714\n",
      "Epoch [15], Iter [91/100] Loss: 0.4655\n",
      "Epoch [16], Iter [91/100] Loss: 0.4563\n",
      "Epoch [17], Iter [91/100] Loss: 0.4674\n",
      "Epoch [18], Iter [91/100] Loss: 0.4339\n",
      "Epoch [19], Iter [91/100] Loss: 0.4254\n",
      "Epoch [20], Iter [91/100] Loss: 0.4044\n",
      "Epoch [21], Iter [91/100] Loss: 0.3968\n",
      "Epoch [22], Iter [91/100] Loss: 0.3785\n",
      "Test MSE: 0.5533357858657837\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6378\n",
      "Epoch [2], Iter [91/100] Loss: 0.5742\n",
      "Epoch [3], Iter [91/100] Loss: 0.5950\n",
      "Epoch [4], Iter [91/100] Loss: 0.5510\n",
      "Epoch [5], Iter [91/100] Loss: 0.5802\n",
      "Epoch [6], Iter [91/100] Loss: 0.5683\n",
      "Epoch [7], Iter [91/100] Loss: 0.5369\n",
      "Epoch [8], Iter [91/100] Loss: 0.5376\n",
      "Epoch [9], Iter [91/100] Loss: 0.5343\n",
      "Epoch [10], Iter [91/100] Loss: 0.5240\n",
      "Epoch [11], Iter [91/100] Loss: 0.5293\n",
      "Epoch [12], Iter [91/100] Loss: 0.4871\n",
      "Epoch [13], Iter [91/100] Loss: 0.4783\n",
      "Epoch [14], Iter [91/100] Loss: 0.4687\n",
      "Epoch [15], Iter [91/100] Loss: 0.4743\n",
      "Epoch [16], Iter [91/100] Loss: 0.4594\n",
      "Epoch [17], Iter [91/100] Loss: 0.4539\n",
      "Epoch [18], Iter [91/100] Loss: 0.4513\n",
      "Test MSE: 0.5534327626228333\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6551\n",
      "Epoch [2], Iter [91/100] Loss: 0.6340\n",
      "Epoch [3], Iter [91/100] Loss: 0.5749\n",
      "Epoch [4], Iter [91/100] Loss: 0.5539\n",
      "Epoch [5], Iter [91/100] Loss: 0.5281\n",
      "Epoch [6], Iter [91/100] Loss: 0.5548\n",
      "Epoch [7], Iter [91/100] Loss: 0.5038\n",
      "Epoch [8], Iter [91/100] Loss: 0.5842\n",
      "Epoch [9], Iter [91/100] Loss: 0.4786\n",
      "Epoch [10], Iter [91/100] Loss: 0.4890\n",
      "Epoch [11], Iter [91/100] Loss: 0.4947\n",
      "Epoch [12], Iter [91/100] Loss: 0.4712\n",
      "Epoch [13], Iter [91/100] Loss: 0.5044\n",
      "Epoch [14], Iter [91/100] Loss: 0.4786\n",
      "Epoch [15], Iter [91/100] Loss: 0.4536\n",
      "Epoch [16], Iter [91/100] Loss: 0.4653\n",
      "Epoch [17], Iter [91/100] Loss: 0.4201\n",
      "Epoch [18], Iter [91/100] Loss: 0.4296\n",
      "Epoch [19], Iter [91/100] Loss: 0.4069\n",
      "Test MSE: 0.536714494228363\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.5947\n",
      "Epoch [2], Iter [91/100] Loss: 0.6011\n",
      "Epoch [3], Iter [91/100] Loss: 0.6093\n",
      "Epoch [4], Iter [91/100] Loss: 0.6427\n",
      "Epoch [5], Iter [91/100] Loss: 0.5648\n",
      "Epoch [6], Iter [91/100] Loss: 0.5434\n",
      "Epoch [7], Iter [91/100] Loss: 0.5268\n",
      "Epoch [8], Iter [91/100] Loss: 0.5308\n",
      "Epoch [9], Iter [91/100] Loss: 0.5155\n",
      "Epoch [10], Iter [91/100] Loss: 0.4857\n",
      "Epoch [11], Iter [91/100] Loss: 0.4924\n",
      "Epoch [12], Iter [91/100] Loss: 0.4904\n",
      "Epoch [13], Iter [91/100] Loss: 0.4673\n",
      "Epoch [14], Iter [91/100] Loss: 0.4852\n",
      "Epoch [15], Iter [91/100] Loss: 0.4518\n",
      "Epoch [16], Iter [91/100] Loss: 0.4885\n",
      "Epoch [17], Iter [91/100] Loss: 0.4431\n",
      "Epoch [18], Iter [91/100] Loss: 0.4367\n",
      "Epoch [19], Iter [91/100] Loss: 0.4250\n",
      "Test MSE: 0.5464476346969604\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6418\n",
      "Epoch [2], Iter [91/100] Loss: 0.6117\n",
      "Epoch [3], Iter [91/100] Loss: 0.5759\n",
      "Epoch [4], Iter [91/100] Loss: 0.5770\n",
      "Epoch [5], Iter [91/100] Loss: 0.5431\n",
      "Epoch [6], Iter [91/100] Loss: 0.5278\n",
      "Epoch [7], Iter [91/100] Loss: 0.5760\n",
      "Epoch [8], Iter [91/100] Loss: 0.5404\n",
      "Epoch [9], Iter [91/100] Loss: 0.5352\n",
      "Epoch [10], Iter [91/100] Loss: 0.5244\n",
      "Epoch [11], Iter [91/100] Loss: 0.4894\n",
      "Epoch [12], Iter [91/100] Loss: 0.4937\n",
      "Epoch [13], Iter [91/100] Loss: 0.4998\n",
      "Epoch [14], Iter [91/100] Loss: 0.4854\n",
      "Epoch [15], Iter [91/100] Loss: 0.4773\n",
      "Epoch [16], Iter [91/100] Loss: 0.4621\n",
      "Epoch [17], Iter [91/100] Loss: 0.4542\n",
      "Epoch [18], Iter [91/100] Loss: 0.4602\n",
      "Epoch [19], Iter [91/100] Loss: 0.4119\n",
      "Epoch [20], Iter [91/100] Loss: 0.4152\n",
      "Test MSE: 0.5458791851997375\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6676\n",
      "Epoch [2], Iter [91/100] Loss: 0.5876\n",
      "Epoch [3], Iter [91/100] Loss: 0.5920\n",
      "Epoch [4], Iter [91/100] Loss: 0.5855\n",
      "Epoch [5], Iter [91/100] Loss: 0.5504\n",
      "Epoch [6], Iter [91/100] Loss: 0.5466\n",
      "Epoch [7], Iter [91/100] Loss: 0.5335\n",
      "Epoch [8], Iter [91/100] Loss: 0.5267\n",
      "Epoch [9], Iter [91/100] Loss: 0.5037\n",
      "Epoch [10], Iter [91/100] Loss: 0.5238\n",
      "Epoch [11], Iter [91/100] Loss: 0.4920\n",
      "Epoch [12], Iter [91/100] Loss: 0.4846\n",
      "Epoch [13], Iter [91/100] Loss: 0.4983\n",
      "Epoch [14], Iter [91/100] Loss: 0.4696\n",
      "Epoch [15], Iter [91/100] Loss: 0.4719\n",
      "Epoch [16], Iter [91/100] Loss: 0.4605\n",
      "Epoch [17], Iter [91/100] Loss: 0.4467\n",
      "Epoch [18], Iter [91/100] Loss: 0.4395\n",
      "Test MSE: 0.5482787489891052\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6895\n",
      "Epoch [2], Iter [91/100] Loss: 0.6092\n",
      "Epoch [3], Iter [91/100] Loss: 0.5708\n",
      "Epoch [4], Iter [91/100] Loss: 0.5536\n",
      "Epoch [5], Iter [91/100] Loss: 0.5828\n",
      "Epoch [6], Iter [91/100] Loss: 0.6104\n",
      "Epoch [7], Iter [91/100] Loss: 0.5776\n",
      "Epoch [8], Iter [91/100] Loss: 0.5160\n",
      "Epoch [9], Iter [91/100] Loss: 0.5000\n",
      "Epoch [10], Iter [91/100] Loss: 0.5114\n",
      "Epoch [11], Iter [91/100] Loss: 0.4975\n",
      "Epoch [12], Iter [91/100] Loss: 0.4778\n",
      "Epoch [13], Iter [91/100] Loss: 0.4833\n",
      "Epoch [14], Iter [91/100] Loss: 0.4471\n",
      "Epoch [15], Iter [91/100] Loss: 0.4631\n",
      "Epoch [16], Iter [91/100] Loss: 0.4492\n",
      "Epoch [17], Iter [91/100] Loss: 0.4355\n",
      "Epoch [18], Iter [91/100] Loss: 0.4275\n",
      "Epoch [19], Iter [91/100] Loss: 0.4093\n",
      "Test MSE: 0.5528718829154968\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6602\n",
      "Epoch [2], Iter [91/100] Loss: 0.5583\n",
      "Epoch [3], Iter [91/100] Loss: 0.5859\n",
      "Epoch [4], Iter [91/100] Loss: 0.5447\n",
      "Epoch [5], Iter [91/100] Loss: 0.5131\n",
      "Epoch [6], Iter [91/100] Loss: 0.5367\n",
      "Epoch [7], Iter [91/100] Loss: 0.5367\n",
      "Epoch [8], Iter [91/100] Loss: 0.5601\n",
      "Epoch [9], Iter [91/100] Loss: 0.4968\n",
      "Epoch [10], Iter [91/100] Loss: 0.5041\n",
      "Epoch [11], Iter [91/100] Loss: 0.5124\n",
      "Epoch [12], Iter [91/100] Loss: 0.4873\n",
      "Epoch [13], Iter [91/100] Loss: 0.4964\n",
      "Epoch [14], Iter [91/100] Loss: 0.4874\n",
      "Epoch [15], Iter [91/100] Loss: 0.4511\n",
      "Epoch [16], Iter [91/100] Loss: 0.4503\n",
      "Epoch [17], Iter [91/100] Loss: 0.4524\n",
      "Epoch [18], Iter [91/100] Loss: 0.4241\n",
      "Epoch [19], Iter [91/100] Loss: 0.4219\n",
      "Epoch [20], Iter [91/100] Loss: 0.3925\n",
      "Test MSE: 0.5570404529571533\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6409\n",
      "Epoch [2], Iter [91/100] Loss: 0.6019\n",
      "Epoch [3], Iter [91/100] Loss: 0.5895\n",
      "Epoch [4], Iter [91/100] Loss: 0.5873\n",
      "Epoch [5], Iter [91/100] Loss: 0.5502\n",
      "Epoch [6], Iter [91/100] Loss: 0.5469\n",
      "Epoch [7], Iter [91/100] Loss: 0.4983\n",
      "Epoch [8], Iter [91/100] Loss: 0.5800\n",
      "Epoch [9], Iter [91/100] Loss: 0.5100\n",
      "Epoch [10], Iter [91/100] Loss: 0.5290\n",
      "Epoch [11], Iter [91/100] Loss: 0.5016\n",
      "Epoch [12], Iter [91/100] Loss: 0.4952\n",
      "Epoch [13], Iter [91/100] Loss: 0.4880\n",
      "Epoch [14], Iter [91/100] Loss: 0.4527\n",
      "Epoch [15], Iter [91/100] Loss: 0.4808\n",
      "Epoch [16], Iter [91/100] Loss: 0.4374\n",
      "Epoch [17], Iter [91/100] Loss: 0.4544\n",
      "Test MSE: 0.5505077242851257\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6217\n",
      "Epoch [2], Iter [91/100] Loss: 0.5865\n",
      "Epoch [3], Iter [91/100] Loss: 0.5839\n",
      "Epoch [4], Iter [91/100] Loss: 0.5441\n",
      "Epoch [5], Iter [91/100] Loss: 0.5438\n",
      "Epoch [6], Iter [91/100] Loss: 0.5665\n",
      "Epoch [7], Iter [91/100] Loss: 0.5142\n",
      "Epoch [8], Iter [91/100] Loss: 0.5311\n",
      "Epoch [9], Iter [91/100] Loss: 0.5242\n",
      "Epoch [10], Iter [91/100] Loss: 0.5015\n",
      "Epoch [11], Iter [91/100] Loss: 0.4854\n",
      "Epoch [12], Iter [91/100] Loss: 0.4963\n",
      "Epoch [13], Iter [91/100] Loss: 0.5138\n",
      "Epoch [14], Iter [91/100] Loss: 0.4769\n",
      "Epoch [15], Iter [91/100] Loss: 0.4615\n",
      "Epoch [16], Iter [91/100] Loss: 0.4613\n",
      "Epoch [17], Iter [91/100] Loss: 0.4652\n",
      "Test MSE: 0.5475022196769714\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d7f23",
   "metadata": {},
   "source": [
    "### UNet deeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed8b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "# training parameters\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 4 # this changes compared to standard UNet\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dfb85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6714\n",
      "Epoch [2], Iter [91/100] Loss: 0.6648\n",
      "Epoch [3], Iter [91/100] Loss: 0.5564\n",
      "Epoch [4], Iter [91/100] Loss: 0.5693\n",
      "Epoch [5], Iter [91/100] Loss: 0.5752\n",
      "Epoch [6], Iter [91/100] Loss: 0.5491\n",
      "Epoch [7], Iter [91/100] Loss: 0.5493\n",
      "Epoch [8], Iter [91/100] Loss: 0.5239\n",
      "Epoch [9], Iter [91/100] Loss: 0.5647\n",
      "Epoch [10], Iter [91/100] Loss: 0.5636\n",
      "Epoch [11], Iter [91/100] Loss: 0.5213\n",
      "Epoch [12], Iter [91/100] Loss: 0.4916\n",
      "Epoch [13], Iter [91/100] Loss: 0.5055\n",
      "Epoch [14], Iter [91/100] Loss: 0.5470\n",
      "Epoch [15], Iter [91/100] Loss: 0.4909\n",
      "Epoch [16], Iter [91/100] Loss: 0.4709\n",
      "Epoch [17], Iter [91/100] Loss: 0.4358\n",
      "Test MSE: 0.5646647810935974\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6241\n",
      "Epoch [2], Iter [91/100] Loss: 0.6013\n",
      "Epoch [3], Iter [91/100] Loss: 0.5984\n",
      "Epoch [4], Iter [91/100] Loss: 0.5700\n",
      "Epoch [5], Iter [91/100] Loss: 0.5736\n",
      "Epoch [6], Iter [91/100] Loss: 0.5878\n",
      "Epoch [7], Iter [91/100] Loss: 0.5848\n",
      "Epoch [8], Iter [91/100] Loss: 0.5308\n",
      "Epoch [9], Iter [91/100] Loss: 0.5421\n",
      "Epoch [10], Iter [91/100] Loss: 0.4975\n",
      "Epoch [11], Iter [91/100] Loss: 0.5209\n",
      "Epoch [12], Iter [91/100] Loss: 0.4959\n",
      "Epoch [13], Iter [91/100] Loss: 0.5109\n",
      "Epoch [14], Iter [91/100] Loss: 0.4936\n",
      "Epoch [15], Iter [91/100] Loss: 0.4779\n",
      "Epoch [16], Iter [91/100] Loss: 0.4789\n",
      "Epoch [17], Iter [91/100] Loss: 0.4852\n",
      "Test MSE: 0.5545372366905212\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6211\n",
      "Epoch [2], Iter [91/100] Loss: 0.6155\n",
      "Epoch [3], Iter [91/100] Loss: 0.5712\n",
      "Epoch [4], Iter [91/100] Loss: 0.6075\n",
      "Epoch [5], Iter [91/100] Loss: 0.5498\n",
      "Epoch [6], Iter [91/100] Loss: 0.5479\n",
      "Epoch [7], Iter [91/100] Loss: 0.5437\n",
      "Epoch [8], Iter [91/100] Loss: 0.5436\n",
      "Epoch [9], Iter [91/100] Loss: 0.5255\n",
      "Epoch [10], Iter [91/100] Loss: 0.5183\n",
      "Epoch [11], Iter [91/100] Loss: 0.5033\n",
      "Epoch [12], Iter [91/100] Loss: 0.4858\n",
      "Epoch [13], Iter [91/100] Loss: 0.5164\n",
      "Epoch [14], Iter [91/100] Loss: 0.4923\n",
      "Epoch [15], Iter [91/100] Loss: 0.4874\n",
      "Epoch [16], Iter [91/100] Loss: 0.4528\n",
      "Epoch [17], Iter [91/100] Loss: 0.4546\n",
      "Epoch [18], Iter [91/100] Loss: 0.4765\n",
      "Epoch [19], Iter [91/100] Loss: 0.4597\n",
      "Test MSE: 0.5558869242668152\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6770\n",
      "Epoch [2], Iter [91/100] Loss: 0.6162\n",
      "Epoch [3], Iter [91/100] Loss: 0.5907\n",
      "Epoch [4], Iter [91/100] Loss: 0.6059\n",
      "Epoch [5], Iter [91/100] Loss: 0.5849\n",
      "Epoch [6], Iter [91/100] Loss: 0.5505\n",
      "Epoch [7], Iter [91/100] Loss: 0.5323\n",
      "Epoch [8], Iter [91/100] Loss: 0.5182\n",
      "Epoch [9], Iter [91/100] Loss: 0.5056\n",
      "Epoch [10], Iter [91/100] Loss: 0.5314\n",
      "Epoch [11], Iter [91/100] Loss: 0.5316\n",
      "Epoch [12], Iter [91/100] Loss: 0.5202\n",
      "Epoch [13], Iter [91/100] Loss: 0.5034\n",
      "Epoch [14], Iter [91/100] Loss: 0.5051\n",
      "Epoch [15], Iter [91/100] Loss: 0.5133\n",
      "Epoch [16], Iter [91/100] Loss: 0.4927\n",
      "Epoch [17], Iter [91/100] Loss: 0.4751\n",
      "Epoch [18], Iter [91/100] Loss: 0.4674\n",
      "Epoch [19], Iter [91/100] Loss: 0.4563\n",
      "Epoch [20], Iter [91/100] Loss: 0.4359\n",
      "Epoch [21], Iter [91/100] Loss: 0.4234\n",
      "Epoch [22], Iter [91/100] Loss: 0.4191\n",
      "Test MSE: 0.5554300546646118\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6331\n",
      "Epoch [2], Iter [91/100] Loss: 0.6517\n",
      "Epoch [3], Iter [91/100] Loss: 0.6388\n",
      "Epoch [4], Iter [91/100] Loss: 0.6105\n",
      "Epoch [5], Iter [91/100] Loss: 0.5668\n",
      "Epoch [6], Iter [91/100] Loss: 0.5480\n",
      "Epoch [7], Iter [91/100] Loss: 0.5284\n",
      "Epoch [8], Iter [91/100] Loss: 0.5345\n",
      "Epoch [9], Iter [91/100] Loss: 0.5624\n",
      "Epoch [10], Iter [91/100] Loss: 0.5018\n",
      "Epoch [11], Iter [91/100] Loss: 0.5172\n",
      "Epoch [12], Iter [91/100] Loss: 0.5334\n",
      "Epoch [13], Iter [91/100] Loss: 0.4932\n",
      "Epoch [14], Iter [91/100] Loss: 0.5007\n",
      "Epoch [15], Iter [91/100] Loss: 0.4641\n",
      "Epoch [16], Iter [91/100] Loss: 0.4931\n",
      "Epoch [17], Iter [91/100] Loss: 0.4926\n",
      "Epoch [18], Iter [91/100] Loss: 0.4839\n",
      "Epoch [19], Iter [91/100] Loss: 0.4563\n",
      "Epoch [20], Iter [91/100] Loss: 0.4584\n",
      "Epoch [21], Iter [91/100] Loss: 0.4437\n",
      "Epoch [22], Iter [91/100] Loss: 0.4217\n",
      "Epoch [23], Iter [91/100] Loss: 0.4185\n",
      "Test MSE: 0.5609139800071716\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6043\n",
      "Epoch [2], Iter [91/100] Loss: 0.5882\n",
      "Epoch [3], Iter [91/100] Loss: 0.5904\n",
      "Epoch [4], Iter [91/100] Loss: 0.5833\n",
      "Epoch [5], Iter [91/100] Loss: 0.5461\n",
      "Epoch [6], Iter [91/100] Loss: 0.5616\n",
      "Epoch [7], Iter [91/100] Loss: 0.5359\n",
      "Epoch [8], Iter [91/100] Loss: 0.5338\n",
      "Epoch [9], Iter [91/100] Loss: 0.5165\n",
      "Epoch [10], Iter [91/100] Loss: 0.5055\n",
      "Epoch [11], Iter [91/100] Loss: 0.5237\n",
      "Epoch [12], Iter [91/100] Loss: 0.4922\n",
      "Epoch [13], Iter [91/100] Loss: 0.4789\n",
      "Epoch [14], Iter [91/100] Loss: 0.4986\n",
      "Epoch [15], Iter [91/100] Loss: 0.4841\n",
      "Epoch [16], Iter [91/100] Loss: 0.4702\n",
      "Test MSE: 0.5527656078338623\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6789\n",
      "Epoch [2], Iter [91/100] Loss: 0.6083\n",
      "Epoch [3], Iter [91/100] Loss: 0.5858\n",
      "Epoch [4], Iter [91/100] Loss: 0.5712\n",
      "Epoch [5], Iter [91/100] Loss: 0.5644\n",
      "Epoch [6], Iter [91/100] Loss: 0.5574\n",
      "Epoch [7], Iter [91/100] Loss: 0.5549\n",
      "Epoch [8], Iter [91/100] Loss: 0.5239\n",
      "Epoch [9], Iter [91/100] Loss: 0.5598\n",
      "Epoch [10], Iter [91/100] Loss: 0.5349\n",
      "Epoch [11], Iter [91/100] Loss: 0.4887\n",
      "Epoch [12], Iter [91/100] Loss: 0.5188\n",
      "Epoch [13], Iter [91/100] Loss: 0.4816\n",
      "Epoch [14], Iter [91/100] Loss: 0.4755\n",
      "Epoch [15], Iter [91/100] Loss: 0.5020\n",
      "Epoch [16], Iter [91/100] Loss: 0.4837\n",
      "Epoch [17], Iter [91/100] Loss: 0.4636\n",
      "Epoch [18], Iter [91/100] Loss: 0.4495\n",
      "Epoch [19], Iter [91/100] Loss: 0.4518\n",
      "Test MSE: 0.5446262955665588\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6538\n",
      "Epoch [2], Iter [91/100] Loss: 0.5952\n",
      "Epoch [3], Iter [91/100] Loss: 0.5926\n",
      "Epoch [4], Iter [91/100] Loss: 0.5756\n",
      "Epoch [5], Iter [91/100] Loss: 0.5996\n",
      "Epoch [6], Iter [91/100] Loss: 0.5289\n",
      "Epoch [7], Iter [91/100] Loss: 0.5390\n",
      "Epoch [8], Iter [91/100] Loss: 0.6168\n",
      "Epoch [9], Iter [91/100] Loss: 0.5385\n",
      "Epoch [10], Iter [91/100] Loss: 0.5050\n",
      "Epoch [11], Iter [91/100] Loss: 0.5032\n",
      "Epoch [12], Iter [91/100] Loss: 0.4810\n",
      "Epoch [13], Iter [91/100] Loss: 0.5108\n",
      "Epoch [14], Iter [91/100] Loss: 0.4756\n",
      "Epoch [15], Iter [91/100] Loss: 0.4604\n",
      "Epoch [16], Iter [91/100] Loss: 0.4654\n",
      "Epoch [17], Iter [91/100] Loss: 0.4611\n",
      "Epoch [18], Iter [91/100] Loss: 0.4620\n",
      "Epoch [19], Iter [91/100] Loss: 0.4407\n",
      "Epoch [20], Iter [91/100] Loss: 0.4600\n",
      "Epoch [21], Iter [91/100] Loss: 0.4208\n",
      "Epoch [22], Iter [91/100] Loss: 0.4372\n",
      "Test MSE: 0.5567862391471863\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6401\n",
      "Epoch [2], Iter [91/100] Loss: 0.5947\n",
      "Epoch [3], Iter [91/100] Loss: 0.5794\n",
      "Epoch [4], Iter [91/100] Loss: 0.5959\n",
      "Epoch [5], Iter [91/100] Loss: 0.5426\n",
      "Epoch [6], Iter [91/100] Loss: 0.5815\n",
      "Epoch [7], Iter [91/100] Loss: 0.5546\n",
      "Epoch [8], Iter [91/100] Loss: 0.5421\n",
      "Epoch [9], Iter [91/100] Loss: 0.5177\n",
      "Epoch [10], Iter [91/100] Loss: 0.5054\n",
      "Epoch [11], Iter [91/100] Loss: 0.5161\n",
      "Epoch [12], Iter [91/100] Loss: 0.5020\n",
      "Epoch [13], Iter [91/100] Loss: 0.5107\n",
      "Epoch [14], Iter [91/100] Loss: 0.4639\n",
      "Epoch [15], Iter [91/100] Loss: 0.4746\n",
      "Epoch [16], Iter [91/100] Loss: 0.4750\n",
      "Epoch [17], Iter [91/100] Loss: 0.4755\n",
      "Epoch [18], Iter [91/100] Loss: 0.4527\n",
      "Epoch [19], Iter [91/100] Loss: 0.4561\n",
      "Epoch [20], Iter [91/100] Loss: 0.4359\n",
      "Test MSE: 0.5503860116004944\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6536\n",
      "Epoch [2], Iter [91/100] Loss: 0.6149\n",
      "Epoch [3], Iter [91/100] Loss: 0.5729\n",
      "Epoch [4], Iter [91/100] Loss: 0.5708\n",
      "Epoch [5], Iter [91/100] Loss: 0.5802\n",
      "Epoch [6], Iter [91/100] Loss: 0.5385\n",
      "Epoch [7], Iter [91/100] Loss: 0.5500\n",
      "Epoch [8], Iter [91/100] Loss: 0.5148\n",
      "Epoch [9], Iter [91/100] Loss: 0.5124\n",
      "Epoch [10], Iter [91/100] Loss: 0.5005\n",
      "Epoch [11], Iter [91/100] Loss: 0.5189\n",
      "Epoch [12], Iter [91/100] Loss: 0.4865\n",
      "Epoch [13], Iter [91/100] Loss: 0.4908\n",
      "Epoch [14], Iter [91/100] Loss: 0.4762\n",
      "Epoch [15], Iter [91/100] Loss: 0.4686\n",
      "Epoch [16], Iter [91/100] Loss: 0.4682\n",
      "Epoch [17], Iter [91/100] Loss: 0.4736\n",
      "Epoch [18], Iter [91/100] Loss: 0.4549\n",
      "Test MSE: 0.5589284300804138\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da09b1",
   "metadata": {},
   "source": [
    "# 3) Precipitation weighting \n",
    "\n",
    "Appendix Table A3 in thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "515a8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets_old\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dad9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6099\n",
      "Epoch [2], Iter [91/100] Loss: 0.5506\n",
      "Epoch [3], Iter [91/100] Loss: 0.5346\n",
      "Epoch [4], Iter [91/100] Loss: 0.5221\n",
      "Epoch [5], Iter [91/100] Loss: 0.5101\n",
      "Epoch [6], Iter [91/100] Loss: 0.5136\n",
      "Epoch [7], Iter [91/100] Loss: 0.5044\n",
      "Epoch [8], Iter [91/100] Loss: 0.5856\n",
      "Epoch [9], Iter [91/100] Loss: 0.5079\n",
      "Epoch [10], Iter [91/100] Loss: 0.4717\n",
      "Epoch [11], Iter [91/100] Loss: 0.4375\n",
      "Epoch [12], Iter [91/100] Loss: 0.4537\n",
      "Epoch [13], Iter [91/100] Loss: 0.4281\n",
      "Epoch [14], Iter [91/100] Loss: 0.4327\n",
      "Epoch [15], Iter [91/100] Loss: 0.4025\n",
      "Epoch [16], Iter [91/100] Loss: 0.3899\n",
      "Epoch [17], Iter [91/100] Loss: 0.3970\n",
      "Epoch [18], Iter [91/100] Loss: 0.4065\n",
      "Epoch [19], Iter [91/100] Loss: 0.3783\n",
      "Epoch [20], Iter [91/100] Loss: 0.3548\n",
      "Epoch [21], Iter [91/100] Loss: 0.3633\n",
      "Epoch [22], Iter [91/100] Loss: 0.3632\n",
      "Test MSE: 0.512116551399231\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6106\n",
      "Epoch [2], Iter [91/100] Loss: 0.5516\n",
      "Epoch [3], Iter [91/100] Loss: 0.5297\n",
      "Epoch [4], Iter [91/100] Loss: 0.5654\n",
      "Epoch [5], Iter [91/100] Loss: 0.5185\n",
      "Epoch [6], Iter [91/100] Loss: 0.5076\n",
      "Epoch [7], Iter [91/100] Loss: 0.4854\n",
      "Epoch [8], Iter [91/100] Loss: 0.5069\n",
      "Epoch [9], Iter [91/100] Loss: 0.4861\n",
      "Epoch [10], Iter [91/100] Loss: 0.4731\n",
      "Epoch [11], Iter [91/100] Loss: 0.4658\n",
      "Epoch [12], Iter [91/100] Loss: 0.4215\n",
      "Epoch [13], Iter [91/100] Loss: 0.4579\n",
      "Epoch [14], Iter [91/100] Loss: 0.4394\n",
      "Epoch [15], Iter [91/100] Loss: 0.4136\n",
      "Epoch [16], Iter [91/100] Loss: 0.4068\n",
      "Epoch [17], Iter [91/100] Loss: 0.4070\n",
      "Epoch [18], Iter [91/100] Loss: 0.4305\n",
      "Test MSE: 0.5183867812156677\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6149\n",
      "Epoch [2], Iter [91/100] Loss: 0.5589\n",
      "Epoch [3], Iter [91/100] Loss: 0.5741\n",
      "Epoch [4], Iter [91/100] Loss: 0.5290\n",
      "Epoch [5], Iter [91/100] Loss: 0.4863\n",
      "Epoch [6], Iter [91/100] Loss: 0.4694\n",
      "Epoch [7], Iter [91/100] Loss: 0.4960\n",
      "Epoch [8], Iter [91/100] Loss: 0.4778\n",
      "Epoch [9], Iter [91/100] Loss: 0.4742\n",
      "Epoch [10], Iter [91/100] Loss: 0.4684\n",
      "Epoch [11], Iter [91/100] Loss: 0.4332\n",
      "Epoch [12], Iter [91/100] Loss: 0.4495\n",
      "Epoch [13], Iter [91/100] Loss: 0.4384\n",
      "Epoch [14], Iter [91/100] Loss: 0.4271\n",
      "Epoch [15], Iter [91/100] Loss: 0.4018\n",
      "Epoch [16], Iter [91/100] Loss: 0.4014\n",
      "Epoch [17], Iter [91/100] Loss: 0.4113\n",
      "Epoch [18], Iter [91/100] Loss: 0.3970\n",
      "Epoch [19], Iter [91/100] Loss: 0.3594\n",
      "Epoch [20], Iter [91/100] Loss: 0.4576\n",
      "Epoch [21], Iter [91/100] Loss: 0.3629\n",
      "Epoch [22], Iter [91/100] Loss: 0.3751\n",
      "Epoch [23], Iter [91/100] Loss: 0.3678\n",
      "Test MSE: 0.5164053440093994\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6159\n",
      "Epoch [2], Iter [91/100] Loss: 0.5654\n",
      "Epoch [3], Iter [91/100] Loss: 0.5321\n",
      "Epoch [4], Iter [91/100] Loss: 0.5507\n",
      "Epoch [5], Iter [91/100] Loss: 0.5172\n",
      "Epoch [6], Iter [91/100] Loss: 0.6439\n",
      "Epoch [7], Iter [91/100] Loss: 0.4708\n",
      "Epoch [8], Iter [91/100] Loss: 0.4789\n",
      "Epoch [9], Iter [91/100] Loss: 0.4745\n",
      "Epoch [10], Iter [91/100] Loss: 0.5251\n",
      "Epoch [11], Iter [91/100] Loss: 0.4576\n",
      "Epoch [12], Iter [91/100] Loss: 0.4523\n",
      "Epoch [13], Iter [91/100] Loss: 0.4290\n",
      "Epoch [14], Iter [91/100] Loss: 0.4276\n",
      "Epoch [15], Iter [91/100] Loss: 0.4167\n",
      "Epoch [16], Iter [91/100] Loss: 0.4227\n",
      "Epoch [17], Iter [91/100] Loss: 0.3921\n",
      "Epoch [18], Iter [91/100] Loss: 0.3951\n",
      "Test MSE: 0.5153887867927551\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6476\n",
      "Epoch [2], Iter [91/100] Loss: 0.6003\n",
      "Epoch [3], Iter [91/100] Loss: 0.5466\n",
      "Epoch [4], Iter [91/100] Loss: 0.5743\n",
      "Epoch [5], Iter [91/100] Loss: 0.5077\n",
      "Epoch [6], Iter [91/100] Loss: 0.5160\n",
      "Epoch [7], Iter [91/100] Loss: 0.4725\n",
      "Epoch [8], Iter [91/100] Loss: 0.4618\n",
      "Epoch [9], Iter [91/100] Loss: 0.4609\n",
      "Epoch [10], Iter [91/100] Loss: 0.4710\n",
      "Epoch [11], Iter [91/100] Loss: 0.4496\n",
      "Epoch [12], Iter [91/100] Loss: 0.4608\n",
      "Epoch [13], Iter [91/100] Loss: 0.4277\n",
      "Epoch [14], Iter [91/100] Loss: 0.4328\n",
      "Epoch [15], Iter [91/100] Loss: 0.4208\n",
      "Epoch [16], Iter [91/100] Loss: 0.4105\n",
      "Epoch [17], Iter [91/100] Loss: 0.3961\n",
      "Epoch [18], Iter [91/100] Loss: 0.3920\n",
      "Test MSE: 0.5108562707901001\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6068\n",
      "Epoch [2], Iter [91/100] Loss: 0.5899\n",
      "Epoch [3], Iter [91/100] Loss: 0.5534\n",
      "Epoch [4], Iter [91/100] Loss: 0.5625\n",
      "Epoch [5], Iter [91/100] Loss: 0.5211\n",
      "Epoch [6], Iter [91/100] Loss: 0.5230\n",
      "Epoch [7], Iter [91/100] Loss: 0.4814\n",
      "Epoch [8], Iter [91/100] Loss: 0.4787\n",
      "Epoch [9], Iter [91/100] Loss: 0.4861\n",
      "Epoch [10], Iter [91/100] Loss: 0.4617\n",
      "Epoch [11], Iter [91/100] Loss: 0.4587\n",
      "Epoch [12], Iter [91/100] Loss: 0.4585\n",
      "Epoch [13], Iter [91/100] Loss: 0.4277\n",
      "Epoch [14], Iter [91/100] Loss: 0.4366\n",
      "Epoch [15], Iter [91/100] Loss: 0.4140\n",
      "Epoch [16], Iter [91/100] Loss: 0.4214\n",
      "Epoch [17], Iter [91/100] Loss: 0.3956\n",
      "Epoch [18], Iter [91/100] Loss: 0.4031\n",
      "Test MSE: 0.5072931051254272\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6015\n",
      "Epoch [2], Iter [91/100] Loss: 0.5472\n",
      "Epoch [3], Iter [91/100] Loss: 0.5421\n",
      "Epoch [4], Iter [91/100] Loss: 0.5021\n",
      "Epoch [5], Iter [91/100] Loss: 0.5041\n",
      "Epoch [6], Iter [91/100] Loss: 0.5148\n",
      "Epoch [7], Iter [91/100] Loss: 0.5063\n",
      "Epoch [8], Iter [91/100] Loss: 0.4958\n",
      "Epoch [9], Iter [91/100] Loss: 0.4919\n",
      "Epoch [10], Iter [91/100] Loss: 0.4588\n",
      "Epoch [11], Iter [91/100] Loss: 0.4597\n",
      "Epoch [12], Iter [91/100] Loss: 0.4468\n",
      "Epoch [13], Iter [91/100] Loss: 0.4598\n",
      "Epoch [14], Iter [91/100] Loss: 0.4238\n",
      "Epoch [15], Iter [91/100] Loss: 0.5138\n",
      "Epoch [16], Iter [91/100] Loss: 0.4233\n",
      "Epoch [17], Iter [91/100] Loss: 0.4170\n",
      "Epoch [18], Iter [91/100] Loss: 0.3993\n",
      "Epoch [19], Iter [91/100] Loss: 0.3843\n",
      "Test MSE: 0.5116320252418518\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6266\n",
      "Epoch [2], Iter [91/100] Loss: 0.5998\n",
      "Epoch [3], Iter [91/100] Loss: 0.5358\n",
      "Epoch [4], Iter [91/100] Loss: 0.5300\n",
      "Epoch [5], Iter [91/100] Loss: 0.5436\n",
      "Epoch [6], Iter [91/100] Loss: 0.4943\n",
      "Epoch [7], Iter [91/100] Loss: 0.4880\n",
      "Epoch [8], Iter [91/100] Loss: 0.4755\n",
      "Epoch [9], Iter [91/100] Loss: 0.4850\n",
      "Epoch [10], Iter [91/100] Loss: 0.4503\n",
      "Epoch [11], Iter [91/100] Loss: 0.4388\n",
      "Epoch [12], Iter [91/100] Loss: 0.4429\n",
      "Epoch [13], Iter [91/100] Loss: 0.4376\n",
      "Epoch [14], Iter [91/100] Loss: 0.4412\n",
      "Epoch [15], Iter [91/100] Loss: 0.4354\n",
      "Epoch [16], Iter [91/100] Loss: 0.4034\n",
      "Epoch [17], Iter [91/100] Loss: 0.4110\n",
      "Epoch [18], Iter [91/100] Loss: 0.3935\n",
      "Epoch [19], Iter [91/100] Loss: 0.3831\n",
      "Epoch [20], Iter [91/100] Loss: 0.3865\n",
      "Epoch [21], Iter [91/100] Loss: 0.3829\n",
      "Epoch [22], Iter [91/100] Loss: 0.3563\n",
      "Test MSE: 0.5166885256767273\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6186\n",
      "Epoch [2], Iter [91/100] Loss: 0.5774\n",
      "Epoch [3], Iter [91/100] Loss: 0.5533\n",
      "Epoch [4], Iter [91/100] Loss: 0.5286\n",
      "Epoch [5], Iter [91/100] Loss: 0.6612\n",
      "Epoch [6], Iter [91/100] Loss: 0.5114\n",
      "Epoch [7], Iter [91/100] Loss: 0.5028\n",
      "Epoch [8], Iter [91/100] Loss: 0.4637\n",
      "Epoch [9], Iter [91/100] Loss: 0.4599\n",
      "Epoch [10], Iter [91/100] Loss: 0.4702\n",
      "Epoch [11], Iter [91/100] Loss: 0.4494\n",
      "Epoch [12], Iter [91/100] Loss: 0.4595\n",
      "Epoch [13], Iter [91/100] Loss: 0.4326\n",
      "Epoch [14], Iter [91/100] Loss: 0.4239\n",
      "Epoch [15], Iter [91/100] Loss: 0.4067\n",
      "Epoch [16], Iter [91/100] Loss: 0.4104\n",
      "Epoch [17], Iter [91/100] Loss: 0.3893\n",
      "Epoch [18], Iter [91/100] Loss: 0.3895\n",
      "Epoch [19], Iter [91/100] Loss: 0.3856\n",
      "Test MSE: 0.5108389854431152\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.5867\n",
      "Epoch [2], Iter [91/100] Loss: 0.5997\n",
      "Epoch [3], Iter [91/100] Loss: 0.5242\n",
      "Epoch [4], Iter [91/100] Loss: 0.5168\n",
      "Epoch [5], Iter [91/100] Loss: 0.5120\n",
      "Epoch [6], Iter [91/100] Loss: 0.5104\n",
      "Epoch [7], Iter [91/100] Loss: 0.5196\n",
      "Epoch [8], Iter [91/100] Loss: 0.4605\n",
      "Epoch [9], Iter [91/100] Loss: 0.4711\n",
      "Epoch [10], Iter [91/100] Loss: 0.4926\n",
      "Epoch [11], Iter [91/100] Loss: 0.4558\n",
      "Epoch [12], Iter [91/100] Loss: 0.4482\n",
      "Epoch [13], Iter [91/100] Loss: 0.4471\n",
      "Epoch [14], Iter [91/100] Loss: 0.4327\n",
      "Epoch [15], Iter [91/100] Loss: 0.4147\n",
      "Epoch [16], Iter [91/100] Loss: 0.4325\n",
      "Epoch [17], Iter [91/100] Loss: 0.4059\n",
      "Epoch [18], Iter [91/100] Loss: 0.4078\n",
      "Epoch [19], Iter [91/100] Loss: 0.3693\n",
      "Test MSE: 0.5117236971855164\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
