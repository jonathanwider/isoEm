{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd1b770",
   "metadata": {},
   "source": [
    "# Reimplement thesis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e90607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from predict import *\n",
    "from evaluate import *\n",
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62b8e1",
   "metadata": {},
   "source": [
    "## 1) Create datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047f5b9",
   "metadata": {},
   "source": [
    "### tas, pr, oro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10fe2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\",\n",
    "                                \"oro\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"oro\": [\"ht\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "624fd208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "Specified dataset already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14068/1541587748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_yearly_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\datasets.py\u001b[0m in \u001b[0;36mcreate_yearly_dataset\u001b[1;34m(description, dataset_folder, output_folder)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_if_folder_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Specified dataset already exists.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: Specified dataset already exists."
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512d24d",
   "metadata": {},
   "source": [
    "## Tas, pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80769317",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02346b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7a6fe",
   "metadata": {},
   "source": [
    "### tas, pr, slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832fa64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d667a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254bb9e",
   "metadata": {},
   "source": [
    "### tas only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b0669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154cd2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916970ee",
   "metadata": {},
   "source": [
    "### pr only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23b7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ab0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef46d1a",
   "metadata": {},
   "source": [
    "### slp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7e15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "027ca037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cce60",
   "metadata": {},
   "source": [
    "### pr, tas precip weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e9ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cb889c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_precip_weighted_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b338477",
   "metadata": {},
   "source": [
    "### pr, tas ico grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31b74e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f42125f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ea85e",
   "metadata": {},
   "source": [
    "## 2) Run experiments yearly dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed8d56",
   "metadata": {},
   "source": [
    "### 4.2.1 Modifications to flat UNet\n",
    "\n",
    "Start by selecting the tas, pr dataset without precipitation weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf2fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f8117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbaabda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [\"MSELoss\", \"AreaWeightedMSELoss\"]\n",
    "use_coord_conv = [False, True]\n",
    "use_cylindrical_padding = [False, True]\n",
    "n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28270b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss False False 0\n"
     ]
    }
   ],
   "source": [
    "for l in loss:\n",
    "    for c_conv in use_coord_conv:\n",
    "        for c_pad in use_cylindrical_padding:\n",
    "            for i in range(n_runs):\n",
    "                print(l, c_conv, c_pad, i)\n",
    "                model_training_description[\"USE_CYLINDRICAL_PADDING\"] = c_pad\n",
    "                model_training_description[\"USE_COORD_CONV\"] = c_conv\n",
    "                model_training_description[\"LOSS\"] = l  # \"MSELoss\" # \"AreaWeightedMSELoss\"\n",
    "                model_training_description[\"RUN_NR\"] = i\n",
    "                unet = train_unet(description, model_training_description, output_folder)\n",
    "                predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e757923",
   "metadata": {},
   "source": [
    "### 4.2.2 Comparing results for different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf587d",
   "metadata": {},
   "source": [
    "We will need to implement the interpolation before we can reproduce the whole table. Until then: Only compare ico architectures on ico grid and flat architectures on flat grid.\n",
    "\n",
    "Results for modified and unmodified flat UNet are already obtained in last cell.(4.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad454efa",
   "metadata": {},
   "source": [
    "### Flat grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974f0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eef2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "model_training_description[\"N_PC_PREDICTORS\"] = 450\n",
    "model_training_description[\"N_PC_TARGETS\"] = 300\n",
    "model_training_description[\"REGTYPE\"] = \"lasso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f45df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pca, pca_targets, model = train_pca(description, model_training_description, output_folder)\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445d9ae",
   "metadata": {},
   "source": [
    "### New linreg baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab67592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"LinReg_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3705d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "models = train_linreg_pixelwise(description, model_training_description, output_folder)\n",
    "predict_save_linreg_pixelwise(description, model_training_description, output_folder, models, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5010b",
   "metadata": {},
   "source": [
    "### New PCA-reg baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a5689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1a00b",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning, compute 50x50 logarithmic grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a934508",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 50)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ba9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results: N_PC_IN: 230 N_PC_OUT: 147, R2_mean, validationset: [0.1696857]\n",
      "Retrain including validation set.\n",
      "Result on test set: [0.18934951]\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89376fae",
   "metadata": {},
   "source": [
    "### New Random-forest baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069fbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"RandomForest_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e193661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  3.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 20.0min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed: 19.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "model = train_random_forest_pixelwise(description, model_training_description, output_folder, verbose=3, n_jobs=-1)\n",
    "predict_save_randomforest_pixelwise(description, model_training_description, output_folder, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc721a",
   "metadata": {},
   "source": [
    "### Icosahedral grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e59925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca80ed",
   "metadata": {},
   "source": [
    "Ico baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe580f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "model_training_description[\"N_PC_PREDICTORS\"] = 450\n",
    "model_training_description[\"N_PC_TARGETS\"] = 300\n",
    "model_training_description[\"REGTYPE\"] = \"lasso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1780209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pca, pca_targets, model = train_pca(description, model_training_description, output_folder)\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98681c",
   "metadata": {},
   "source": [
    "Ico UNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327ecbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = IcoBatchNorm2d # torch.nn.BatchNorm2d\n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "model_training_description[\"LOSS\"] = \"MSELoss\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab785a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8554\n",
      "Epoch [2], Iter [91/100] Loss: 0.6937\n",
      "Epoch [3], Iter [91/100] Loss: 0.7577\n",
      "Epoch [4], Iter [91/100] Loss: 0.6342\n",
      "Epoch [5], Iter [91/100] Loss: 0.5978\n",
      "Epoch [6], Iter [91/100] Loss: 0.5955\n",
      "Epoch [7], Iter [91/100] Loss: 0.5592\n",
      "Epoch [8], Iter [91/100] Loss: 0.5608\n",
      "Epoch [9], Iter [91/100] Loss: 0.5620\n",
      "Epoch [10], Iter [91/100] Loss: 0.5367\n",
      "Epoch [11], Iter [91/100] Loss: 0.5335\n",
      "Epoch [12], Iter [91/100] Loss: 0.5215\n",
      "Epoch [13], Iter [91/100] Loss: 0.4910\n",
      "Epoch [14], Iter [91/100] Loss: 0.4889\n",
      "Epoch [15], Iter [91/100] Loss: 0.4753\n",
      "Epoch [16], Iter [91/100] Loss: 0.4519\n",
      "Epoch [17], Iter [91/100] Loss: 0.4222\n",
      "Epoch [18], Iter [91/100] Loss: 0.4376\n",
      "Test MSE: 0.5950362682342529\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8556\n",
      "Epoch [2], Iter [91/100] Loss: 0.7561\n",
      "Epoch [3], Iter [91/100] Loss: 0.7349\n",
      "Epoch [4], Iter [91/100] Loss: 0.6746\n",
      "Epoch [5], Iter [91/100] Loss: 0.6359\n",
      "Epoch [6], Iter [91/100] Loss: 0.6060\n",
      "Epoch [7], Iter [91/100] Loss: 0.5976\n",
      "Epoch [8], Iter [91/100] Loss: 0.6098\n",
      "Epoch [9], Iter [91/100] Loss: 0.5959\n",
      "Epoch [10], Iter [91/100] Loss: 0.5413\n",
      "Epoch [11], Iter [91/100] Loss: 0.5641\n",
      "Epoch [12], Iter [91/100] Loss: 0.5434\n",
      "Epoch [13], Iter [91/100] Loss: 0.5028\n",
      "Epoch [14], Iter [91/100] Loss: 0.5086\n",
      "Epoch [15], Iter [91/100] Loss: 0.5156\n",
      "Epoch [16], Iter [91/100] Loss: 0.5190\n",
      "Epoch [17], Iter [91/100] Loss: 0.4619\n",
      "Epoch [18], Iter [91/100] Loss: 0.4418\n",
      "Epoch [19], Iter [91/100] Loss: 0.4337\n",
      "Epoch [20], Iter [91/100] Loss: 0.4223\n",
      "Epoch [21], Iter [91/100] Loss: 0.4042\n",
      "Epoch [22], Iter [91/100] Loss: 0.3978\n",
      "Epoch [23], Iter [91/100] Loss: 0.3958\n",
      "Test MSE: 0.5995284914970398\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8363\n",
      "Epoch [2], Iter [91/100] Loss: 0.7673\n",
      "Epoch [3], Iter [91/100] Loss: 0.7068\n",
      "Epoch [4], Iter [31/100] Loss: 0.7069"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38a58f",
   "metadata": {},
   "source": [
    "## 4.2.3 COMPARING RESULTS FOR DIFFERENT PREDICTOR COMBINATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1937da6",
   "metadata": {},
   "source": [
    "### tas only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a898b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e4e06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d0x2001e3cd9525e875_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.9993\n",
      "Epoch [2], Iter [91/100] Loss: 0.7778\n",
      "Epoch [3], Iter [91/100] Loss: 0.7906\n",
      "Epoch [4], Iter [91/100] Loss: 0.7242\n",
      "Epoch [5], Iter [91/100] Loss: 0.7667\n",
      "Epoch [6], Iter [91/100] Loss: 0.7126\n",
      "Epoch [7], Iter [91/100] Loss: 0.6960\n",
      "Epoch [8], Iter [91/100] Loss: 0.6596\n",
      "Epoch [9], Iter [91/100] Loss: 0.6678\n",
      "Epoch [10], Iter [91/100] Loss: 0.6552\n",
      "Epoch [11], Iter [91/100] Loss: 0.6615\n",
      "Epoch [12], Iter [91/100] Loss: 0.6439\n",
      "Epoch [13], Iter [91/100] Loss: 0.6480\n",
      "Epoch [14], Iter [91/100] Loss: 0.6441\n",
      "Epoch [15], Iter [91/100] Loss: 0.6686\n",
      "Epoch [16], Iter [91/100] Loss: 0.5737\n",
      "Test MSE: 0.7194887399673462\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d0x11513fb10c87ed79_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7710\n",
      "Epoch [2], Iter [91/100] Loss: 0.8351\n",
      "Epoch [3], Iter [91/100] Loss: 0.8805\n",
      "Epoch [4], Iter [91/100] Loss: 0.7504\n",
      "Epoch [5], Iter [91/100] Loss: 0.7041\n",
      "Epoch [6], Iter [91/100] Loss: 0.6803\n",
      "Epoch [7], Iter [91/100] Loss: 0.7063\n",
      "Epoch [8], Iter [91/100] Loss: 0.6691\n",
      "Epoch [9], Iter [91/100] Loss: 0.6612\n",
      "Epoch [10], Iter [91/100] Loss: 0.6427\n",
      "Epoch [11], Iter [91/100] Loss: 0.6280\n",
      "Epoch [12], Iter [91/100] Loss: 0.6228\n",
      "Epoch [13], Iter [91/100] Loss: 0.6376\n",
      "Epoch [14], Iter [91/100] Loss: 0.6223\n",
      "Epoch [15], Iter [91/100] Loss: 0.6143\n",
      "Epoch [16], Iter [91/100] Loss: 0.6100\n",
      "Epoch [17], Iter [91/100] Loss: 0.6025\n",
      "Epoch [18], Iter [91/100] Loss: 0.5691\n",
      "Epoch [19], Iter [91/100] Loss: 0.5767\n",
      "Epoch [20], Iter [91/100] Loss: 0.5737\n",
      "Epoch [21], Iter [91/100] Loss: 0.5738\n",
      "Test MSE: 0.7185825705528259\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d0x4eb25e6f7a6888f6_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8379\n",
      "Epoch [2], Iter [91/100] Loss: 0.7930\n",
      "Epoch [3], Iter [91/100] Loss: 0.7355\n",
      "Epoch [4], Iter [91/100] Loss: 0.6924\n",
      "Epoch [5], Iter [91/100] Loss: 0.6748\n",
      "Epoch [6], Iter [91/100] Loss: 0.7805\n",
      "Epoch [7], Iter [91/100] Loss: 0.6927\n",
      "Epoch [8], Iter [91/100] Loss: 0.6999\n",
      "Epoch [9], Iter [91/100] Loss: 0.6896\n",
      "Epoch [10], Iter [91/100] Loss: 0.6379\n",
      "Epoch [11], Iter [91/100] Loss: 0.6502\n",
      "Epoch [12], Iter [91/100] Loss: 0.6155\n",
      "Epoch [13], Iter [91/100] Loss: 0.6467\n",
      "Epoch [14], Iter [91/100] Loss: 0.6118\n",
      "Epoch [15], Iter [91/100] Loss: 0.6087\n",
      "Epoch [16], Iter [91/100] Loss: 0.6061\n",
      "Epoch [17], Iter [91/100] Loss: 0.5641\n",
      "Epoch [18], Iter [91/100] Loss: 0.5865\n",
      "Test MSE: 0.7199058532714844\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d-0x2ce381bef4c8b428_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8141\n",
      "Epoch [2], Iter [91/100] Loss: 0.8088\n",
      "Epoch [3], Iter [91/100] Loss: 0.7289\n",
      "Epoch [4], Iter [91/100] Loss: 0.8126\n",
      "Epoch [5], Iter [91/100] Loss: 0.7332\n",
      "Epoch [6], Iter [91/100] Loss: 0.6960\n",
      "Epoch [7], Iter [91/100] Loss: 0.7099\n",
      "Epoch [8], Iter [91/100] Loss: 0.6756\n",
      "Epoch [9], Iter [91/100] Loss: 0.6552\n",
      "Epoch [10], Iter [91/100] Loss: 0.6481\n",
      "Epoch [11], Iter [91/100] Loss: 0.6739\n",
      "Epoch [12], Iter [91/100] Loss: 0.6348\n",
      "Epoch [13], Iter [91/100] Loss: 0.6599\n",
      "Epoch [14], Iter [91/100] Loss: 0.6249\n",
      "Epoch [15], Iter [91/100] Loss: 0.6420\n",
      "Epoch [16], Iter [91/100] Loss: 0.6044\n",
      "Epoch [17], Iter [91/100] Loss: 0.6136\n",
      "Epoch [18], Iter [91/100] Loss: 0.5978\n",
      "Epoch [19], Iter [91/100] Loss: 0.5850\n",
      "Epoch [20], Iter [91/100] Loss: 0.5692\n",
      "Epoch [21], Iter [91/100] Loss: 0.5535\n",
      "Test MSE: 0.7140544056892395\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d-0x46f31af2bf27b0f2_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8166\n",
      "Epoch [2], Iter [91/100] Loss: 0.7797\n",
      "Epoch [3], Iter [91/100] Loss: 0.7954\n",
      "Epoch [4], Iter [91/100] Loss: 0.8296\n",
      "Epoch [5], Iter [91/100] Loss: 0.7077\n",
      "Epoch [6], Iter [91/100] Loss: 0.7312\n",
      "Epoch [7], Iter [91/100] Loss: 0.7602\n",
      "Epoch [8], Iter [91/100] Loss: 0.6812\n",
      "Epoch [9], Iter [91/100] Loss: 0.6812\n",
      "Epoch [10], Iter [91/100] Loss: 0.6644\n",
      "Epoch [11], Iter [91/100] Loss: 0.6832\n",
      "Epoch [12], Iter [91/100] Loss: 0.6481\n",
      "Epoch [13], Iter [91/100] Loss: 0.6244\n",
      "Epoch [14], Iter [91/100] Loss: 0.6328\n",
      "Epoch [15], Iter [91/100] Loss: 0.6211\n",
      "Epoch [16], Iter [91/100] Loss: 0.6171\n",
      "Epoch [17], Iter [91/100] Loss: 0.5932\n",
      "Epoch [18], Iter [91/100] Loss: 0.5756\n",
      "Epoch [19], Iter [91/100] Loss: 0.5956\n",
      "Test MSE: 0.7342720031738281\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d0x2190328005ebee9d_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8018\n",
      "Epoch [2], Iter [91/100] Loss: 0.8007\n",
      "Epoch [3], Iter [91/100] Loss: 0.7579\n",
      "Epoch [4], Iter [91/100] Loss: 0.7410\n",
      "Epoch [5], Iter [91/100] Loss: 0.7004\n",
      "Epoch [6], Iter [91/100] Loss: 0.6947\n",
      "Epoch [7], Iter [91/100] Loss: 0.6798\n",
      "Epoch [8], Iter [91/100] Loss: 0.7089\n",
      "Epoch [9], Iter [91/100] Loss: 0.6674\n",
      "Epoch [10], Iter [91/100] Loss: 0.6668\n",
      "Epoch [11], Iter [91/100] Loss: 0.6692\n",
      "Epoch [12], Iter [91/100] Loss: 0.6443\n",
      "Epoch [13], Iter [91/100] Loss: 0.6424\n",
      "Epoch [14], Iter [91/100] Loss: 0.6555\n",
      "Epoch [15], Iter [91/100] Loss: 0.6185\n",
      "Epoch [16], Iter [91/100] Loss: 0.5901\n",
      "Epoch [17], Iter [91/100] Loss: 0.5947\n",
      "Epoch [18], Iter [91/100] Loss: 0.5807\n",
      "Epoch [19], Iter [91/100] Loss: 0.5689\n",
      "Test MSE: 0.7366964221000671\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d0x49e2a3a5cafd50d5_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8306\n",
      "Epoch [2], Iter [91/100] Loss: 0.8081\n",
      "Epoch [3], Iter [91/100] Loss: 0.7758\n",
      "Epoch [4], Iter [91/100] Loss: 0.7303\n",
      "Epoch [5], Iter [91/100] Loss: 0.6818\n",
      "Epoch [6], Iter [91/100] Loss: 0.7356\n",
      "Epoch [7], Iter [91/100] Loss: 0.7047\n",
      "Epoch [8], Iter [91/100] Loss: 0.7082\n",
      "Epoch [9], Iter [91/100] Loss: 0.6660\n",
      "Epoch [10], Iter [91/100] Loss: 0.6667\n",
      "Epoch [11], Iter [91/100] Loss: 0.6585\n",
      "Epoch [12], Iter [91/100] Loss: 0.6609\n",
      "Epoch [13], Iter [91/100] Loss: 0.6251\n",
      "Epoch [14], Iter [91/100] Loss: 0.6174\n",
      "Epoch [15], Iter [91/100] Loss: 0.5932\n",
      "Epoch [16], Iter [91/100] Loss: 0.5992\n",
      "Epoch [17], Iter [91/100] Loss: 0.5892\n",
      "Epoch [18], Iter [91/100] Loss: 0.5937\n",
      "Epoch [19], Iter [91/100] Loss: 0.5624\n",
      "Test MSE: 0.7207685112953186\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d-0x12d3729313c1c517_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8600\n",
      "Epoch [2], Iter [91/100] Loss: 0.7661\n",
      "Epoch [3], Iter [91/100] Loss: 0.7502\n",
      "Epoch [4], Iter [91/100] Loss: 0.8316\n",
      "Epoch [5], Iter [91/100] Loss: 0.6891\n",
      "Epoch [6], Iter [91/100] Loss: 0.7164\n",
      "Epoch [7], Iter [91/100] Loss: 0.6589\n",
      "Epoch [8], Iter [91/100] Loss: 0.6931\n",
      "Epoch [9], Iter [91/100] Loss: 0.7181\n",
      "Epoch [10], Iter [91/100] Loss: 0.6739\n",
      "Epoch [11], Iter [91/100] Loss: 0.6711\n",
      "Epoch [12], Iter [91/100] Loss: 0.6411\n",
      "Epoch [13], Iter [91/100] Loss: 0.6321\n",
      "Epoch [14], Iter [91/100] Loss: 0.6627\n",
      "Epoch [15], Iter [91/100] Loss: 0.6185\n",
      "Epoch [16], Iter [91/100] Loss: 0.6132\n",
      "Epoch [17], Iter [91/100] Loss: 0.6236\n",
      "Epoch [18], Iter [91/100] Loss: 0.5626\n",
      "Test MSE: 0.7310577630996704\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d0x6a034e85eb27a24a_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8010\n",
      "Epoch [2], Iter [91/100] Loss: 0.7836\n",
      "Epoch [3], Iter [91/100] Loss: 0.8178\n",
      "Epoch [4], Iter [91/100] Loss: 0.7311\n",
      "Epoch [5], Iter [91/100] Loss: 0.7077\n",
      "Epoch [6], Iter [91/100] Loss: 0.7720\n",
      "Epoch [7], Iter [91/100] Loss: 0.7034\n",
      "Epoch [8], Iter [91/100] Loss: 0.7871\n",
      "Epoch [9], Iter [91/100] Loss: 0.6332\n",
      "Epoch [10], Iter [91/100] Loss: 0.6604\n",
      "Epoch [11], Iter [91/100] Loss: 0.6380\n",
      "Epoch [12], Iter [91/100] Loss: 0.6393\n",
      "Epoch [13], Iter [91/100] Loss: 0.6028\n",
      "Epoch [14], Iter [91/100] Loss: 0.5962\n",
      "Epoch [15], Iter [91/100] Loss: 0.6060\n",
      "Epoch [16], Iter [91/100] Loss: 0.5977\n",
      "Epoch [17], Iter [91/100] Loss: 0.5984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18], Iter [91/100] Loss: 0.5886\n",
      "Epoch [19], Iter [91/100] Loss: 0.6157\n",
      "Epoch [20], Iter [91/100] Loss: 0.5852\n",
      "Test MSE: 0.7288823127746582\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x564d001470fd038d-0x3cef6966096f05f6_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7740\n",
      "Epoch [2], Iter [91/100] Loss: 0.7954\n",
      "Epoch [3], Iter [91/100] Loss: 0.7801\n",
      "Epoch [4], Iter [91/100] Loss: 0.7312\n",
      "Epoch [5], Iter [91/100] Loss: 0.7360\n",
      "Epoch [6], Iter [91/100] Loss: 0.6493\n",
      "Epoch [7], Iter [91/100] Loss: 0.7400\n",
      "Epoch [8], Iter [91/100] Loss: 0.7104\n",
      "Epoch [9], Iter [91/100] Loss: 0.7056\n",
      "Epoch [10], Iter [91/100] Loss: 0.6587\n",
      "Epoch [11], Iter [91/100] Loss: 0.6419\n",
      "Epoch [12], Iter [91/100] Loss: 0.6266\n",
      "Epoch [13], Iter [91/100] Loss: 0.6291\n",
      "Epoch [14], Iter [91/100] Loss: 0.6371\n",
      "Epoch [15], Iter [91/100] Loss: 0.6212\n",
      "Epoch [16], Iter [91/100] Loss: 0.6702\n",
      "Epoch [17], Iter [91/100] Loss: 0.6097\n",
      "Epoch [18], Iter [91/100] Loss: 0.5912\n",
      "Epoch [19], Iter [91/100] Loss: 0.5689\n",
      "Epoch [20], Iter [91/100] Loss: 0.5546\n",
      "Test MSE: 0.7275220155715942\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder, use_tensorboard=True)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18c429",
   "metadata": {},
   "source": [
    "### precip only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4df08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a31566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd0x2001e3cd9525e875_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6778\n",
      "Epoch [2], Iter [91/100] Loss: 0.6760\n",
      "Epoch [3], Iter [91/100] Loss: 0.6392\n",
      "Epoch [4], Iter [91/100] Loss: 0.6244\n",
      "Epoch [5], Iter [91/100] Loss: 0.6426\n",
      "Epoch [6], Iter [91/100] Loss: 0.6194\n",
      "Epoch [7], Iter [91/100] Loss: 0.6248\n",
      "Epoch [8], Iter [91/100] Loss: 0.5763\n",
      "Epoch [9], Iter [91/100] Loss: 0.5810\n",
      "Epoch [10], Iter [91/100] Loss: 0.5633\n",
      "Epoch [11], Iter [91/100] Loss: 0.5865\n",
      "Epoch [12], Iter [91/100] Loss: 0.5539\n",
      "Epoch [13], Iter [91/100] Loss: 0.5596\n",
      "Epoch [14], Iter [91/100] Loss: 0.5475\n",
      "Epoch [15], Iter [91/100] Loss: 0.5489\n",
      "Epoch [16], Iter [91/100] Loss: 0.5361\n",
      "Epoch [17], Iter [91/100] Loss: 0.5594\n",
      "Epoch [18], Iter [91/100] Loss: 0.5377\n",
      "Epoch [19], Iter [91/100] Loss: 0.5296\n",
      "Epoch [20], Iter [91/100] Loss: 0.5019\n",
      "Test MSE: 0.6367641091346741\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd0x11513fb10c87ed79_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7727\n",
      "Epoch [2], Iter [91/100] Loss: 0.6664\n",
      "Epoch [3], Iter [91/100] Loss: 0.6489\n",
      "Epoch [4], Iter [91/100] Loss: 0.6241\n",
      "Epoch [5], Iter [91/100] Loss: 0.6332\n",
      "Epoch [6], Iter [91/100] Loss: 0.5799\n",
      "Epoch [7], Iter [91/100] Loss: 0.6308\n",
      "Epoch [8], Iter [91/100] Loss: 0.7576\n",
      "Epoch [9], Iter [91/100] Loss: 0.5918\n",
      "Epoch [10], Iter [91/100] Loss: 0.6051\n",
      "Epoch [11], Iter [91/100] Loss: 0.5886\n",
      "Epoch [12], Iter [91/100] Loss: 0.5637\n",
      "Epoch [13], Iter [91/100] Loss: 0.5619\n",
      "Epoch [14], Iter [91/100] Loss: 0.5540\n",
      "Epoch [15], Iter [91/100] Loss: 0.5453\n",
      "Epoch [16], Iter [91/100] Loss: 0.5399\n",
      "Epoch [17], Iter [91/100] Loss: 0.5325\n",
      "Test MSE: 0.6190158128738403\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd0x4eb25e6f7a6888f6_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6835\n",
      "Epoch [2], Iter [91/100] Loss: 0.6494\n",
      "Epoch [3], Iter [91/100] Loss: 0.6749\n",
      "Epoch [4], Iter [91/100] Loss: 0.6387\n",
      "Epoch [5], Iter [91/100] Loss: 0.6310\n",
      "Epoch [6], Iter [91/100] Loss: 0.6030\n",
      "Epoch [7], Iter [91/100] Loss: 0.6009\n",
      "Epoch [8], Iter [91/100] Loss: 0.6018\n",
      "Epoch [9], Iter [91/100] Loss: 0.5906\n",
      "Epoch [10], Iter [91/100] Loss: 0.5921\n",
      "Epoch [11], Iter [91/100] Loss: 0.5811\n",
      "Epoch [12], Iter [91/100] Loss: 0.5716\n",
      "Epoch [13], Iter [91/100] Loss: 0.5544\n",
      "Epoch [14], Iter [91/100] Loss: 0.5680\n",
      "Epoch [15], Iter [91/100] Loss: 0.5638\n",
      "Epoch [16], Iter [91/100] Loss: 0.5279\n",
      "Epoch [17], Iter [91/100] Loss: 0.5188\n",
      "Epoch [18], Iter [91/100] Loss: 0.5043\n",
      "Test MSE: 0.6337435245513916\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd-0x2ce381bef4c8b428_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7176\n",
      "Epoch [2], Iter [91/100] Loss: 0.6901\n",
      "Epoch [3], Iter [91/100] Loss: 0.6742\n",
      "Epoch [4], Iter [91/100] Loss: 0.6977\n",
      "Epoch [5], Iter [91/100] Loss: 0.6512\n",
      "Epoch [6], Iter [91/100] Loss: 0.5930\n",
      "Epoch [7], Iter [91/100] Loss: 0.5987\n",
      "Epoch [8], Iter [91/100] Loss: 0.5817\n",
      "Epoch [9], Iter [91/100] Loss: 0.5794\n",
      "Epoch [10], Iter [91/100] Loss: 0.5905\n",
      "Epoch [11], Iter [91/100] Loss: 0.5694\n",
      "Epoch [12], Iter [91/100] Loss: 0.5644\n",
      "Epoch [13], Iter [91/100] Loss: 0.5529\n",
      "Epoch [14], Iter [91/100] Loss: 0.5515\n",
      "Epoch [15], Iter [91/100] Loss: 0.5527\n",
      "Epoch [16], Iter [91/100] Loss: 0.5794\n",
      "Epoch [17], Iter [91/100] Loss: 0.5190\n",
      "Epoch [18], Iter [91/100] Loss: 0.5182\n",
      "Epoch [19], Iter [91/100] Loss: 0.5301\n",
      "Epoch [20], Iter [91/100] Loss: 0.4851\n",
      "Epoch [21], Iter [91/100] Loss: 0.4862\n",
      "Epoch [22], Iter [91/100] Loss: 0.4905\n",
      "Test MSE: 0.6358669400215149\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd-0x46f31af2bf27b0f2_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7075\n",
      "Epoch [2], Iter [91/100] Loss: 0.6378\n",
      "Epoch [3], Iter [91/100] Loss: 0.6505\n",
      "Epoch [4], Iter [91/100] Loss: 0.6300\n",
      "Epoch [5], Iter [91/100] Loss: 0.6508\n",
      "Epoch [6], Iter [91/100] Loss: 0.6546\n",
      "Epoch [7], Iter [91/100] Loss: 0.6049\n",
      "Epoch [8], Iter [91/100] Loss: 0.6181\n",
      "Epoch [9], Iter [91/100] Loss: 0.5787\n",
      "Epoch [10], Iter [91/100] Loss: 0.5861\n",
      "Epoch [11], Iter [91/100] Loss: 0.5512\n",
      "Epoch [12], Iter [91/100] Loss: 0.5711\n",
      "Epoch [13], Iter [91/100] Loss: 0.5675\n",
      "Epoch [14], Iter [91/100] Loss: 0.5587\n",
      "Epoch [15], Iter [91/100] Loss: 0.5489\n",
      "Epoch [16], Iter [91/100] Loss: 0.5535\n",
      "Epoch [17], Iter [91/100] Loss: 0.5096\n",
      "Epoch [18], Iter [91/100] Loss: 0.5115\n",
      "Epoch [19], Iter [91/100] Loss: 0.5117\n",
      "Epoch [20], Iter [91/100] Loss: 0.5174\n",
      "Test MSE: 0.6285592913627625\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd0x2190328005ebee9d_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6767\n",
      "Epoch [2], Iter [91/100] Loss: 0.6603\n",
      "Epoch [3], Iter [91/100] Loss: 0.6926\n",
      "Epoch [4], Iter [91/100] Loss: 0.6221\n",
      "Epoch [5], Iter [91/100] Loss: 0.6133\n",
      "Epoch [6], Iter [91/100] Loss: 0.5875\n",
      "Epoch [7], Iter [91/100] Loss: 0.6009\n",
      "Epoch [8], Iter [91/100] Loss: 0.6303\n",
      "Epoch [9], Iter [91/100] Loss: 0.5908\n",
      "Epoch [10], Iter [91/100] Loss: 0.6189\n",
      "Epoch [11], Iter [91/100] Loss: 0.5769\n",
      "Epoch [12], Iter [91/100] Loss: 0.5651\n",
      "Epoch [13], Iter [91/100] Loss: 0.5491\n",
      "Epoch [14], Iter [91/100] Loss: 0.5688\n",
      "Epoch [15], Iter [91/100] Loss: 0.5703\n",
      "Epoch [16], Iter [91/100] Loss: 0.5473\n",
      "Epoch [17], Iter [91/100] Loss: 0.5307\n",
      "Epoch [18], Iter [91/100] Loss: 0.5343\n",
      "Epoch [19], Iter [91/100] Loss: 0.5021\n",
      "Epoch [20], Iter [91/100] Loss: 0.5020\n",
      "Test MSE: 0.6368220448493958\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd0x49e2a3a5cafd50d5_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7176\n",
      "Epoch [2], Iter [91/100] Loss: 0.6268\n",
      "Epoch [3], Iter [91/100] Loss: 0.6484\n",
      "Epoch [4], Iter [91/100] Loss: 0.6307\n",
      "Epoch [5], Iter [91/100] Loss: 0.6682\n",
      "Epoch [6], Iter [91/100] Loss: 0.6443\n",
      "Epoch [7], Iter [91/100] Loss: 0.6044\n",
      "Epoch [8], Iter [91/100] Loss: 0.6213\n",
      "Epoch [9], Iter [91/100] Loss: 0.5628\n",
      "Epoch [10], Iter [91/100] Loss: 0.5891\n",
      "Epoch [11], Iter [91/100] Loss: 0.5701\n",
      "Epoch [12], Iter [91/100] Loss: 0.5392\n",
      "Epoch [13], Iter [91/100] Loss: 0.5753\n",
      "Epoch [14], Iter [91/100] Loss: 0.5464\n",
      "Epoch [15], Iter [91/100] Loss: 0.5567\n",
      "Epoch [16], Iter [91/100] Loss: 0.5265\n",
      "Test MSE: 0.6500351428985596\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd-0x12d3729313c1c517_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7133\n",
      "Epoch [2], Iter [91/100] Loss: 0.7425\n",
      "Epoch [3], Iter [91/100] Loss: 0.6611\n",
      "Epoch [4], Iter [91/100] Loss: 0.6004\n",
      "Epoch [5], Iter [91/100] Loss: 0.6633\n",
      "Epoch [6], Iter [91/100] Loss: 0.6303\n",
      "Epoch [7], Iter [91/100] Loss: 0.6114\n",
      "Epoch [8], Iter [91/100] Loss: 0.5843\n",
      "Epoch [9], Iter [91/100] Loss: 0.6217\n",
      "Epoch [10], Iter [91/100] Loss: 0.5748\n",
      "Epoch [11], Iter [91/100] Loss: 0.5875\n",
      "Epoch [12], Iter [91/100] Loss: 0.5784\n",
      "Epoch [13], Iter [91/100] Loss: 0.5530\n",
      "Epoch [14], Iter [91/100] Loss: 0.6133\n",
      "Epoch [15], Iter [91/100] Loss: 0.5579\n",
      "Epoch [16], Iter [91/100] Loss: 0.5429\n",
      "Epoch [17], Iter [91/100] Loss: 0.5432\n",
      "Test MSE: 0.6387669444084167\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd0x6a034e85eb27a24a_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6525\n",
      "Epoch [2], Iter [91/100] Loss: 0.6773\n",
      "Epoch [3], Iter [91/100] Loss: 0.6769\n",
      "Epoch [4], Iter [91/100] Loss: 0.6853\n",
      "Epoch [5], Iter [91/100] Loss: 0.6208\n",
      "Epoch [6], Iter [91/100] Loss: 0.6253\n",
      "Epoch [7], Iter [91/100] Loss: 0.6064\n",
      "Epoch [8], Iter [91/100] Loss: 0.6107\n",
      "Epoch [9], Iter [91/100] Loss: 0.5950\n",
      "Epoch [10], Iter [91/100] Loss: 0.6113\n",
      "Epoch [11], Iter [91/100] Loss: 0.5745\n",
      "Epoch [12], Iter [91/100] Loss: 0.5446\n",
      "Epoch [13], Iter [91/100] Loss: 0.5489\n",
      "Epoch [14], Iter [91/100] Loss: 0.5524\n",
      "Epoch [15], Iter [91/100] Loss: 0.6132\n",
      "Epoch [16], Iter [91/100] Loss: 0.5352\n",
      "Epoch [17], Iter [91/100] Loss: 0.5387\n",
      "Epoch [18], Iter [91/100] Loss: 0.5203\n",
      "Test MSE: 0.6361862421035767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce\\-0x3563f4e63a2f6cdd-0x3cef6966096f05f6_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6835\n",
      "Epoch [2], Iter [91/100] Loss: 0.6495\n",
      "Epoch [3], Iter [91/100] Loss: 0.6282\n",
      "Epoch [4], Iter [91/100] Loss: 0.6100\n",
      "Epoch [5], Iter [91/100] Loss: 0.6189\n",
      "Epoch [6], Iter [91/100] Loss: 0.6499\n",
      "Epoch [7], Iter [91/100] Loss: 0.6175\n",
      "Epoch [8], Iter [91/100] Loss: 0.6165\n",
      "Epoch [9], Iter [91/100] Loss: 0.5674\n",
      "Epoch [10], Iter [91/100] Loss: 0.5830\n",
      "Epoch [11], Iter [91/100] Loss: 0.5457\n",
      "Epoch [12], Iter [91/100] Loss: 0.5826\n",
      "Epoch [13], Iter [91/100] Loss: 0.5479\n",
      "Epoch [14], Iter [91/100] Loss: 0.5370\n",
      "Epoch [15], Iter [91/100] Loss: 0.5652\n",
      "Epoch [16], Iter [91/100] Loss: 0.5194\n",
      "Epoch [17], Iter [91/100] Loss: 0.5120\n",
      "Epoch [18], Iter [91/100] Loss: 0.5273\n",
      "Test MSE: 0.6369450092315674\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder, use_tensorboard=True)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a61f3",
   "metadata": {},
   "source": [
    "### slp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5db3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db7807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7981\n",
      "Epoch [2], Iter [91/100] Loss: 0.7657\n",
      "Epoch [3], Iter [91/100] Loss: 0.7499\n",
      "Epoch [4], Iter [91/100] Loss: 0.6663\n",
      "Epoch [5], Iter [91/100] Loss: 0.7047\n",
      "Epoch [6], Iter [91/100] Loss: 0.6446\n",
      "Epoch [7], Iter [91/100] Loss: 0.6370\n",
      "Epoch [8], Iter [91/100] Loss: 0.6690\n",
      "Epoch [9], Iter [91/100] Loss: 0.6429\n",
      "Epoch [10], Iter [91/100] Loss: 0.6333\n",
      "Epoch [11], Iter [91/100] Loss: 0.5972\n",
      "Epoch [12], Iter [91/100] Loss: 0.6218\n",
      "Epoch [13], Iter [91/100] Loss: 0.5829\n",
      "Epoch [14], Iter [91/100] Loss: 0.5820\n",
      "Epoch [15], Iter [91/100] Loss: 0.5641\n",
      "Epoch [16], Iter [91/100] Loss: 0.5871\n",
      "Epoch [17], Iter [91/100] Loss: 0.5804\n",
      "Epoch [18], Iter [91/100] Loss: 0.5508\n",
      "Epoch [19], Iter [91/100] Loss: 0.5835\n",
      "Test MSE: 0.6836003065109253\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8085\n",
      "Epoch [2], Iter [91/100] Loss: 0.7617\n",
      "Epoch [3], Iter [91/100] Loss: 0.6845\n",
      "Epoch [4], Iter [91/100] Loss: 0.7028\n",
      "Epoch [5], Iter [91/100] Loss: 0.6909\n",
      "Epoch [6], Iter [91/100] Loss: 0.6906\n",
      "Epoch [7], Iter [91/100] Loss: 0.6559\n",
      "Epoch [8], Iter [91/100] Loss: 0.6499\n",
      "Epoch [9], Iter [91/100] Loss: 0.6421\n",
      "Epoch [10], Iter [91/100] Loss: 0.6256\n",
      "Epoch [11], Iter [1/100] Loss: 0.6420"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28548b91",
   "metadata": {},
   "source": [
    "### slp, tas, precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0342f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\",\n",
    "                                \"temp\",\n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"],\n",
    "                                      \"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a67f9",
   "metadata": {},
   "source": [
    "### precip, tas, orogrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\",\n",
    "                                \"temp\",\n",
    "                                \"oro\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"oro\": [\"ht\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Global\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b775d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a818a7",
   "metadata": {},
   "source": [
    "## 4.2.4 Hyperparameter tuning\n",
    "\n",
    "Tune the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1352095",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.logspace(-4,-1,20)\n",
    "runs_per_lr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "    for i in range(runs_per_lr):\n",
    "        model_training_description[\"RUN_NR\"] = i\n",
    "        model_training_description[\"LEARNING_RATE\"] = lr  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "        unet = train_unet(description, model_training_description, output_folder)\n",
    "        predict_save_unet(description, model_training_description, output_folder, unet, output_folder)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb9ddb",
   "metadata": {},
   "source": [
    "### UNet wider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8531d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Global\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 64\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (64,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b07b27",
   "metadata": {},
   "source": [
    "### UNet deeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "# training parameters\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Global\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 4 # this changes compared to standard UNet\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa093ab",
   "metadata": {},
   "source": [
    "# 3) Precipitation weighting \n",
    "\n",
    "Appendix Table A3 in thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fa2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"temp\",\n",
    "                                \"precip\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"temp\": [\"temp_1\"],\n",
    "                                      \"precip\": [\"precip\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"dO18\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"temp\", \n",
    "                                   \"precip\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 654\n",
    "description[\"END_YEAR\"] = 1654\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a51e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112aec0",
   "metadata": {},
   "source": [
    "## 4) Monthly timescale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
