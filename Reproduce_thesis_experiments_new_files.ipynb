{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplement thesis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from predict import *\n",
    "from evaluate import *\n",
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Create datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tas, pr, oro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\",\n",
    "                                \"oro\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"oro\": [\"ht\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tas, pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tas, pr, slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "Specified dataset already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21876/1541587748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_yearly_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\datasets.py\u001b[0m in \u001b[0;36mcreate_yearly_dataset\u001b[1;34m(description, dataset_folder, output_folder)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_if_folder_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Specified dataset already exists.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: Specified dataset already exists."
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tas only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pr only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pr, tas precip weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_precip_weighted_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pr, tas ico grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "Specified dataset already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2969596/1541587748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_yearly_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/IcoCNN_new/datasets.py\u001b[0m in \u001b[0;36mcreate_yearly_dataset\u001b[0;34m(description, dataset_folder, output_folder)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_if_folder_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Specified dataset already exists.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: Specified dataset already exists."
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Run experiments yearly dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Modifications to flat UNet\n",
    "\n",
    "Start by selecting the tas, pr dataset without precipitation weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [\"Masked_MSELoss\", \"Masked_AreaWeightedMSELoss\"]\n",
    "use_coord_conv = [False, True]\n",
    "use_cylindrical_padding = [False, True]\n",
    "n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked_MSELoss False False 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7641\n",
      "Epoch [2], Iter [91/101] Loss: 0.6493\n",
      "Epoch [3], Iter [91/101] Loss: 0.6329\n",
      "Epoch [4], Iter [91/101] Loss: 0.5696\n",
      "Epoch [5], Iter [91/101] Loss: 0.6191\n",
      "Epoch [6], Iter [91/101] Loss: 0.5892\n",
      "Epoch [7], Iter [91/101] Loss: 0.5546\n",
      "Epoch [8], Iter [91/101] Loss: 0.5472\n",
      "Epoch [9], Iter [91/101] Loss: 0.5298\n",
      "Epoch [10], Iter [91/101] Loss: 0.5203\n",
      "Epoch [11], Iter [91/101] Loss: 0.5182\n",
      "Epoch [12], Iter [91/101] Loss: 0.5299\n",
      "Epoch [13], Iter [91/101] Loss: 0.4826\n",
      "Epoch [14], Iter [91/101] Loss: 0.4676\n",
      "Epoch [15], Iter [91/101] Loss: 0.5369\n",
      "Epoch [16], Iter [91/101] Loss: 0.4621\n",
      "Epoch [17], Iter [91/101] Loss: 0.5077\n",
      "Test MSE: 0.6426965594291687\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6987\n",
      "Epoch [2], Iter [91/101] Loss: 0.6966\n",
      "Epoch [3], Iter [91/101] Loss: 0.6113\n",
      "Epoch [4], Iter [91/101] Loss: 0.6444\n",
      "Epoch [5], Iter [91/101] Loss: 0.5613\n",
      "Epoch [6], Iter [91/101] Loss: 0.5704\n",
      "Epoch [7], Iter [91/101] Loss: 0.5506\n",
      "Epoch [8], Iter [91/101] Loss: 0.5901\n",
      "Epoch [9], Iter [91/101] Loss: 0.5341\n",
      "Epoch [10], Iter [91/101] Loss: 0.5220\n",
      "Epoch [11], Iter [91/101] Loss: 0.4952\n",
      "Epoch [12], Iter [91/101] Loss: 0.5002\n",
      "Epoch [13], Iter [91/101] Loss: 0.4876\n",
      "Epoch [14], Iter [91/101] Loss: 0.4921\n",
      "Epoch [15], Iter [91/101] Loss: 0.4770\n",
      "Test MSE: 0.6412099003791809\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7056\n",
      "Epoch [2], Iter [91/101] Loss: 0.7603\n",
      "Epoch [3], Iter [91/101] Loss: 0.6371\n",
      "Epoch [4], Iter [91/101] Loss: 0.5857\n",
      "Epoch [5], Iter [91/101] Loss: 0.5772\n",
      "Epoch [6], Iter [91/101] Loss: 0.5718\n",
      "Epoch [7], Iter [91/101] Loss: 0.5616\n",
      "Epoch [8], Iter [91/101] Loss: 0.5297\n",
      "Epoch [9], Iter [91/101] Loss: 0.5325\n",
      "Epoch [10], Iter [91/101] Loss: 0.5216\n",
      "Test MSE: 0.6646396517753601\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7002\n",
      "Epoch [2], Iter [91/101] Loss: 0.6417\n",
      "Epoch [3], Iter [91/101] Loss: 0.5761\n",
      "Epoch [4], Iter [91/101] Loss: 0.6123\n",
      "Epoch [5], Iter [91/101] Loss: 0.7481\n",
      "Epoch [6], Iter [91/101] Loss: 0.5945\n",
      "Epoch [7], Iter [91/101] Loss: 0.5759\n",
      "Epoch [8], Iter [91/101] Loss: 0.5671\n",
      "Epoch [9], Iter [91/101] Loss: 0.5384\n",
      "Epoch [10], Iter [91/101] Loss: 0.5131\n",
      "Epoch [11], Iter [91/101] Loss: 0.5290\n",
      "Epoch [12], Iter [91/101] Loss: 0.5136\n",
      "Epoch [13], Iter [91/101] Loss: 0.5558\n",
      "Epoch [14], Iter [91/101] Loss: 0.9878\n",
      "Epoch [15], Iter [91/101] Loss: 0.4743\n",
      "Test MSE: 0.6567184329032898\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 4\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7123\n",
      "Epoch [2], Iter [91/101] Loss: 0.6592\n",
      "Epoch [3], Iter [91/101] Loss: 0.6222\n",
      "Epoch [4], Iter [91/101] Loss: 0.6516\n",
      "Epoch [5], Iter [91/101] Loss: 0.5664\n",
      "Epoch [6], Iter [91/101] Loss: 0.6213\n",
      "Epoch [7], Iter [91/101] Loss: 0.5515\n",
      "Epoch [8], Iter [91/101] Loss: 0.5512\n",
      "Epoch [9], Iter [91/101] Loss: 0.5152\n",
      "Epoch [10], Iter [91/101] Loss: 0.5266\n",
      "Epoch [11], Iter [91/101] Loss: 0.5156\n",
      "Epoch [12], Iter [91/101] Loss: 0.5198\n",
      "Epoch [13], Iter [91/101] Loss: 0.4983\n",
      "Epoch [14], Iter [91/101] Loss: 0.4721\n",
      "Epoch [15], Iter [91/101] Loss: 0.4820\n",
      "Epoch [16], Iter [91/101] Loss: 0.4701\n",
      "Epoch [17], Iter [91/101] Loss: 0.4495\n",
      "Test MSE: 0.649075448513031\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 5\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6316\n",
      "Epoch [2], Iter [91/101] Loss: 0.6329\n",
      "Epoch [3], Iter [91/101] Loss: 0.5933\n",
      "Epoch [4], Iter [91/101] Loss: 0.6044\n",
      "Epoch [5], Iter [91/101] Loss: 0.6065\n",
      "Epoch [6], Iter [91/101] Loss: 0.5932\n",
      "Epoch [7], Iter [91/101] Loss: 0.5687\n",
      "Epoch [8], Iter [91/101] Loss: 0.5148\n",
      "Epoch [9], Iter [91/101] Loss: 0.5608\n",
      "Epoch [10], Iter [91/101] Loss: 0.5290\n",
      "Epoch [11], Iter [91/101] Loss: 0.6063\n",
      "Epoch [12], Iter [91/101] Loss: 0.5008\n",
      "Epoch [13], Iter [91/101] Loss: 0.5117\n",
      "Epoch [14], Iter [91/101] Loss: 0.4762\n",
      "Epoch [15], Iter [91/101] Loss: 0.5511\n",
      "Test MSE: 0.6361759305000305\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 6\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7367\n",
      "Epoch [2], Iter [91/101] Loss: 0.6315\n",
      "Epoch [3], Iter [91/101] Loss: 0.6479\n",
      "Epoch [4], Iter [91/101] Loss: 0.6102\n",
      "Epoch [5], Iter [91/101] Loss: 0.5964\n",
      "Epoch [6], Iter [91/101] Loss: 0.5842\n",
      "Epoch [7], Iter [91/101] Loss: 0.5480\n",
      "Epoch [8], Iter [91/101] Loss: 0.5373\n",
      "Epoch [9], Iter [91/101] Loss: 0.5290\n",
      "Epoch [10], Iter [91/101] Loss: 0.5142\n",
      "Epoch [11], Iter [91/101] Loss: 0.5328\n",
      "Epoch [12], Iter [91/101] Loss: 0.5138\n",
      "Epoch [13], Iter [91/101] Loss: 0.4926\n",
      "Epoch [14], Iter [91/101] Loss: 0.5367\n",
      "Epoch [15], Iter [91/101] Loss: 0.4760\n",
      "Epoch [16], Iter [91/101] Loss: 0.4964\n",
      "Test MSE: 0.6393559575080872\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 7\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6688\n",
      "Epoch [2], Iter [91/101] Loss: 0.6234\n",
      "Epoch [3], Iter [91/101] Loss: 0.6219\n",
      "Epoch [4], Iter [91/101] Loss: 0.6700\n",
      "Epoch [5], Iter [91/101] Loss: 0.5691\n",
      "Epoch [6], Iter [91/101] Loss: 0.5616\n",
      "Epoch [7], Iter [91/101] Loss: 0.5632\n",
      "Epoch [8], Iter [91/101] Loss: 0.5554\n",
      "Epoch [9], Iter [91/101] Loss: 0.5295\n",
      "Epoch [10], Iter [91/101] Loss: 0.4992\n",
      "Epoch [11], Iter [91/101] Loss: 0.4960\n",
      "Epoch [12], Iter [91/101] Loss: 0.4863\n",
      "Epoch [13], Iter [91/101] Loss: 0.4817\n",
      "Epoch [14], Iter [91/101] Loss: 0.5301\n",
      "Test MSE: 0.6325976252555847\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 8\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7515\n",
      "Epoch [2], Iter [91/101] Loss: 0.6510\n",
      "Epoch [3], Iter [91/101] Loss: 0.6053\n",
      "Epoch [4], Iter [91/101] Loss: 0.6155\n",
      "Epoch [5], Iter [91/101] Loss: 0.5642\n",
      "Epoch [6], Iter [91/101] Loss: 0.5875\n",
      "Epoch [7], Iter [91/101] Loss: 0.5506\n",
      "Epoch [8], Iter [91/101] Loss: 0.5515\n",
      "Epoch [9], Iter [91/101] Loss: 0.5323\n",
      "Test MSE: 0.6903131604194641\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 9\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6390\n",
      "Epoch [2], Iter [91/101] Loss: 0.6360\n",
      "Epoch [3], Iter [91/101] Loss: 0.6297\n",
      "Epoch [4], Iter [91/101] Loss: 0.5900\n",
      "Epoch [5], Iter [91/101] Loss: 0.5768\n",
      "Epoch [6], Iter [91/101] Loss: 0.5682\n",
      "Epoch [7], Iter [91/101] Loss: 0.5595\n",
      "Epoch [8], Iter [91/101] Loss: 0.5331\n",
      "Epoch [9], Iter [91/101] Loss: 0.5293\n",
      "Epoch [10], Iter [91/101] Loss: 1.1326\n",
      "Epoch [11], Iter [91/101] Loss: 0.5414\n",
      "Epoch [12], Iter [91/101] Loss: 0.5209\n",
      "Epoch [13], Iter [91/101] Loss: 0.4622\n",
      "Epoch [14], Iter [91/101] Loss: 0.4743\n",
      "Epoch [15], Iter [91/101] Loss: 0.4773\n",
      "Test MSE: 0.6343790292739868\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6190\n",
      "Epoch [2], Iter [91/101] Loss: 0.6615\n",
      "Epoch [3], Iter [91/101] Loss: 0.6276\n",
      "Epoch [4], Iter [91/101] Loss: 0.5827\n",
      "Epoch [5], Iter [91/101] Loss: 0.5869\n",
      "Epoch [6], Iter [91/101] Loss: 0.5639\n",
      "Epoch [7], Iter [91/101] Loss: 0.5345\n",
      "Epoch [8], Iter [91/101] Loss: 0.5270\n",
      "Epoch [9], Iter [91/101] Loss: 0.5338\n",
      "Epoch [10], Iter [91/101] Loss: 0.5093\n",
      "Epoch [11], Iter [91/101] Loss: 0.5239\n",
      "Test MSE: 0.6789689064025879\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6785\n",
      "Epoch [2], Iter [91/101] Loss: 0.6212\n",
      "Epoch [3], Iter [91/101] Loss: 0.5782\n",
      "Epoch [4], Iter [91/101] Loss: 0.6580\n",
      "Epoch [5], Iter [91/101] Loss: 0.5843\n",
      "Epoch [6], Iter [91/101] Loss: 0.5670\n",
      "Epoch [7], Iter [91/101] Loss: 0.5609\n",
      "Epoch [8], Iter [91/101] Loss: 0.5413\n",
      "Epoch [9], Iter [91/101] Loss: 0.5282\n",
      "Epoch [10], Iter [91/101] Loss: 0.5036\n",
      "Epoch [11], Iter [91/101] Loss: 0.5050\n",
      "Epoch [12], Iter [91/101] Loss: 0.4946\n",
      "Epoch [13], Iter [91/101] Loss: 0.4886\n",
      "Epoch [14], Iter [91/101] Loss: 0.4754\n",
      "Epoch [15], Iter [91/101] Loss: 0.4703\n",
      "Test MSE: 0.6406639218330383\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7026\n",
      "Epoch [2], Iter [91/101] Loss: 0.6928\n",
      "Epoch [3], Iter [91/101] Loss: 0.6163\n",
      "Epoch [4], Iter [91/101] Loss: 0.5704\n",
      "Epoch [5], Iter [91/101] Loss: 0.5784\n",
      "Epoch [6], Iter [91/101] Loss: 0.5669\n",
      "Epoch [7], Iter [91/101] Loss: 0.6448\n",
      "Epoch [8], Iter [91/101] Loss: 0.6024\n",
      "Epoch [9], Iter [91/101] Loss: 0.5407\n",
      "Epoch [10], Iter [91/101] Loss: 0.5264\n",
      "Epoch [11], Iter [91/101] Loss: 0.5053\n",
      "Epoch [12], Iter [91/101] Loss: 0.5297\n",
      "Epoch [13], Iter [91/101] Loss: 0.5398\n",
      "Epoch [14], Iter [91/101] Loss: 0.4783\n",
      "Epoch [15], Iter [91/101] Loss: 0.4875\n",
      "Test MSE: 0.6427064538002014\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6844\n",
      "Epoch [2], Iter [91/101] Loss: 0.6587\n",
      "Epoch [3], Iter [91/101] Loss: 0.6233\n",
      "Epoch [4], Iter [91/101] Loss: 0.5802\n",
      "Epoch [5], Iter [91/101] Loss: 0.5529\n",
      "Epoch [6], Iter [91/101] Loss: 0.5491\n",
      "Epoch [7], Iter [91/101] Loss: 0.5354\n",
      "Epoch [8], Iter [91/101] Loss: 0.5855\n",
      "Epoch [9], Iter [91/101] Loss: 0.5447\n",
      "Epoch [10], Iter [91/101] Loss: 0.5230\n",
      "Epoch [11], Iter [91/101] Loss: 0.5248\n",
      "Epoch [12], Iter [91/101] Loss: 0.4956\n",
      "Epoch [13], Iter [91/101] Loss: 0.5191\n",
      "Epoch [14], Iter [91/101] Loss: 0.4809\n",
      "Epoch [15], Iter [91/101] Loss: 0.4537\n",
      "Test MSE: 0.643721878528595\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 4\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6965\n",
      "Epoch [2], Iter [91/101] Loss: 0.6450\n",
      "Epoch [3], Iter [91/101] Loss: 0.6274\n",
      "Epoch [4], Iter [91/101] Loss: 0.5827\n",
      "Epoch [5], Iter [91/101] Loss: 0.6056\n",
      "Epoch [6], Iter [91/101] Loss: 0.5514\n",
      "Epoch [7], Iter [91/101] Loss: 0.5813\n",
      "Epoch [8], Iter [91/101] Loss: 0.5300\n",
      "Epoch [9], Iter [91/101] Loss: 0.5095\n",
      "Epoch [10], Iter [91/101] Loss: 0.5091\n",
      "Epoch [11], Iter [91/101] Loss: 0.5136\n",
      "Epoch [12], Iter [91/101] Loss: 0.4821\n",
      "Epoch [13], Iter [91/101] Loss: 0.4780\n",
      "Epoch [14], Iter [91/101] Loss: 0.4792\n",
      "Epoch [15], Iter [91/101] Loss: 0.4713\n",
      "Epoch [16], Iter [91/101] Loss: 0.4488\n",
      "Test MSE: 0.6489227414131165\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 5\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6692\n",
      "Epoch [2], Iter [91/101] Loss: 0.6408\n",
      "Epoch [3], Iter [91/101] Loss: 0.6069\n",
      "Epoch [4], Iter [91/101] Loss: 0.5926\n",
      "Epoch [5], Iter [91/101] Loss: 0.5580\n",
      "Epoch [6], Iter [91/101] Loss: 0.5676\n",
      "Epoch [7], Iter [91/101] Loss: 0.5444\n",
      "Epoch [8], Iter [91/101] Loss: 0.5227\n",
      "Epoch [9], Iter [91/101] Loss: 0.5401\n",
      "Epoch [10], Iter [91/101] Loss: 0.5782\n",
      "Epoch [11], Iter [91/101] Loss: 0.5096\n",
      "Epoch [12], Iter [91/101] Loss: 0.4783\n",
      "Epoch [13], Iter [91/101] Loss: 0.4957\n",
      "Epoch [14], Iter [91/101] Loss: 0.4485\n",
      "Test MSE: 0.6320208311080933\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 6\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6969\n",
      "Epoch [2], Iter [91/101] Loss: 0.6071\n",
      "Epoch [3], Iter [91/101] Loss: 0.6568\n",
      "Epoch [4], Iter [91/101] Loss: 0.5814\n",
      "Epoch [5], Iter [91/101] Loss: 0.5916\n",
      "Epoch [6], Iter [91/101] Loss: 0.5486\n",
      "Epoch [7], Iter [91/101] Loss: 0.5655\n",
      "Epoch [8], Iter [91/101] Loss: 0.5377\n",
      "Epoch [9], Iter [91/101] Loss: 0.5222\n",
      "Epoch [10], Iter [91/101] Loss: 0.5559\n",
      "Epoch [11], Iter [91/101] Loss: 0.5324\n",
      "Epoch [12], Iter [91/101] Loss: 0.5171\n",
      "Epoch [13], Iter [91/101] Loss: 0.4847\n",
      "Epoch [14], Iter [91/101] Loss: 0.5814\n",
      "Epoch [15], Iter [91/101] Loss: 0.4821\n",
      "Test MSE: 0.6461873650550842\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 7\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6314\n",
      "Epoch [2], Iter [91/101] Loss: 0.6064\n",
      "Epoch [3], Iter [91/101] Loss: 0.6718\n",
      "Epoch [4], Iter [91/101] Loss: 0.6002\n",
      "Epoch [5], Iter [91/101] Loss: 0.5813\n",
      "Epoch [6], Iter [91/101] Loss: 0.5492\n",
      "Epoch [7], Iter [91/101] Loss: 0.5534\n",
      "Epoch [8], Iter [91/101] Loss: 0.5561\n",
      "Epoch [9], Iter [91/101] Loss: 0.5358\n",
      "Epoch [10], Iter [91/101] Loss: 0.5261\n",
      "Epoch [11], Iter [91/101] Loss: 0.5003\n",
      "Epoch [12], Iter [91/101] Loss: 0.4879\n",
      "Epoch [13], Iter [91/101] Loss: 0.4824\n",
      "Epoch [14], Iter [91/101] Loss: 0.4632\n",
      "Test MSE: 0.6408135294914246\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 8\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6556\n",
      "Epoch [2], Iter [91/101] Loss: 0.6019\n",
      "Epoch [3], Iter [91/101] Loss: 0.6066\n",
      "Epoch [4], Iter [91/101] Loss: 0.6908\n",
      "Epoch [5], Iter [91/101] Loss: 0.5646\n",
      "Epoch [6], Iter [91/101] Loss: 0.5626\n",
      "Epoch [7], Iter [91/101] Loss: 0.5431\n",
      "Epoch [8], Iter [91/101] Loss: 0.5260\n",
      "Epoch [9], Iter [91/101] Loss: 0.5247\n",
      "Epoch [10], Iter [91/101] Loss: 0.5291\n",
      "Epoch [11], Iter [91/101] Loss: 0.5051\n",
      "Epoch [12], Iter [91/101] Loss: 0.4862\n",
      "Epoch [13], Iter [91/101] Loss: 0.5005\n",
      "Epoch [14], Iter [91/101] Loss: 0.4793\n",
      "Epoch [15], Iter [91/101] Loss: 0.4573\n",
      "Epoch [16], Iter [91/101] Loss: 0.4497\n",
      "Test MSE: 0.6523646712303162\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 9\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8012\n",
      "Epoch [2], Iter [91/101] Loss: 0.6561\n",
      "Epoch [3], Iter [91/101] Loss: 0.5904\n",
      "Epoch [4], Iter [91/101] Loss: 0.6083\n",
      "Epoch [5], Iter [91/101] Loss: 0.5779\n",
      "Epoch [6], Iter [91/101] Loss: 0.5543\n",
      "Epoch [7], Iter [91/101] Loss: 0.5543\n",
      "Epoch [8], Iter [91/101] Loss: 0.5489\n",
      "Epoch [9], Iter [91/101] Loss: 0.5338\n",
      "Epoch [10], Iter [91/101] Loss: 0.5084\n",
      "Epoch [11], Iter [91/101] Loss: 0.5599\n",
      "Epoch [12], Iter [91/101] Loss: 0.5164\n",
      "Epoch [13], Iter [91/101] Loss: 0.4776\n",
      "Epoch [14], Iter [91/101] Loss: 0.4821\n",
      "Epoch [15], Iter [91/101] Loss: 0.4757\n",
      "Epoch [16], Iter [91/101] Loss: 0.4628\n",
      "Test MSE: 0.6485497355461121\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6591\n",
      "Epoch [2], Iter [91/101] Loss: 0.8172\n",
      "Epoch [3], Iter [91/101] Loss: 0.5992\n",
      "Epoch [4], Iter [91/101] Loss: 0.5844\n",
      "Epoch [5], Iter [91/101] Loss: 0.5657\n",
      "Epoch [6], Iter [91/101] Loss: 0.5371\n",
      "Epoch [7], Iter [91/101] Loss: 0.5667\n",
      "Epoch [8], Iter [91/101] Loss: 1.1754\n",
      "Epoch [9], Iter [91/101] Loss: 0.5308\n",
      "Epoch [10], Iter [91/101] Loss: 0.5751\n",
      "Epoch [11], Iter [91/101] Loss: 0.5432\n",
      "Epoch [12], Iter [91/101] Loss: 0.5249\n",
      "Epoch [13], Iter [91/101] Loss: 0.5139\n",
      "Epoch [14], Iter [91/101] Loss: 0.5544\n",
      "Epoch [15], Iter [91/101] Loss: 0.4816\n",
      "Epoch [16], Iter [91/101] Loss: 0.4920\n",
      "Epoch [17], Iter [91/101] Loss: 0.4868\n",
      "Epoch [18], Iter [91/101] Loss: 0.4700\n",
      "Epoch [19], Iter [91/101] Loss: 0.4764\n",
      "Epoch [20], Iter [91/101] Loss: 0.4755\n",
      "Epoch [21], Iter [91/101] Loss: 0.4611\n",
      "Epoch [22], Iter [91/101] Loss: 0.4673\n",
      "Epoch [23], Iter [91/101] Loss: 0.4451\n",
      "Test MSE: 0.6289869546890259\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6022\n",
      "Epoch [2], Iter [91/101] Loss: 0.6363\n",
      "Epoch [3], Iter [91/101] Loss: 0.6850\n",
      "Epoch [4], Iter [91/101] Loss: 0.6191\n",
      "Epoch [5], Iter [91/101] Loss: 0.5803\n",
      "Epoch [6], Iter [91/101] Loss: 0.6045\n",
      "Epoch [7], Iter [91/101] Loss: 0.5721\n",
      "Epoch [8], Iter [91/101] Loss: 1.2370\n",
      "Epoch [9], Iter [91/101] Loss: 0.5196\n",
      "Epoch [10], Iter [91/101] Loss: 0.6093\n",
      "Epoch [11], Iter [91/101] Loss: 0.5344\n",
      "Epoch [12], Iter [91/101] Loss: 0.5135\n",
      "Epoch [13], Iter [91/101] Loss: 0.5387\n",
      "Epoch [14], Iter [91/101] Loss: 0.5210\n",
      "Epoch [15], Iter [91/101] Loss: 0.4903\n",
      "Epoch [16], Iter [91/101] Loss: 0.5068\n",
      "Epoch [17], Iter [91/101] Loss: 0.5186\n",
      "Epoch [18], Iter [91/101] Loss: 0.4805\n",
      "Epoch [19], Iter [91/101] Loss: 0.4771\n",
      "Test MSE: 0.6314963698387146\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6298\n",
      "Epoch [2], Iter [91/101] Loss: 0.6415\n",
      "Epoch [3], Iter [91/101] Loss: 0.6199\n",
      "Epoch [4], Iter [91/101] Loss: 0.6356\n",
      "Epoch [5], Iter [91/101] Loss: 0.5723\n",
      "Epoch [6], Iter [91/101] Loss: 0.5819\n",
      "Epoch [7], Iter [91/101] Loss: 0.5409\n",
      "Epoch [8], Iter [91/101] Loss: 0.5487\n",
      "Epoch [9], Iter [91/101] Loss: 0.5483\n",
      "Epoch [10], Iter [91/101] Loss: 0.5458\n",
      "Test MSE: 0.6625823974609375\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6720\n",
      "Epoch [2], Iter [91/101] Loss: 0.6264\n",
      "Epoch [3], Iter [91/101] Loss: 0.6307\n",
      "Epoch [4], Iter [91/101] Loss: 0.5996\n",
      "Epoch [5], Iter [91/101] Loss: 0.5938\n",
      "Epoch [6], Iter [91/101] Loss: 0.5910\n",
      "Epoch [7], Iter [91/101] Loss: 0.5487\n",
      "Epoch [8], Iter [91/101] Loss: 0.5857\n",
      "Epoch [9], Iter [91/101] Loss: 0.5422\n",
      "Epoch [10], Iter [91/101] Loss: 0.5404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Iter [91/101] Loss: 0.6185\n",
      "Epoch [12], Iter [91/101] Loss: 0.5309\n",
      "Epoch [13], Iter [91/101] Loss: 0.5266\n",
      "Epoch [14], Iter [91/101] Loss: 0.5181\n",
      "Test MSE: 0.6279321312904358\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 4\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6952\n",
      "Epoch [2], Iter [91/101] Loss: 0.6635\n",
      "Epoch [3], Iter [91/101] Loss: 0.7132\n",
      "Epoch [4], Iter [91/101] Loss: 0.6199\n",
      "Epoch [5], Iter [91/101] Loss: 0.6088\n",
      "Epoch [6], Iter [91/101] Loss: 0.6131\n",
      "Epoch [7], Iter [91/101] Loss: 0.5581\n",
      "Epoch [8], Iter [91/101] Loss: 0.5776\n",
      "Epoch [9], Iter [91/101] Loss: 0.5805\n",
      "Epoch [10], Iter [91/101] Loss: 0.5345\n",
      "Epoch [11], Iter [91/101] Loss: 0.5306\n",
      "Epoch [12], Iter [91/101] Loss: 0.5269\n",
      "Epoch [13], Iter [91/101] Loss: 0.5181\n",
      "Epoch [14], Iter [91/101] Loss: 0.5097\n",
      "Epoch [15], Iter [91/101] Loss: 0.5016\n",
      "Test MSE: 0.6302862167358398\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 5\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6524\n",
      "Epoch [2], Iter [91/101] Loss: 0.6463\n",
      "Epoch [3], Iter [91/101] Loss: 0.6550\n",
      "Epoch [4], Iter [91/101] Loss: 0.5870\n",
      "Epoch [5], Iter [91/101] Loss: 0.5822\n",
      "Epoch [6], Iter [91/101] Loss: 0.6095\n",
      "Epoch [7], Iter [91/101] Loss: 0.5897\n",
      "Epoch [8], Iter [91/101] Loss: 0.5756\n",
      "Epoch [9], Iter [91/101] Loss: 0.5627\n",
      "Epoch [10], Iter [91/101] Loss: 0.5878\n",
      "Epoch [11], Iter [91/101] Loss: 0.5292\n",
      "Epoch [12], Iter [91/101] Loss: 0.5132\n",
      "Epoch [13], Iter [91/101] Loss: 0.5187\n",
      "Epoch [14], Iter [91/101] Loss: 0.5917\n",
      "Epoch [15], Iter [91/101] Loss: 0.5097\n",
      "Epoch [16], Iter [91/101] Loss: 0.4908\n",
      "Test MSE: 0.6206768155097961\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 6\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7025\n",
      "Epoch [2], Iter [91/101] Loss: 0.6004\n",
      "Epoch [3], Iter [91/101] Loss: 0.6254\n",
      "Epoch [4], Iter [91/101] Loss: 0.5995\n",
      "Epoch [5], Iter [91/101] Loss: 0.5944\n",
      "Epoch [6], Iter [91/101] Loss: 0.5678\n",
      "Epoch [7], Iter [91/101] Loss: 0.5839\n",
      "Epoch [8], Iter [91/101] Loss: 0.5637\n",
      "Epoch [9], Iter [91/101] Loss: 0.5832\n",
      "Epoch [10], Iter [91/101] Loss: 0.5612\n",
      "Epoch [11], Iter [91/101] Loss: 0.5261\n",
      "Epoch [12], Iter [91/101] Loss: 0.5374\n",
      "Epoch [13], Iter [91/101] Loss: 0.4983\n",
      "Epoch [14], Iter [91/101] Loss: 0.5046\n",
      "Epoch [15], Iter [91/101] Loss: 0.5376\n",
      "Epoch [16], Iter [91/101] Loss: 0.5219\n",
      "Epoch [17], Iter [91/101] Loss: 0.5185\n",
      "Epoch [18], Iter [91/101] Loss: 0.4858\n",
      "Epoch [19], Iter [91/101] Loss: 0.4976\n",
      "Epoch [20], Iter [91/101] Loss: 0.4858\n",
      "Test MSE: 0.6214090585708618\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 7\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7392\n",
      "Epoch [2], Iter [91/101] Loss: 0.6440\n",
      "Epoch [3], Iter [91/101] Loss: 0.6342\n",
      "Epoch [4], Iter [91/101] Loss: 0.5789\n",
      "Epoch [5], Iter [91/101] Loss: 0.6239\n",
      "Epoch [6], Iter [91/101] Loss: 0.5742\n",
      "Epoch [7], Iter [91/101] Loss: 0.5388\n",
      "Epoch [8], Iter [91/101] Loss: 0.5462\n",
      "Epoch [9], Iter [91/101] Loss: 0.5472\n",
      "Epoch [10], Iter [91/101] Loss: 0.5420\n",
      "Epoch [11], Iter [91/101] Loss: 0.5706\n",
      "Epoch [12], Iter [91/101] Loss: 0.5194\n",
      "Epoch [13], Iter [91/101] Loss: 0.5256\n",
      "Epoch [14], Iter [91/101] Loss: 0.5179\n",
      "Epoch [15], Iter [91/101] Loss: 0.5145\n",
      "Epoch [16], Iter [91/101] Loss: 0.5625\n",
      "Epoch [17], Iter [91/101] Loss: 0.5050\n",
      "Epoch [18], Iter [91/101] Loss: 0.5239\n",
      "Epoch [19], Iter [91/101] Loss: 0.5196\n",
      "Epoch [20], Iter [91/101] Loss: 0.5086\n",
      "Test MSE: 0.6306899189949036\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 8\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6341\n",
      "Epoch [2], Iter [91/101] Loss: 0.6334\n",
      "Epoch [3], Iter [91/101] Loss: 0.6116\n",
      "Epoch [4], Iter [91/101] Loss: 0.6008\n",
      "Epoch [5], Iter [91/101] Loss: 0.6191\n",
      "Epoch [6], Iter [91/101] Loss: 0.5636\n",
      "Epoch [7], Iter [91/101] Loss: 0.5780\n",
      "Epoch [8], Iter [91/101] Loss: 0.5522\n",
      "Epoch [9], Iter [91/101] Loss: 0.5606\n",
      "Epoch [10], Iter [91/101] Loss: 0.5819\n",
      "Epoch [11], Iter [91/101] Loss: 1.1278\n",
      "Epoch [12], Iter [91/101] Loss: 0.5295\n",
      "Epoch [13], Iter [91/101] Loss: 0.5449\n",
      "Epoch [14], Iter [91/101] Loss: 0.5229\n",
      "Epoch [15], Iter [91/101] Loss: 0.4838\n",
      "Epoch [16], Iter [91/101] Loss: 0.4885\n",
      "Test MSE: 0.6342342495918274\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 9\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6843\n",
      "Epoch [2], Iter [91/101] Loss: 0.6005\n",
      "Epoch [3], Iter [91/101] Loss: 0.6382\n",
      "Epoch [4], Iter [91/101] Loss: 0.5813\n",
      "Epoch [5], Iter [91/101] Loss: 0.6095\n",
      "Epoch [6], Iter [91/101] Loss: 0.5810\n",
      "Epoch [7], Iter [91/101] Loss: 0.6165\n",
      "Epoch [8], Iter [91/101] Loss: 0.6157\n",
      "Epoch [9], Iter [91/101] Loss: 0.5486\n",
      "Epoch [10], Iter [91/101] Loss: 0.5632\n",
      "Epoch [11], Iter [91/101] Loss: 0.5425\n",
      "Epoch [12], Iter [91/101] Loss: 0.5351\n",
      "Epoch [13], Iter [91/101] Loss: 0.5628\n",
      "Epoch [14], Iter [91/101] Loss: 0.5167\n",
      "Epoch [15], Iter [91/101] Loss: 0.5060\n",
      "Epoch [16], Iter [91/101] Loss: 0.4898\n",
      "Epoch [17], Iter [91/101] Loss: 0.4938\n",
      "Epoch [18], Iter [91/101] Loss: 0.4754\n",
      "Test MSE: 0.6308499574661255\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6501\n",
      "Epoch [2], Iter [91/101] Loss: 0.7596\n",
      "Epoch [3], Iter [91/101] Loss: 0.6040\n",
      "Epoch [4], Iter [91/101] Loss: 0.6098\n",
      "Epoch [5], Iter [91/101] Loss: 0.5476\n",
      "Epoch [6], Iter [91/101] Loss: 0.5696\n",
      "Epoch [7], Iter [91/101] Loss: 0.5845\n",
      "Epoch [8], Iter [91/101] Loss: 0.5384\n",
      "Epoch [9], Iter [91/101] Loss: 0.5611\n",
      "Epoch [10], Iter [91/101] Loss: 0.5373\n",
      "Epoch [11], Iter [91/101] Loss: 0.5284\n",
      "Epoch [12], Iter [91/101] Loss: 0.5316\n",
      "Epoch [13], Iter [91/101] Loss: 0.5389\n",
      "Epoch [14], Iter [91/101] Loss: 0.5267\n",
      "Epoch [15], Iter [91/101] Loss: 0.5357\n",
      "Epoch [16], Iter [91/101] Loss: 0.5059\n",
      "Epoch [17], Iter [91/101] Loss: 0.5147\n",
      "Epoch [18], Iter [91/101] Loss: 0.4930\n",
      "Epoch [19], Iter [91/101] Loss: 0.4886\n",
      "Epoch [20], Iter [91/101] Loss: 0.4760\n",
      "Epoch [21], Iter [91/101] Loss: 0.5040\n",
      "Epoch [22], Iter [91/101] Loss: 0.4689\n",
      "Epoch [23], Iter [91/101] Loss: 0.4645\n",
      "Epoch [24], Iter [91/101] Loss: 0.4736\n",
      "Test MSE: 0.6250794529914856\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6428\n",
      "Epoch [2], Iter [91/101] Loss: 0.6542\n",
      "Epoch [3], Iter [91/101] Loss: 0.6702\n",
      "Epoch [4], Iter [91/101] Loss: 0.6232\n",
      "Epoch [5], Iter [91/101] Loss: 0.6006\n",
      "Epoch [6], Iter [91/101] Loss: 0.5651\n",
      "Epoch [7], Iter [91/101] Loss: 0.5795\n",
      "Epoch [8], Iter [91/101] Loss: 0.5622\n",
      "Epoch [9], Iter [91/101] Loss: 0.5497\n",
      "Epoch [10], Iter [91/101] Loss: 0.5670\n",
      "Epoch [11], Iter [91/101] Loss: 0.5377\n",
      "Epoch [12], Iter [91/101] Loss: 0.6110\n",
      "Epoch [13], Iter [91/101] Loss: 0.5237\n",
      "Epoch [14], Iter [91/101] Loss: 0.5218\n",
      "Epoch [15], Iter [91/101] Loss: 0.5230\n",
      "Epoch [16], Iter [91/101] Loss: 0.5407\n",
      "Epoch [17], Iter [91/101] Loss: 0.5038\n",
      "Epoch [18], Iter [91/101] Loss: 0.4998\n",
      "Epoch [19], Iter [91/101] Loss: 0.5586\n",
      "Epoch [20], Iter [91/101] Loss: 0.4900\n",
      "Test MSE: 0.6169754862785339\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6558\n",
      "Epoch [2], Iter [91/101] Loss: 0.6557\n",
      "Epoch [3], Iter [91/101] Loss: 0.6150\n",
      "Epoch [4], Iter [91/101] Loss: 0.5885\n",
      "Epoch [5], Iter [91/101] Loss: 0.5768\n",
      "Epoch [6], Iter [91/101] Loss: 0.6802\n",
      "Epoch [7], Iter [91/101] Loss: 0.5780\n",
      "Epoch [8], Iter [91/101] Loss: 0.5968\n",
      "Epoch [9], Iter [91/101] Loss: 0.5445\n",
      "Epoch [10], Iter [91/101] Loss: 0.5633\n",
      "Epoch [11], Iter [91/101] Loss: 0.6362\n",
      "Epoch [12], Iter [91/101] Loss: 0.5404\n",
      "Epoch [13], Iter [91/101] Loss: 0.5451\n",
      "Epoch [14], Iter [91/101] Loss: 0.5326\n",
      "Epoch [15], Iter [91/101] Loss: 0.5513\n",
      "Epoch [16], Iter [91/101] Loss: 0.5012\n",
      "Epoch [17], Iter [91/101] Loss: 0.5295\n",
      "Epoch [18], Iter [91/101] Loss: 0.4727\n",
      "Epoch [19], Iter [91/101] Loss: 0.4674\n",
      "Epoch [20], Iter [91/101] Loss: 0.4781\n",
      "Epoch [21], Iter [91/101] Loss: 0.4795\n",
      "Epoch [22], Iter [91/101] Loss: 0.4931\n",
      "Test MSE: 0.6206631660461426\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6487\n",
      "Epoch [2], Iter [91/101] Loss: 0.6147\n",
      "Epoch [3], Iter [91/101] Loss: 0.6272\n",
      "Epoch [4], Iter [91/101] Loss: 0.5859\n",
      "Epoch [5], Iter [91/101] Loss: 0.5895\n",
      "Epoch [6], Iter [91/101] Loss: 0.5826\n",
      "Epoch [7], Iter [91/101] Loss: 0.7577\n",
      "Epoch [8], Iter [91/101] Loss: 0.6134\n",
      "Epoch [9], Iter [91/101] Loss: 0.5371\n",
      "Epoch [10], Iter [91/101] Loss: 0.5426\n",
      "Epoch [11], Iter [91/101] Loss: 0.5568\n",
      "Epoch [12], Iter [91/101] Loss: 0.5287\n",
      "Epoch [13], Iter [91/101] Loss: 0.5373\n",
      "Epoch [14], Iter [91/101] Loss: 0.5307\n",
      "Epoch [15], Iter [91/101] Loss: 0.5205\n",
      "Epoch [16], Iter [91/101] Loss: 0.4882\n",
      "Epoch [17], Iter [91/101] Loss: 0.5019\n",
      "Epoch [18], Iter [91/101] Loss: 0.4975\n",
      "Epoch [19], Iter [91/101] Loss: 0.5026\n",
      "Epoch [20], Iter [91/101] Loss: 0.4823\n",
      "Test MSE: 0.613523006439209\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 4\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6606\n",
      "Epoch [2], Iter [91/101] Loss: 1.3797\n",
      "Epoch [3], Iter [91/101] Loss: 0.6114\n",
      "Epoch [4], Iter [91/101] Loss: 0.5974\n",
      "Epoch [5], Iter [91/101] Loss: 0.5834\n",
      "Epoch [6], Iter [91/101] Loss: 0.5547\n",
      "Epoch [7], Iter [91/101] Loss: 0.5711\n",
      "Epoch [8], Iter [91/101] Loss: 0.5414\n",
      "Epoch [9], Iter [91/101] Loss: 0.5615\n",
      "Epoch [10], Iter [91/101] Loss: 0.5650\n",
      "Epoch [11], Iter [91/101] Loss: 0.5884\n",
      "Epoch [12], Iter [91/101] Loss: 0.5399\n",
      "Epoch [13], Iter [91/101] Loss: 0.5704\n",
      "Epoch [14], Iter [91/101] Loss: 0.5307\n",
      "Epoch [15], Iter [91/101] Loss: 0.5098\n",
      "Epoch [16], Iter [91/101] Loss: 0.5159\n",
      "Epoch [17], Iter [91/101] Loss: 0.5382\n",
      "Test MSE: 0.6229087114334106\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 5\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6626\n",
      "Epoch [2], Iter [91/101] Loss: 0.6351\n",
      "Epoch [3], Iter [91/101] Loss: 0.6002\n",
      "Epoch [4], Iter [91/101] Loss: 0.5897\n",
      "Epoch [5], Iter [91/101] Loss: 0.5629\n",
      "Epoch [6], Iter [91/101] Loss: 0.5532\n",
      "Epoch [7], Iter [91/101] Loss: 0.5790\n",
      "Epoch [8], Iter [91/101] Loss: 0.5549\n",
      "Epoch [9], Iter [91/101] Loss: 0.5328\n",
      "Epoch [10], Iter [91/101] Loss: 0.5509\n",
      "Epoch [11], Iter [91/101] Loss: 0.5396\n",
      "Epoch [12], Iter [91/101] Loss: 0.5259\n",
      "Epoch [13], Iter [91/101] Loss: 0.5561\n",
      "Epoch [14], Iter [91/101] Loss: 0.5603\n",
      "Epoch [15], Iter [91/101] Loss: 0.5180\n",
      "Epoch [16], Iter [91/101] Loss: 0.4901\n",
      "Epoch [17], Iter [91/101] Loss: 0.5029\n",
      "Epoch [18], Iter [91/101] Loss: 0.5039\n",
      "Epoch [19], Iter [91/101] Loss: 0.4964\n",
      "Epoch [20], Iter [91/101] Loss: 0.4885\n",
      "Epoch [21], Iter [91/101] Loss: 0.4619\n",
      "Epoch [22], Iter [91/101] Loss: 0.4697\n",
      "Epoch [23], Iter [91/101] Loss: 0.4757\n",
      "Test MSE: 0.6303257346153259\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 6\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 1.3722\n",
      "Epoch [2], Iter [91/101] Loss: 1.3638\n",
      "Epoch [3], Iter [91/101] Loss: 0.6402\n",
      "Epoch [4], Iter [91/101] Loss: 0.5734\n",
      "Epoch [5], Iter [91/101] Loss: 0.5927\n",
      "Epoch [6], Iter [91/101] Loss: 0.6008\n",
      "Epoch [7], Iter [91/101] Loss: 0.5586\n",
      "Epoch [8], Iter [91/101] Loss: 0.5718\n",
      "Epoch [9], Iter [91/101] Loss: 0.5745\n",
      "Epoch [10], Iter [91/101] Loss: 0.5341\n",
      "Epoch [11], Iter [91/101] Loss: 0.5370\n",
      "Epoch [12], Iter [91/101] Loss: 0.5515\n",
      "Epoch [13], Iter [91/101] Loss: 0.5231\n",
      "Epoch [14], Iter [91/101] Loss: 0.5159\n",
      "Epoch [15], Iter [91/101] Loss: 0.5236\n",
      "Test MSE: 0.6359338760375977\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 7\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6556\n",
      "Epoch [2], Iter [91/101] Loss: 0.6467\n",
      "Epoch [3], Iter [91/101] Loss: 0.5910\n",
      "Epoch [4], Iter [91/101] Loss: 0.6000\n",
      "Epoch [5], Iter [91/101] Loss: 0.6140\n",
      "Epoch [6], Iter [91/101] Loss: 0.6067\n",
      "Epoch [7], Iter [91/101] Loss: 0.5928\n",
      "Epoch [8], Iter [91/101] Loss: 0.5528\n",
      "Epoch [9], Iter [91/101] Loss: 0.5769\n",
      "Epoch [10], Iter [91/101] Loss: 0.5467\n",
      "Epoch [11], Iter [91/101] Loss: 0.5524\n",
      "Epoch [12], Iter [91/101] Loss: 0.5238\n",
      "Epoch [13], Iter [91/101] Loss: 0.6005\n",
      "Epoch [14], Iter [91/101] Loss: 0.5089\n",
      "Epoch [15], Iter [91/101] Loss: 0.4969\n",
      "Epoch [16], Iter [91/101] Loss: 0.5272\n",
      "Epoch [17], Iter [91/101] Loss: 0.4955\n",
      "Epoch [18], Iter [91/101] Loss: 0.5115\n",
      "Test MSE: 0.63280189037323\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 8\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6471\n",
      "Epoch [2], Iter [91/101] Loss: 0.6026\n",
      "Epoch [3], Iter [91/101] Loss: 0.5907\n",
      "Epoch [4], Iter [91/101] Loss: 0.6176\n",
      "Epoch [5], Iter [91/101] Loss: 0.6227\n",
      "Epoch [6], Iter [91/101] Loss: 0.5999\n",
      "Epoch [7], Iter [91/101] Loss: 0.5600\n",
      "Epoch [8], Iter [91/101] Loss: 0.5550\n",
      "Epoch [9], Iter [91/101] Loss: 0.6841\n",
      "Epoch [10], Iter [91/101] Loss: 0.5866\n",
      "Epoch [11], Iter [91/101] Loss: 0.5574\n",
      "Epoch [12], Iter [91/101] Loss: 0.5294\n",
      "Epoch [13], Iter [91/101] Loss: 0.5496\n",
      "Epoch [14], Iter [91/101] Loss: 0.5504\n",
      "Epoch [15], Iter [91/101] Loss: 0.5225\n",
      "Epoch [16], Iter [91/101] Loss: 0.5180\n",
      "Epoch [17], Iter [91/101] Loss: 0.5121\n",
      "Test MSE: 0.6184586882591248\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 9\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6333\n",
      "Epoch [2], Iter [91/101] Loss: 0.6570\n",
      "Epoch [3], Iter [91/101] Loss: 0.7153\n",
      "Epoch [4], Iter [91/101] Loss: 0.6129\n",
      "Epoch [5], Iter [91/101] Loss: 0.6038\n",
      "Epoch [6], Iter [91/101] Loss: 0.5949\n",
      "Epoch [7], Iter [91/101] Loss: 0.5915\n",
      "Epoch [8], Iter [91/101] Loss: 0.5353\n",
      "Epoch [9], Iter [91/101] Loss: 0.5737\n",
      "Epoch [10], Iter [91/101] Loss: 0.5468\n",
      "Epoch [11], Iter [91/101] Loss: 0.5285\n",
      "Epoch [12], Iter [91/101] Loss: 0.5327\n",
      "Epoch [13], Iter [91/101] Loss: 0.5504\n",
      "Epoch [14], Iter [91/101] Loss: 0.5160\n",
      "Epoch [15], Iter [91/101] Loss: 0.5185\n",
      "Epoch [16], Iter [91/101] Loss: 0.5073\n",
      "Epoch [17], Iter [91/101] Loss: 0.4787\n",
      "Epoch [18], Iter [91/101] Loss: 0.5005\n",
      "Test MSE: 0.610654354095459\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7861\n",
      "Epoch [2], Iter [91/101] Loss: 0.6537\n",
      "Epoch [3], Iter [91/101] Loss: 0.6311\n",
      "Epoch [4], Iter [91/101] Loss: 0.5607\n",
      "Epoch [5], Iter [91/101] Loss: 0.5836\n",
      "Epoch [6], Iter [91/101] Loss: 0.5463\n",
      "Epoch [7], Iter [91/101] Loss: 0.5365\n",
      "Epoch [8], Iter [91/101] Loss: 0.5616\n",
      "Epoch [9], Iter [91/101] Loss: 0.5301\n",
      "Epoch [10], Iter [91/101] Loss: 0.5052\n",
      "Epoch [11], Iter [91/101] Loss: 0.4858\n",
      "Epoch [12], Iter [91/101] Loss: 0.4924\n",
      "Epoch [13], Iter [91/101] Loss: 0.4962\n",
      "Epoch [14], Iter [91/101] Loss: 0.5586\n",
      "Epoch [15], Iter [91/101] Loss: 0.4513\n",
      "Epoch [16], Iter [91/101] Loss: 0.4458\n",
      "Test MSE: 0.6299080848693848\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6369\n",
      "Epoch [2], Iter [91/101] Loss: 0.6192\n",
      "Epoch [3], Iter [91/101] Loss: 0.6260\n",
      "Epoch [4], Iter [91/101] Loss: 0.6183\n",
      "Epoch [5], Iter [91/101] Loss: 0.5571\n",
      "Epoch [6], Iter [91/101] Loss: 0.5235\n",
      "Epoch [7], Iter [91/101] Loss: 0.5276\n",
      "Epoch [8], Iter [91/101] Loss: 0.5292\n",
      "Epoch [9], Iter [91/101] Loss: 0.5157\n",
      "Epoch [10], Iter [91/101] Loss: 0.4929\n",
      "Epoch [11], Iter [91/101] Loss: 0.4988\n",
      "Epoch [12], Iter [91/101] Loss: 0.4807\n",
      "Epoch [13], Iter [91/101] Loss: 0.4544\n",
      "Epoch [14], Iter [91/101] Loss: 0.4614\n",
      "Test MSE: 0.6319593787193298\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6568\n",
      "Epoch [2], Iter [91/101] Loss: 0.6310\n",
      "Epoch [3], Iter [91/101] Loss: 0.5978\n",
      "Epoch [4], Iter [91/101] Loss: 0.5835\n",
      "Epoch [5], Iter [91/101] Loss: 0.5589\n",
      "Epoch [6], Iter [91/101] Loss: 0.5428\n",
      "Epoch [7], Iter [91/101] Loss: 0.6020\n",
      "Epoch [8], Iter [91/101] Loss: 0.5109\n",
      "Epoch [9], Iter [91/101] Loss: 0.5234\n",
      "Epoch [10], Iter [91/101] Loss: 0.5147\n",
      "Epoch [11], Iter [91/101] Loss: 0.4963\n",
      "Epoch [12], Iter [91/101] Loss: 0.5315\n",
      "Epoch [13], Iter [91/101] Loss: 0.4788\n",
      "Epoch [14], Iter [91/101] Loss: 0.4966\n",
      "Test MSE: 0.6387014389038086\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6878\n",
      "Epoch [2], Iter [91/101] Loss: 0.6126\n",
      "Epoch [3], Iter [91/101] Loss: 0.6077\n",
      "Epoch [4], Iter [91/101] Loss: 0.5729\n",
      "Epoch [5], Iter [91/101] Loss: 0.6967\n",
      "Epoch [6], Iter [91/101] Loss: 0.5774\n",
      "Epoch [7], Iter [91/101] Loss: 0.5749\n",
      "Epoch [8], Iter [91/101] Loss: 0.5220\n",
      "Epoch [9], Iter [91/101] Loss: 0.5164\n",
      "Epoch [10], Iter [91/101] Loss: 0.5357\n",
      "Epoch [11], Iter [91/101] Loss: 0.4897\n",
      "Epoch [12], Iter [91/101] Loss: 0.5000\n",
      "Epoch [13], Iter [91/101] Loss: 0.4986\n",
      "Epoch [14], Iter [91/101] Loss: 0.4708\n",
      "Test MSE: 0.6353611946105957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 4\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6750\n",
      "Epoch [2], Iter [91/101] Loss: 0.6431\n",
      "Epoch [3], Iter [91/101] Loss: 0.5718\n",
      "Epoch [4], Iter [91/101] Loss: 0.6269\n",
      "Epoch [5], Iter [91/101] Loss: 0.5639\n",
      "Epoch [6], Iter [91/101] Loss: 0.5723\n",
      "Epoch [7], Iter [91/101] Loss: 0.5504\n",
      "Epoch [8], Iter [91/101] Loss: 0.5304\n",
      "Epoch [9], Iter [91/101] Loss: 0.5086\n",
      "Epoch [10], Iter [91/101] Loss: 0.5082\n",
      "Epoch [11], Iter [91/101] Loss: 0.4874\n",
      "Epoch [12], Iter [91/101] Loss: 0.4769\n",
      "Epoch [13], Iter [91/101] Loss: 0.4710\n",
      "Epoch [14], Iter [91/101] Loss: 0.4646\n",
      "Test MSE: 0.6432587504386902\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 5\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6413\n",
      "Epoch [2], Iter [91/101] Loss: 0.6773\n",
      "Epoch [3], Iter [91/101] Loss: 0.6228\n",
      "Epoch [4], Iter [91/101] Loss: 0.5553\n",
      "Epoch [5], Iter [91/101] Loss: 0.5787\n",
      "Epoch [6], Iter [91/101] Loss: 0.5298\n",
      "Epoch [7], Iter [91/101] Loss: 0.5532\n",
      "Epoch [8], Iter [91/101] Loss: 0.5323\n",
      "Epoch [9], Iter [91/101] Loss: 0.6269\n",
      "Epoch [10], Iter [91/101] Loss: 0.5128\n",
      "Epoch [11], Iter [91/101] Loss: 0.5135\n",
      "Epoch [12], Iter [91/101] Loss: 0.5022\n",
      "Epoch [13], Iter [91/101] Loss: 0.4723\n",
      "Epoch [14], Iter [91/101] Loss: 0.4740\n",
      "Epoch [15], Iter [91/101] Loss: 0.4707\n",
      "Epoch [16], Iter [91/101] Loss: 0.4511\n",
      "Test MSE: 0.6378099322319031\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 6\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6630\n",
      "Epoch [2], Iter [91/101] Loss: 0.6821\n",
      "Epoch [3], Iter [91/101] Loss: 0.6039\n",
      "Epoch [4], Iter [91/101] Loss: 0.6206\n",
      "Epoch [5], Iter [91/101] Loss: 0.5750\n",
      "Epoch [6], Iter [91/101] Loss: 0.5766\n",
      "Epoch [7], Iter [91/101] Loss: 0.5687\n",
      "Epoch [8], Iter [91/101] Loss: 0.5436\n",
      "Epoch [9], Iter [91/101] Loss: 0.5230\n",
      "Epoch [10], Iter [91/101] Loss: 0.5467\n",
      "Epoch [11], Iter [91/101] Loss: 0.4899\n",
      "Epoch [12], Iter [91/101] Loss: 0.5134\n",
      "Epoch [13], Iter [91/101] Loss: 0.4872\n",
      "Test MSE: 0.6402871012687683\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 7\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6461\n",
      "Epoch [2], Iter [91/101] Loss: 0.6166\n",
      "Epoch [3], Iter [91/101] Loss: 0.6096\n",
      "Epoch [4], Iter [91/101] Loss: 0.5928\n",
      "Epoch [5], Iter [91/101] Loss: 0.5604\n",
      "Epoch [6], Iter [91/101] Loss: 0.5467\n",
      "Epoch [7], Iter [91/101] Loss: 0.5448\n",
      "Epoch [8], Iter [91/101] Loss: 0.5026\n",
      "Epoch [9], Iter [91/101] Loss: 0.5158\n",
      "Epoch [10], Iter [91/101] Loss: 0.5166\n",
      "Epoch [11], Iter [91/101] Loss: 0.5463\n",
      "Epoch [12], Iter [91/101] Loss: 0.5254\n",
      "Epoch [13], Iter [91/101] Loss: 0.4605\n",
      "Epoch [14], Iter [91/101] Loss: 0.4812\n",
      "Test MSE: 0.6411304473876953\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 8\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7907\n",
      "Epoch [2], Iter [91/101] Loss: 0.6046\n",
      "Epoch [3], Iter [91/101] Loss: 0.6207\n",
      "Epoch [4], Iter [91/101] Loss: 0.5961\n",
      "Epoch [5], Iter [91/101] Loss: 0.5701\n",
      "Epoch [6], Iter [91/101] Loss: 0.6804\n",
      "Epoch [7], Iter [91/101] Loss: 0.5327\n",
      "Epoch [8], Iter [91/101] Loss: 0.5560\n",
      "Epoch [9], Iter [91/101] Loss: 0.5225\n",
      "Epoch [10], Iter [91/101] Loss: 0.4991\n",
      "Epoch [11], Iter [91/101] Loss: 0.5202\n",
      "Epoch [12], Iter [91/101] Loss: 0.4859\n",
      "Epoch [13], Iter [91/101] Loss: 0.4831\n",
      "Epoch [14], Iter [91/101] Loss: 0.4867\n",
      "Epoch [15], Iter [91/101] Loss: 0.4815\n",
      "Epoch [16], Iter [91/101] Loss: 0.4605\n",
      "Test MSE: 0.6282810568809509\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 9\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6578\n",
      "Epoch [2], Iter [91/101] Loss: 0.6321\n",
      "Epoch [3], Iter [91/101] Loss: 0.6097\n",
      "Epoch [4], Iter [91/101] Loss: 0.5895\n",
      "Epoch [5], Iter [91/101] Loss: 0.5882\n",
      "Epoch [6], Iter [91/101] Loss: 0.5420\n",
      "Epoch [7], Iter [91/101] Loss: 0.5464\n",
      "Epoch [8], Iter [91/101] Loss: 0.5171\n",
      "Epoch [9], Iter [91/101] Loss: 0.5205\n",
      "Epoch [10], Iter [91/101] Loss: 0.5317\n",
      "Epoch [11], Iter [91/101] Loss: 0.5564\n",
      "Epoch [12], Iter [91/101] Loss: 0.4981\n",
      "Epoch [13], Iter [91/101] Loss: 0.4921\n",
      "Epoch [14], Iter [91/101] Loss: 0.4529\n",
      "Epoch [15], Iter [91/101] Loss: 0.5146\n",
      "Epoch [16], Iter [91/101] Loss: 0.4744\n",
      "Test MSE: 0.6375225782394409\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False True 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6644\n",
      "Epoch [2], Iter [91/101] Loss: 0.8008\n",
      "Epoch [3], Iter [91/101] Loss: 0.6034\n",
      "Epoch [4], Iter [91/101] Loss: 0.7071\n",
      "Epoch [5], Iter [91/101] Loss: 0.5804\n",
      "Epoch [6], Iter [91/101] Loss: 0.5681\n",
      "Epoch [7], Iter [91/101] Loss: 0.5602\n",
      "Epoch [8], Iter [91/101] Loss: 0.5170\n",
      "Epoch [9], Iter [91/101] Loss: 0.4976\n",
      "Epoch [10], Iter [91/101] Loss: 0.5123\n",
      "Epoch [11], Iter [91/101] Loss: 0.4920\n",
      "Epoch [12], Iter [91/101] Loss: 0.5043\n",
      "Epoch [13], Iter [91/101] Loss: 0.4888\n",
      "Epoch [14], Iter [91/101] Loss: 0.4629\n",
      "Epoch [15], Iter [91/101] Loss: 0.4788\n",
      "Epoch [16], Iter [91/101] Loss: 0.4808\n",
      "Test MSE: 0.6350587010383606\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False True 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6837\n",
      "Epoch [2], Iter [91/101] Loss: 0.6172\n",
      "Epoch [3], Iter [91/101] Loss: 0.6102\n",
      "Epoch [4], Iter [91/101] Loss: 0.5592\n",
      "Epoch [5], Iter [91/101] Loss: 0.5702\n",
      "Epoch [6], Iter [91/101] Loss: 0.5544\n",
      "Epoch [7], Iter [91/101] Loss: 0.5447\n",
      "Epoch [8], Iter [91/101] Loss: 0.5515\n",
      "Epoch [9], Iter [91/101] Loss: 0.5263\n",
      "Epoch [10], Iter [91/101] Loss: 0.5163\n",
      "Epoch [11], Iter [91/101] Loss: 0.4996\n",
      "Epoch [12], Iter [91/101] Loss: 0.5083\n",
      "Epoch [13], Iter [91/101] Loss: 0.4772\n",
      "Epoch [14], Iter [91/101] Loss: 0.4744\n",
      "Epoch [15], Iter [91/101] Loss: 0.4796\n",
      "Epoch [16], Iter [91/101] Loss: 0.4878\n",
      "Epoch [17], Iter [91/101] Loss: 0.4590\n",
      "Epoch [18], Iter [91/101] Loss: 0.4397\n",
      "Epoch [19], Iter [91/101] Loss: 0.4304\n",
      "Test MSE: 0.6381804347038269\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False True 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6990\n",
      "Epoch [2], Iter [91/101] Loss: 0.8039\n",
      "Epoch [3], Iter [91/101] Loss: 0.6128\n",
      "Epoch [4], Iter [91/101] Loss: 0.5831\n",
      "Epoch [5], Iter [91/101] Loss: 0.5664\n",
      "Epoch [6], Iter [91/101] Loss: 0.6235\n",
      "Epoch [7], Iter [91/101] Loss: 0.6452\n",
      "Epoch [8], Iter [91/101] Loss: 0.5559\n",
      "Epoch [9], Iter [91/101] Loss: 0.5290\n",
      "Epoch [10], Iter [91/101] Loss: 0.4909\n",
      "Epoch [11], Iter [91/101] Loss: 0.5061\n",
      "Epoch [12], Iter [91/101] Loss: 0.4814\n",
      "Epoch [13], Iter [91/101] Loss: 0.4795\n",
      "Test MSE: 0.6414908170700073\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False True 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6554\n",
      "Epoch [2], Iter [91/101] Loss: 0.6260\n",
      "Epoch [3], Iter [91/101] Loss: 0.5855\n",
      "Epoch [4], Iter [91/101] Loss: 0.8028\n",
      "Epoch [5], Iter [91/101] Loss: 0.5378\n",
      "Epoch [6], Iter [91/101] Loss: 0.5593\n",
      "Epoch [7], Iter [91/101] Loss: 0.5483\n",
      "Epoch [8], Iter [91/101] Loss: 0.5284\n",
      "Epoch [9], Iter [91/101] Loss: 0.5172\n",
      "Epoch [10], Iter [91/101] Loss: 0.5133\n",
      "Epoch [11], Iter [1/101] Loss: 0.4933"
     ]
    }
   ],
   "source": [
    "for l in loss:\n",
    "    for c_conv in use_coord_conv:\n",
    "        for c_pad in use_cylindrical_padding:\n",
    "            for i in range(n_runs):\n",
    "                print(l, c_conv, c_pad, i)\n",
    "                model_training_description[\"USE_CYLINDRICAL_PADDING\"] = c_pad\n",
    "                model_training_description[\"USE_COORD_CONV\"] = c_conv\n",
    "                model_training_description[\"LOSS\"] = l  # \"MSELoss\" # \"AreaWeightedMSELoss\"\n",
    "                model_training_description[\"RUN_NR\"] = i\n",
    "                unet = train_unet(description, model_training_description, output_folder)\n",
    "                predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Comparing results for different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to implement the interpolation before we can reproduce the whole table. Until then: Only compare ico architectures on ico grid and flat architectures on flat grid.\n",
    "\n",
    "Results for modified and unmodified flat UNet are already obtained in last cell.(4.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "model_training_description[\"N_PC_PREDICTORS\"] = 450\n",
    "model_training_description[\"N_PC_TARGETS\"] = 300\n",
    "model_training_description[\"REGTYPE\"] = \"lasso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pca, pca_targets, model = train_pca(description, model_training_description, output_folder)\n",
    "print(\"finished training\")\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New linreg baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"LinReg_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "models = train_linreg_pixelwise(description, model_training_description, output_folder)\n",
    "predict_save_linreg_pixelwise(description, model_training_description, output_folder, models, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New PCA-reg baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning, compute 50x50 logarithmic grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 50)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291/2500], N_PC_IN=257, N_PC_OUT=5, R2=[0.058904034061053843]. Best: [0.05892903299744694]]\r"
     ]
    }
   ],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Random-forest baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"RandomForest_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  3.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 21.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed: 24.9min finished\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "Specified configuration of data set, model and training configuration already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11864/3484084733.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_random_forest_pixelwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict_save_randomforest_pixelwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\predict.py\u001b[0m in \u001b[0;36mpredict_save_randomforest_pixelwise\u001b[1;34m(dataset_description, model_training_description, base_folder, model, output_folder, save_model)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mdescriptions_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"descriptions.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_if_folder_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Specified configuration of data set, model and training configuration already exists.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: Specified configuration of data set, model and training configuration already exists."
     ]
    }
   ],
   "source": [
    "model = train_random_forest_pixelwise(description, model_training_description, output_folder, verbose=3, n_jobs=-1)\n",
    "predict_save_randomforest_pixelwise(description, model_training_description, output_folder, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Icosahedral grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ico baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 20)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/400], N_PC_IN=39, N_PC_OUT=3, R2=[0.04993109]. Best: [0.05033371]]\r"
     ]
    }
   ],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ico UNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = IcoBatchNorm2d # torch.nn.BatchNorm2d\n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "model_training_description[\"LOSS\"] = \"MSELoss\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 COMPARING RESULTS FOR DIFFERENT PREDICTOR COMBINATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tas only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8467\n",
      "Epoch [2], Iter [91/101] Loss: 0.7728\n",
      "Epoch [3], Iter [91/101] Loss: 0.7915\n",
      "Epoch [4], Iter [91/101] Loss: 0.6983\n",
      "Epoch [5], Iter [91/101] Loss: 0.6690\n",
      "Epoch [6], Iter [91/101] Loss: 0.6719\n",
      "Epoch [7], Iter [91/101] Loss: 0.7733\n",
      "Epoch [8], Iter [91/101] Loss: 0.7234\n",
      "Epoch [9], Iter [91/101] Loss: 0.6453\n",
      "Epoch [10], Iter [91/101] Loss: 0.6396\n",
      "Epoch [11], Iter [91/101] Loss: 0.6110\n",
      "Epoch [12], Iter [91/101] Loss: 0.6494\n",
      "Epoch [13], Iter [91/101] Loss: 0.6388\n",
      "Epoch [14], Iter [91/101] Loss: 0.6039\n",
      "Epoch [15], Iter [91/101] Loss: 0.6104\n",
      "Epoch [16], Iter [91/101] Loss: 0.6292\n",
      "Epoch [17], Iter [91/101] Loss: 0.6842\n",
      "Epoch [18], Iter [91/101] Loss: 0.5940\n",
      "Epoch [19], Iter [91/101] Loss: 0.5690\n",
      "Test MSE: 0.760962724685669\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8063\n",
      "Epoch [2], Iter [91/101] Loss: 0.8362\n",
      "Epoch [3], Iter [91/101] Loss: 0.8331\n",
      "Epoch [4], Iter [91/101] Loss: 0.7368\n",
      "Epoch [5], Iter [91/101] Loss: 0.7725\n",
      "Epoch [6], Iter [91/101] Loss: 0.7582\n",
      "Epoch [7], Iter [91/101] Loss: 0.6800\n",
      "Epoch [8], Iter [91/101] Loss: 0.6937\n",
      "Epoch [9], Iter [91/101] Loss: 0.7475\n",
      "Epoch [10], Iter [91/101] Loss: 0.6639\n",
      "Epoch [11], Iter [91/101] Loss: 0.6387\n",
      "Epoch [12], Iter [91/101] Loss: 0.6267\n",
      "Epoch [13], Iter [91/101] Loss: 0.6317\n",
      "Epoch [14], Iter [91/101] Loss: 0.6620\n",
      "Epoch [15], Iter [91/101] Loss: 0.6099\n",
      "Epoch [16], Iter [91/101] Loss: 0.6022\n",
      "Test MSE: 0.7668517231941223\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9429\n",
      "Epoch [2], Iter [91/101] Loss: 0.7838\n",
      "Epoch [3], Iter [91/101] Loss: 0.7556\n",
      "Epoch [4], Iter [91/101] Loss: 0.6992\n",
      "Epoch [5], Iter [91/101] Loss: 0.8620\n",
      "Epoch [6], Iter [91/101] Loss: 0.6799\n",
      "Epoch [7], Iter [91/101] Loss: 0.6994\n",
      "Epoch [8], Iter [91/101] Loss: 0.7458\n",
      "Epoch [9], Iter [91/101] Loss: 0.6751\n",
      "Epoch [10], Iter [91/101] Loss: 0.7066\n",
      "Epoch [11], Iter [91/101] Loss: 0.6437\n",
      "Epoch [12], Iter [91/101] Loss: 0.6860\n",
      "Epoch [13], Iter [91/101] Loss: 0.6546\n",
      "Epoch [14], Iter [91/101] Loss: 0.6356\n",
      "Epoch [15], Iter [91/101] Loss: 0.6350\n",
      "Epoch [16], Iter [91/101] Loss: 0.6064\n",
      "Epoch [17], Iter [91/101] Loss: 0.5982\n",
      "Epoch [18], Iter [91/101] Loss: 0.5956\n",
      "Test MSE: 0.7740318775177002\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8204\n",
      "Epoch [2], Iter [91/101] Loss: 0.8062\n",
      "Epoch [3], Iter [91/101] Loss: 0.7434\n",
      "Epoch [4], Iter [91/101] Loss: 0.7304\n",
      "Epoch [5], Iter [91/101] Loss: 0.7201\n",
      "Epoch [6], Iter [91/101] Loss: 0.8026\n",
      "Epoch [7], Iter [91/101] Loss: 0.6902\n",
      "Epoch [8], Iter [91/101] Loss: 0.6845\n",
      "Epoch [9], Iter [91/101] Loss: 0.6727\n",
      "Epoch [10], Iter [91/101] Loss: 0.6720\n",
      "Epoch [11], Iter [91/101] Loss: 0.6559\n",
      "Epoch [12], Iter [91/101] Loss: 0.6774\n",
      "Epoch [13], Iter [91/101] Loss: 0.6576\n",
      "Epoch [14], Iter [91/101] Loss: 0.6404\n",
      "Epoch [15], Iter [91/101] Loss: 0.6317\n",
      "Epoch [16], Iter [91/101] Loss: 0.6729\n",
      "Epoch [17], Iter [91/101] Loss: 0.6150\n",
      "Epoch [18], Iter [91/101] Loss: 0.6101\n",
      "Epoch [19], Iter [91/101] Loss: 0.5825\n",
      "Epoch [20], Iter [91/101] Loss: 0.5770\n",
      "Epoch [21], Iter [91/101] Loss: 0.5908\n",
      "Epoch [22], Iter [91/101] Loss: 0.5627\n",
      "Epoch [23], Iter [91/101] Loss: 0.5668\n",
      "Epoch [24], Iter [91/101] Loss: 0.5698\n",
      "Test MSE: 0.7659046053886414\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8045\n",
      "Epoch [2], Iter [91/101] Loss: 0.7467\n",
      "Epoch [3], Iter [91/101] Loss: 0.7576\n",
      "Epoch [4], Iter [91/101] Loss: 0.7304\n",
      "Epoch [5], Iter [91/101] Loss: 0.7294\n",
      "Epoch [6], Iter [91/101] Loss: 0.7249\n",
      "Epoch [7], Iter [91/101] Loss: 0.6883\n",
      "Epoch [8], Iter [91/101] Loss: 0.7136\n",
      "Epoch [9], Iter [91/101] Loss: 0.6928\n",
      "Epoch [10], Iter [91/101] Loss: 0.6805\n",
      "Epoch [11], Iter [91/101] Loss: 0.6398\n",
      "Epoch [12], Iter [91/101] Loss: 0.6476\n",
      "Epoch [13], Iter [91/101] Loss: 0.6390\n",
      "Epoch [14], Iter [91/101] Loss: 0.6614\n",
      "Epoch [15], Iter [91/101] Loss: 0.6400\n",
      "Epoch [16], Iter [91/101] Loss: 0.6481\n",
      "Epoch [17], Iter [91/101] Loss: 0.6193\n",
      "Epoch [18], Iter [91/101] Loss: 0.5765\n",
      "Epoch [19], Iter [91/101] Loss: 0.5933\n",
      "Test MSE: 0.7680153250694275\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8426\n",
      "Epoch [2], Iter [91/101] Loss: 0.7621\n",
      "Epoch [3], Iter [91/101] Loss: 0.7212\n",
      "Epoch [4], Iter [91/101] Loss: 0.7408\n",
      "Epoch [5], Iter [91/101] Loss: 0.7258\n",
      "Epoch [6], Iter [91/101] Loss: 0.6665\n",
      "Epoch [7], Iter [91/101] Loss: 0.7049\n",
      "Epoch [8], Iter [91/101] Loss: 0.6963\n",
      "Epoch [9], Iter [91/101] Loss: 0.6769\n",
      "Epoch [10], Iter [91/101] Loss: 0.6272\n",
      "Epoch [11], Iter [91/101] Loss: 0.7436\n",
      "Epoch [12], Iter [91/101] Loss: 0.6654\n",
      "Epoch [13], Iter [91/101] Loss: 0.6364\n",
      "Epoch [14], Iter [91/101] Loss: 0.6268\n",
      "Epoch [15], Iter [91/101] Loss: 0.6360\n",
      "Epoch [16], Iter [91/101] Loss: 0.6330\n",
      "Epoch [17], Iter [91/101] Loss: 0.6291\n",
      "Test MSE: 0.7667409181594849\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8239\n",
      "Epoch [2], Iter [91/101] Loss: 0.7872\n",
      "Epoch [3], Iter [91/101] Loss: 0.7557\n",
      "Epoch [4], Iter [91/101] Loss: 0.7227\n",
      "Epoch [5], Iter [91/101] Loss: 0.7000\n",
      "Epoch [6], Iter [91/101] Loss: 0.6812\n",
      "Epoch [7], Iter [91/101] Loss: 0.6778\n",
      "Epoch [8], Iter [91/101] Loss: 0.6541\n",
      "Epoch [9], Iter [91/101] Loss: 0.6706\n",
      "Epoch [10], Iter [91/101] Loss: 0.7055\n",
      "Epoch [11], Iter [91/101] Loss: 0.6758\n",
      "Epoch [12], Iter [91/101] Loss: 0.6387\n",
      "Epoch [13], Iter [91/101] Loss: 0.6292\n",
      "Epoch [14], Iter [91/101] Loss: 0.6471\n",
      "Epoch [15], Iter [91/101] Loss: 0.6123\n",
      "Epoch [16], Iter [91/101] Loss: 0.5997\n",
      "Epoch [17], Iter [91/101] Loss: 0.6536\n",
      "Epoch [18], Iter [91/101] Loss: 0.5908\n",
      "Test MSE: 0.7632282972335815\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7983\n",
      "Epoch [2], Iter [91/101] Loss: 0.7813\n",
      "Epoch [3], Iter [91/101] Loss: 0.7443\n",
      "Epoch [4], Iter [91/101] Loss: 0.7193\n",
      "Epoch [5], Iter [91/101] Loss: 0.6988\n",
      "Epoch [6], Iter [91/101] Loss: 0.7781\n",
      "Epoch [7], Iter [91/101] Loss: 0.6589\n",
      "Epoch [8], Iter [91/101] Loss: 0.6800\n",
      "Epoch [9], Iter [91/101] Loss: 0.6723\n",
      "Epoch [10], Iter [91/101] Loss: 0.6523\n",
      "Epoch [11], Iter [91/101] Loss: 0.6494\n",
      "Epoch [12], Iter [91/101] Loss: 0.6957\n",
      "Epoch [13], Iter [91/101] Loss: 0.6569\n",
      "Epoch [14], Iter [91/101] Loss: 0.6218\n",
      "Epoch [15], Iter [91/101] Loss: 0.6329\n",
      "Epoch [16], Iter [91/101] Loss: 0.5892\n",
      "Epoch [17], Iter [91/101] Loss: 0.6090\n",
      "Test MSE: 0.7695837020874023\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7900\n",
      "Epoch [2], Iter [91/101] Loss: 0.8943\n",
      "Epoch [3], Iter [91/101] Loss: 0.7269\n",
      "Epoch [4], Iter [91/101] Loss: 0.7490\n",
      "Epoch [5], Iter [91/101] Loss: 0.6971\n",
      "Epoch [6], Iter [91/101] Loss: 0.7045\n",
      "Epoch [7], Iter [91/101] Loss: 0.7069\n",
      "Epoch [8], Iter [91/101] Loss: 0.6707\n",
      "Epoch [9], Iter [91/101] Loss: 0.6654\n",
      "Epoch [10], Iter [91/101] Loss: 0.6703\n",
      "Epoch [11], Iter [91/101] Loss: 0.6629\n",
      "Epoch [12], Iter [91/101] Loss: 0.6441\n",
      "Epoch [13], Iter [91/101] Loss: 0.6467\n",
      "Epoch [14], Iter [91/101] Loss: 0.6390\n",
      "Epoch [15], Iter [91/101] Loss: 0.6356\n",
      "Epoch [16], Iter [91/101] Loss: 0.7152\n",
      "Epoch [17], Iter [91/101] Loss: 1.2326\n",
      "Test MSE: 0.7647383809089661\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8132\n",
      "Epoch [2], Iter [91/101] Loss: 0.7272\n",
      "Epoch [3], Iter [91/101] Loss: 0.7560\n",
      "Epoch [4], Iter [91/101] Loss: 0.7484\n",
      "Epoch [5], Iter [91/101] Loss: 0.6979\n",
      "Epoch [6], Iter [91/101] Loss: 0.6890\n",
      "Epoch [7], Iter [91/101] Loss: 0.7885\n",
      "Epoch [8], Iter [91/101] Loss: 0.6608\n",
      "Epoch [9], Iter [91/101] Loss: 0.6644\n",
      "Epoch [10], Iter [91/101] Loss: 0.6621\n",
      "Epoch [11], Iter [91/101] Loss: 0.6484\n",
      "Epoch [12], Iter [91/101] Loss: 0.6248\n",
      "Epoch [13], Iter [91/101] Loss: 0.6654\n",
      "Epoch [14], Iter [91/101] Loss: 0.6381\n",
      "Epoch [15], Iter [91/101] Loss: 0.6302\n",
      "Epoch [16], Iter [91/101] Loss: 0.6479\n",
      "Epoch [17], Iter [91/101] Loss: 0.6197\n",
      "Epoch [18], Iter [91/101] Loss: 0.6005\n",
      "Test MSE: 0.7691001892089844\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precip only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a867e1a85d9fe2098b8904069b6a715f9_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6644\n",
      "Epoch [2], Iter [91/101] Loss: 0.6568\n",
      "Epoch [3], Iter [91/101] Loss: 0.6269\n",
      "Epoch [4], Iter [91/101] Loss: 0.6259\n",
      "Epoch [5], Iter [91/101] Loss: 0.6249\n",
      "Epoch [6], Iter [91/101] Loss: 0.6121\n",
      "Epoch [7], Iter [91/101] Loss: 0.6301\n",
      "Epoch [8], Iter [91/101] Loss: 0.5872\n",
      "Epoch [9], Iter [91/101] Loss: 0.6076\n",
      "Epoch [10], Iter [91/101] Loss: 0.5840\n",
      "Epoch [11], Iter [91/101] Loss: 0.5859\n",
      "Epoch [12], Iter [91/101] Loss: 0.5838\n",
      "Epoch [13], Iter [91/101] Loss: 0.6103\n",
      "Epoch [14], Iter [91/101] Loss: 0.5568\n",
      "Epoch [15], Iter [91/101] Loss: 0.5696\n",
      "Epoch [16], Iter [91/101] Loss: 0.5763\n",
      "Epoch [17], Iter [91/101] Loss: 0.5442\n",
      "Epoch [18], Iter [91/101] Loss: 0.5481\n",
      "Epoch [19], Iter [91/101] Loss: 0.5191\n",
      "Epoch [20], Iter [91/101] Loss: 0.5228\n",
      "Test MSE: 0.6802402138710022\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a861153d84f63886ce7e529bdad699c68_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6995\n",
      "Epoch [2], Iter [91/101] Loss: 0.6675\n",
      "Epoch [3], Iter [91/101] Loss: 0.6227\n",
      "Epoch [4], Iter [91/101] Loss: 0.6810\n",
      "Epoch [5], Iter [91/101] Loss: 0.6056\n",
      "Epoch [6], Iter [91/101] Loss: 0.6473\n",
      "Epoch [7], Iter [91/101] Loss: 0.6310\n",
      "Epoch [8], Iter [91/101] Loss: 0.7024\n",
      "Epoch [9], Iter [91/101] Loss: 0.6243\n",
      "Epoch [10], Iter [91/101] Loss: 0.6067\n",
      "Epoch [11], Iter [91/101] Loss: 0.5603\n",
      "Epoch [12], Iter [91/101] Loss: 0.5629\n",
      "Epoch [13], Iter [91/101] Loss: 0.5958\n",
      "Epoch [14], Iter [91/101] Loss: 0.5439\n",
      "Epoch [15], Iter [91/101] Loss: 0.5433\n",
      "Epoch [16], Iter [91/101] Loss: 0.5562\n",
      "Test MSE: 0.6783640384674072\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a869f8dcec6d4cf91eb588869b349abcd_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7035\n",
      "Epoch [2], Iter [91/101] Loss: 0.6560\n",
      "Epoch [3], Iter [91/101] Loss: 0.6291\n",
      "Epoch [4], Iter [91/101] Loss: 0.6520\n",
      "Epoch [5], Iter [91/101] Loss: 0.6428\n",
      "Epoch [6], Iter [91/101] Loss: 0.6021\n",
      "Epoch [7], Iter [91/101] Loss: 0.6330\n",
      "Epoch [8], Iter [91/101] Loss: 0.6019\n",
      "Epoch [9], Iter [91/101] Loss: 0.6232\n",
      "Epoch [10], Iter [91/101] Loss: 0.5749\n",
      "Epoch [11], Iter [91/101] Loss: 0.5703\n",
      "Epoch [12], Iter [91/101] Loss: 0.5664\n",
      "Epoch [13], Iter [91/101] Loss: 0.5690\n",
      "Epoch [14], Iter [91/101] Loss: 0.5847\n",
      "Epoch [15], Iter [91/101] Loss: 0.5368\n",
      "Epoch [16], Iter [91/101] Loss: 0.5416\n",
      "Epoch [17], Iter [91/101] Loss: 0.5511\n",
      "Epoch [18], Iter [91/101] Loss: 0.5416\n",
      "Epoch [19], Iter [91/101] Loss: 0.5095\n",
      "Epoch [20], Iter [91/101] Loss: 0.5361\n",
      "Epoch [21], Iter [91/101] Loss: 0.5126\n",
      "Epoch [22], Iter [91/101] Loss: 0.4959\n",
      "Epoch [23], Iter [91/101] Loss: 0.4889\n",
      "Epoch [24], Iter [91/101] Loss: 0.4834\n",
      "Test MSE: 0.6803274154663086\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a861f8da943095164008ccd6eb97d4980_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7106\n",
      "Epoch [2], Iter [91/101] Loss: 0.6719\n",
      "Epoch [3], Iter [91/101] Loss: 0.6659\n",
      "Epoch [4], Iter [91/101] Loss: 0.6257\n",
      "Epoch [5], Iter [91/101] Loss: 0.6086\n",
      "Epoch [6], Iter [91/101] Loss: 0.7203\n",
      "Epoch [7], Iter [91/101] Loss: 0.5898\n",
      "Epoch [8], Iter [91/101] Loss: 0.5873\n",
      "Epoch [9], Iter [91/101] Loss: 0.5966\n",
      "Epoch [10], Iter [91/101] Loss: 1.3515\n",
      "Epoch [11], Iter [91/101] Loss: 0.5806\n",
      "Epoch [12], Iter [91/101] Loss: 0.5678\n",
      "Epoch [13], Iter [91/101] Loss: 0.6327\n",
      "Epoch [14], Iter [91/101] Loss: 0.5538\n",
      "Epoch [15], Iter [91/101] Loss: 0.5440\n",
      "Epoch [16], Iter [91/101] Loss: 0.5796\n",
      "Epoch [17], Iter [91/101] Loss: 0.5443\n",
      "Epoch [18], Iter [91/101] Loss: 0.5491\n",
      "Epoch [19], Iter [91/101] Loss: 0.5648\n",
      "Test MSE: 0.6720426678657532\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a86bf6651f7b7b1c8b652a12688aa7c9e_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7137\n",
      "Epoch [2], Iter [91/101] Loss: 0.8027\n",
      "Epoch [3], Iter [91/101] Loss: 0.6737\n",
      "Epoch [4], Iter [91/101] Loss: 0.6257\n",
      "Epoch [5], Iter [91/101] Loss: 0.6737\n",
      "Epoch [6], Iter [91/101] Loss: 0.5975\n",
      "Epoch [7], Iter [91/101] Loss: 0.6073\n",
      "Epoch [8], Iter [91/101] Loss: 0.6073\n",
      "Epoch [9], Iter [91/101] Loss: 0.5873\n",
      "Epoch [10], Iter [91/101] Loss: 0.5861\n",
      "Epoch [11], Iter [91/101] Loss: 0.6091\n",
      "Epoch [12], Iter [91/101] Loss: 0.5768\n",
      "Epoch [13], Iter [91/101] Loss: 0.5655\n",
      "Epoch [14], Iter [91/101] Loss: 0.5535\n",
      "Epoch [15], Iter [91/101] Loss: 0.5578\n",
      "Epoch [16], Iter [91/101] Loss: 0.6161\n",
      "Epoch [17], Iter [91/101] Loss: 0.5224\n",
      "Epoch [18], Iter [91/101] Loss: 0.5506\n",
      "Epoch [19], Iter [91/101] Loss: 0.5243\n",
      "Test MSE: 0.6839964389801025\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a8629f0df5147f02c96ff1ab4563c01e8_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6901\n",
      "Epoch [2], Iter [91/101] Loss: 0.7006\n",
      "Epoch [3], Iter [91/101] Loss: 0.6877\n",
      "Epoch [4], Iter [91/101] Loss: 0.6677\n",
      "Epoch [5], Iter [91/101] Loss: 0.6318\n",
      "Epoch [6], Iter [91/101] Loss: 0.6496\n",
      "Epoch [7], Iter [91/101] Loss: 0.6178\n",
      "Epoch [8], Iter [91/101] Loss: 0.6131\n",
      "Epoch [9], Iter [91/101] Loss: 0.5968\n",
      "Epoch [10], Iter [91/101] Loss: 0.6227\n",
      "Epoch [11], Iter [91/101] Loss: 0.6423\n",
      "Epoch [12], Iter [91/101] Loss: 0.5733\n",
      "Epoch [13], Iter [91/101] Loss: 0.5684\n",
      "Epoch [14], Iter [91/101] Loss: 0.5517\n",
      "Epoch [15], Iter [91/101] Loss: 0.5683\n",
      "Epoch [16], Iter [91/101] Loss: 0.5725\n",
      "Epoch [17], Iter [91/101] Loss: 0.5360\n",
      "Test MSE: 0.681359589099884\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a86f7ddf81f12463a47e017d1eedb7422_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6767\n",
      "Epoch [2], Iter [91/101] Loss: 0.6969\n",
      "Epoch [3], Iter [91/101] Loss: 0.6643\n",
      "Epoch [4], Iter [91/101] Loss: 0.6618\n",
      "Epoch [5], Iter [91/101] Loss: 0.6187\n",
      "Epoch [6], Iter [91/101] Loss: 0.6268\n",
      "Epoch [7], Iter [91/101] Loss: 0.6134\n",
      "Epoch [8], Iter [91/101] Loss: 0.7394\n",
      "Epoch [9], Iter [91/101] Loss: 0.5636\n",
      "Epoch [10], Iter [91/101] Loss: 0.5717\n",
      "Epoch [11], Iter [91/101] Loss: 0.5699\n",
      "Epoch [12], Iter [91/101] Loss: 0.5842\n",
      "Epoch [13], Iter [91/101] Loss: 0.5803\n",
      "Epoch [14], Iter [91/101] Loss: 0.5440\n",
      "Epoch [15], Iter [91/101] Loss: 0.5652\n",
      "Epoch [16], Iter [91/101] Loss: 0.5560\n",
      "Epoch [17], Iter [91/101] Loss: 0.5308\n",
      "Epoch [18], Iter [91/101] Loss: 0.5231\n",
      "Test MSE: 0.6927605867385864\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a86d257d53d8ef589a76654824e510f69_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6671\n",
      "Epoch [2], Iter [91/101] Loss: 0.6864\n",
      "Epoch [3], Iter [91/101] Loss: 0.6886\n",
      "Epoch [4], Iter [91/101] Loss: 0.6346\n",
      "Epoch [5], Iter [91/101] Loss: 0.6135\n",
      "Epoch [6], Iter [91/101] Loss: 0.5919\n",
      "Epoch [7], Iter [91/101] Loss: 0.6045\n",
      "Epoch [8], Iter [91/101] Loss: 0.5716\n",
      "Epoch [9], Iter [91/101] Loss: 0.5729\n",
      "Epoch [10], Iter [91/101] Loss: 0.6434\n",
      "Epoch [11], Iter [91/101] Loss: 0.6050\n",
      "Epoch [12], Iter [91/101] Loss: 0.5699\n",
      "Epoch [13], Iter [91/101] Loss: 0.5917\n",
      "Epoch [14], Iter [91/101] Loss: 0.5644\n",
      "Epoch [15], Iter [91/101] Loss: 0.5557\n",
      "Epoch [16], Iter [91/101] Loss: 0.5771\n",
      "Epoch [17], Iter [91/101] Loss: 0.5354\n",
      "Epoch [18], Iter [91/101] Loss: 0.5382\n",
      "Test MSE: 0.6777284741401672\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a86c4703203dbcdfcf27494db986692f8_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7356\n",
      "Epoch [2], Iter [91/101] Loss: 0.7089\n",
      "Epoch [3], Iter [91/101] Loss: 0.6217\n",
      "Epoch [4], Iter [91/101] Loss: 0.6596\n",
      "Epoch [5], Iter [91/101] Loss: 0.6229\n",
      "Epoch [6], Iter [91/101] Loss: 0.6258\n",
      "Epoch [7], Iter [91/101] Loss: 0.6192\n",
      "Epoch [8], Iter [91/101] Loss: 0.6052\n",
      "Epoch [9], Iter [91/101] Loss: 0.5931\n",
      "Epoch [10], Iter [91/101] Loss: 0.5993\n",
      "Epoch [11], Iter [91/101] Loss: 0.6780\n",
      "Epoch [12], Iter [91/101] Loss: 0.5626\n",
      "Epoch [13], Iter [91/101] Loss: 0.6147\n",
      "Epoch [14], Iter [91/101] Loss: 0.5879\n",
      "Epoch [15], Iter [91/101] Loss: 0.5712\n",
      "Epoch [16], Iter [91/101] Loss: 0.5663\n",
      "Epoch [17], Iter [91/101] Loss: 0.5628\n",
      "Epoch [18], Iter [91/101] Loss: 0.5139\n",
      "Epoch [19], Iter [91/101] Loss: 0.5411\n",
      "Epoch [20], Iter [91/101] Loss: 0.5178\n",
      "Epoch [21], Iter [91/101] Loss: 0.5153\n",
      "Epoch [22], Iter [91/101] Loss: 0.5162\n",
      "Test MSE: 0.6754432916641235\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce_new\\8e0390afdcfa45ac2d1e39bd129a86900efe69fa2ab0ac31487b096630e8_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7264\n",
      "Epoch [2], Iter [91/101] Loss: 0.7536\n",
      "Epoch [3], Iter [91/101] Loss: 0.7105\n",
      "Epoch [4], Iter [91/101] Loss: 0.6300\n",
      "Epoch [5], Iter [91/101] Loss: 0.6447\n",
      "Epoch [6], Iter [91/101] Loss: 0.5968\n",
      "Epoch [7], Iter [91/101] Loss: 0.6407\n",
      "Epoch [8], Iter [91/101] Loss: 0.7332\n",
      "Epoch [9], Iter [91/101] Loss: 0.5884\n",
      "Epoch [10], Iter [91/101] Loss: 0.5995\n",
      "Epoch [11], Iter [91/101] Loss: 0.5972\n",
      "Epoch [12], Iter [91/101] Loss: 0.5811\n",
      "Epoch [13], Iter [91/101] Loss: 0.5638\n",
      "Epoch [14], Iter [91/101] Loss: 0.5701\n",
      "Epoch [15], Iter [91/101] Loss: 0.5487\n",
      "Epoch [16], Iter [91/101] Loss: 0.5585\n",
      "Test MSE: 0.6784927845001221\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder, use_tensorboard=True)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8795\n",
      "Epoch [2], Iter [91/101] Loss: 0.7819\n",
      "Epoch [3], Iter [91/101] Loss: 0.7030\n",
      "Epoch [4], Iter [91/101] Loss: 0.6784\n",
      "Epoch [5], Iter [91/101] Loss: 0.6793\n",
      "Epoch [6], Iter [91/101] Loss: 0.8070\n",
      "Epoch [7], Iter [91/101] Loss: 0.6520\n",
      "Epoch [8], Iter [91/101] Loss: 0.6431\n",
      "Epoch [9], Iter [91/101] Loss: 0.6529\n",
      "Epoch [10], Iter [91/101] Loss: 0.6045\n",
      "Epoch [11], Iter [91/101] Loss: 0.6185\n",
      "Epoch [12], Iter [91/101] Loss: 0.6544\n",
      "Epoch [13], Iter [91/101] Loss: 0.6029\n",
      "Epoch [14], Iter [91/101] Loss: 0.5955\n",
      "Epoch [15], Iter [91/101] Loss: 0.6013\n",
      "Epoch [16], Iter [91/101] Loss: 0.5812\n",
      "Epoch [17], Iter [91/101] Loss: 0.5962\n",
      "Epoch [18], Iter [91/101] Loss: 0.5667\n",
      "Test MSE: 0.727611780166626\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7996\n",
      "Epoch [2], Iter [91/101] Loss: 0.7293\n",
      "Epoch [3], Iter [91/101] Loss: 0.7033\n",
      "Epoch [4], Iter [91/101] Loss: 0.7298\n",
      "Epoch [5], Iter [91/101] Loss: 0.6603\n",
      "Epoch [6], Iter [91/101] Loss: 0.6775\n",
      "Epoch [7], Iter [91/101] Loss: 0.6734\n",
      "Epoch [8], Iter [91/101] Loss: 0.6622\n",
      "Epoch [9], Iter [91/101] Loss: 0.6474\n",
      "Epoch [10], Iter [91/101] Loss: 0.6205\n",
      "Epoch [11], Iter [91/101] Loss: 0.6100\n",
      "Epoch [12], Iter [91/101] Loss: 0.6281\n",
      "Epoch [13], Iter [91/101] Loss: 0.6497\n",
      "Epoch [14], Iter [91/101] Loss: 0.6342\n",
      "Epoch [15], Iter [91/101] Loss: 0.6222\n",
      "Epoch [16], Iter [91/101] Loss: 0.5939\n",
      "Epoch [17], Iter [91/101] Loss: 0.5687\n",
      "Epoch [18], Iter [91/101] Loss: 0.5700\n",
      "Epoch [19], Iter [91/101] Loss: 0.5916\n",
      "Epoch [20], Iter [91/101] Loss: 0.5710\n",
      "Epoch [21], Iter [91/101] Loss: 0.5600\n",
      "Epoch [22], Iter [91/101] Loss: 0.5620\n",
      "Epoch [23], Iter [91/101] Loss: 0.5531\n",
      "Epoch [24], Iter [91/101] Loss: 0.5498\n",
      "Test MSE: 0.7313403487205505\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7627\n",
      "Epoch [2], Iter [91/101] Loss: 0.8433\n",
      "Epoch [3], Iter [91/101] Loss: 0.7340\n",
      "Epoch [4], Iter [91/101] Loss: 0.6740\n",
      "Epoch [5], Iter [91/101] Loss: 0.6749\n",
      "Epoch [6], Iter [91/101] Loss: 0.7557\n",
      "Epoch [7], Iter [91/101] Loss: 0.6515\n",
      "Epoch [8], Iter [91/101] Loss: 0.6645\n",
      "Epoch [9], Iter [91/101] Loss: 0.6101\n",
      "Epoch [10], Iter [91/101] Loss: 0.6856\n",
      "Epoch [11], Iter [91/101] Loss: 0.6407\n",
      "Epoch [12], Iter [91/101] Loss: 0.6075\n",
      "Epoch [13], Iter [91/101] Loss: 0.5926\n",
      "Epoch [14], Iter [91/101] Loss: 0.5969\n",
      "Epoch [15], Iter [91/101] Loss: 0.6103\n",
      "Epoch [16], Iter [91/101] Loss: 0.5796\n",
      "Epoch [17], Iter [91/101] Loss: 0.6329\n",
      "Epoch [18], Iter [91/101] Loss: 1.2495\n",
      "Epoch [19], Iter [91/101] Loss: 0.6153\n",
      "Epoch [20], Iter [91/101] Loss: 0.5614\n",
      "Epoch [21], Iter [91/101] Loss: 0.6015\n",
      "Epoch [22], Iter [91/101] Loss: 0.5566\n",
      "Test MSE: 0.7319598197937012\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8137\n",
      "Epoch [2], Iter [91/101] Loss: 0.7349\n",
      "Epoch [3], Iter [91/101] Loss: 0.7407\n",
      "Epoch [4], Iter [91/101] Loss: 0.7273\n",
      "Epoch [5], Iter [91/101] Loss: 0.6254\n",
      "Epoch [6], Iter [91/101] Loss: 0.7317\n",
      "Epoch [7], Iter [91/101] Loss: 0.6820\n",
      "Epoch [8], Iter [91/101] Loss: 0.6678\n",
      "Epoch [9], Iter [91/101] Loss: 0.6177\n",
      "Epoch [10], Iter [91/101] Loss: 0.6390\n",
      "Epoch [11], Iter [91/101] Loss: 0.6078\n",
      "Epoch [12], Iter [91/101] Loss: 0.6042\n",
      "Epoch [13], Iter [91/101] Loss: 0.5979\n",
      "Epoch [14], Iter [91/101] Loss: 0.5795\n",
      "Epoch [15], Iter [91/101] Loss: 0.5975\n",
      "Epoch [16], Iter [91/101] Loss: 0.5939\n",
      "Epoch [17], Iter [91/101] Loss: 0.5674\n",
      "Epoch [18], Iter [91/101] Loss: 0.5635\n",
      "Epoch [19], Iter [91/101] Loss: 0.5604\n",
      "Test MSE: 0.7259851694107056\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9319\n",
      "Epoch [2], Iter [91/101] Loss: 0.7645\n",
      "Epoch [3], Iter [91/101] Loss: 0.7131\n",
      "Epoch [4], Iter [91/101] Loss: 0.6731\n",
      "Epoch [5], Iter [91/101] Loss: 0.7091\n",
      "Epoch [6], Iter [91/101] Loss: 0.6692\n",
      "Epoch [7], Iter [91/101] Loss: 0.6386\n",
      "Epoch [8], Iter [91/101] Loss: 0.6422\n",
      "Epoch [9], Iter [91/101] Loss: 0.6508\n",
      "Epoch [10], Iter [91/101] Loss: 0.6166\n",
      "Epoch [11], Iter [91/101] Loss: 0.6302\n",
      "Epoch [12], Iter [91/101] Loss: 0.6038\n",
      "Epoch [13], Iter [91/101] Loss: 0.6003\n",
      "Epoch [14], Iter [91/101] Loss: 0.6149\n",
      "Epoch [15], Iter [91/101] Loss: 0.6890\n",
      "Epoch [16], Iter [91/101] Loss: 0.5863\n",
      "Epoch [17], Iter [91/101] Loss: 0.5779\n",
      "Epoch [18], Iter [91/101] Loss: 0.5719\n",
      "Epoch [19], Iter [91/101] Loss: 0.5864\n",
      "Test MSE: 0.731033205986023\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7898\n",
      "Epoch [2], Iter [91/101] Loss: 0.7164\n",
      "Epoch [3], Iter [91/101] Loss: 0.7585\n",
      "Epoch [4], Iter [91/101] Loss: 0.6585\n",
      "Epoch [5], Iter [91/101] Loss: 0.6805\n",
      "Epoch [6], Iter [91/101] Loss: 0.6526\n",
      "Epoch [7], Iter [91/101] Loss: 0.6474\n",
      "Epoch [8], Iter [91/101] Loss: 0.6152\n",
      "Epoch [9], Iter [91/101] Loss: 0.6921\n",
      "Epoch [10], Iter [91/101] Loss: 0.6403\n",
      "Epoch [11], Iter [91/101] Loss: 0.6011\n",
      "Epoch [12], Iter [91/101] Loss: 0.6179\n",
      "Epoch [13], Iter [91/101] Loss: 0.5969\n",
      "Epoch [14], Iter [91/101] Loss: 0.5887\n",
      "Epoch [15], Iter [91/101] Loss: 0.5984\n",
      "Epoch [16], Iter [91/101] Loss: 0.5896\n",
      "Epoch [17], Iter [91/101] Loss: 0.5874\n",
      "Epoch [18], Iter [91/101] Loss: 0.5698\n",
      "Epoch [19], Iter [91/101] Loss: 0.5726\n",
      "Test MSE: 0.7231959700584412\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7669\n",
      "Epoch [2], Iter [91/101] Loss: 0.7022\n",
      "Epoch [3], Iter [91/101] Loss: 0.6781\n",
      "Epoch [4], Iter [91/101] Loss: 0.6956\n",
      "Epoch [5], Iter [91/101] Loss: 0.6939\n",
      "Epoch [6], Iter [91/101] Loss: 0.7070\n",
      "Epoch [7], Iter [91/101] Loss: 0.6499\n",
      "Epoch [8], Iter [91/101] Loss: 0.6404\n",
      "Epoch [9], Iter [91/101] Loss: 0.6743\n",
      "Epoch [10], Iter [91/101] Loss: 0.6507\n",
      "Epoch [11], Iter [91/101] Loss: 0.6266\n",
      "Epoch [12], Iter [91/101] Loss: 0.6053\n",
      "Epoch [13], Iter [91/101] Loss: 0.6260\n",
      "Epoch [14], Iter [91/101] Loss: 0.6018\n",
      "Epoch [15], Iter [91/101] Loss: 0.6072\n",
      "Epoch [16], Iter [91/101] Loss: 0.6011\n",
      "Epoch [17], Iter [91/101] Loss: 0.6124\n",
      "Epoch [18], Iter [91/101] Loss: 0.6039\n",
      "Test MSE: 0.7311387658119202\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8830\n",
      "Epoch [2], Iter [91/101] Loss: 0.7393\n",
      "Epoch [3], Iter [91/101] Loss: 0.7833\n",
      "Epoch [4], Iter [91/101] Loss: 0.7805\n",
      "Epoch [5], Iter [91/101] Loss: 0.7571\n",
      "Epoch [6], Iter [91/101] Loss: 0.6763\n",
      "Epoch [7], Iter [91/101] Loss: 0.6687\n",
      "Epoch [8], Iter [91/101] Loss: 0.6262\n",
      "Epoch [9], Iter [91/101] Loss: 0.6235\n",
      "Epoch [10], Iter [91/101] Loss: 0.6514\n",
      "Epoch [11], Iter [91/101] Loss: 0.6795\n",
      "Epoch [12], Iter [91/101] Loss: 0.6587\n",
      "Epoch [13], Iter [91/101] Loss: 0.6041\n",
      "Epoch [14], Iter [91/101] Loss: 0.6010\n",
      "Epoch [15], Iter [91/101] Loss: 0.5792\n",
      "Epoch [16], Iter [91/101] Loss: 0.5773\n",
      "Epoch [17], Iter [91/101] Loss: 0.6084\n",
      "Epoch [18], Iter [91/101] Loss: 0.5745\n",
      "Epoch [19], Iter [91/101] Loss: 0.5679\n",
      "Test MSE: 0.7330056428909302\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8310\n",
      "Epoch [2], Iter [91/101] Loss: 0.7751\n",
      "Epoch [3], Iter [91/101] Loss: 0.7384\n",
      "Epoch [4], Iter [91/101] Loss: 0.7259\n",
      "Epoch [5], Iter [91/101] Loss: 0.7317\n",
      "Epoch [6], Iter [91/101] Loss: 0.6656\n",
      "Epoch [7], Iter [91/101] Loss: 0.6894\n",
      "Epoch [8], Iter [91/101] Loss: 0.6969\n",
      "Epoch [9], Iter [91/101] Loss: 0.6269\n",
      "Epoch [10], Iter [91/101] Loss: 0.6567\n",
      "Epoch [11], Iter [91/101] Loss: 0.6200\n",
      "Epoch [12], Iter [91/101] Loss: 0.6461\n",
      "Epoch [13], Iter [91/101] Loss: 0.6382\n",
      "Epoch [14], Iter [91/101] Loss: 0.6267\n",
      "Epoch [15], Iter [91/101] Loss: 0.6260\n",
      "Epoch [16], Iter [91/101] Loss: 0.5568\n",
      "Epoch [17], Iter [91/101] Loss: 0.6582\n",
      "Epoch [18], Iter [91/101] Loss: 0.5887\n",
      "Epoch [19], Iter [91/101] Loss: 0.5821\n",
      "Epoch [20], Iter [91/101] Loss: 0.5629\n",
      "Test MSE: 0.7347075939178467\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8312\n",
      "Epoch [2], Iter [91/101] Loss: 0.7628\n",
      "Epoch [3], Iter [91/101] Loss: 0.8085\n",
      "Epoch [4], Iter [91/101] Loss: 0.7060\n",
      "Epoch [5], Iter [91/101] Loss: 0.7049\n",
      "Epoch [6], Iter [91/101] Loss: 0.7393\n",
      "Epoch [7], Iter [91/101] Loss: 0.6682\n",
      "Epoch [8], Iter [91/101] Loss: 0.6576\n",
      "Epoch [9], Iter [91/101] Loss: 0.7679\n",
      "Epoch [10], Iter [91/101] Loss: 0.6463\n",
      "Epoch [11], Iter [91/101] Loss: 0.6348\n",
      "Epoch [12], Iter [91/101] Loss: 0.6110\n",
      "Epoch [13], Iter [91/101] Loss: 0.6213\n",
      "Epoch [14], Iter [91/101] Loss: 0.6114\n",
      "Epoch [15], Iter [91/101] Loss: 0.5923\n",
      "Epoch [16], Iter [91/101] Loss: 0.5801\n",
      "Epoch [17], Iter [91/101] Loss: 0.5867\n",
      "Test MSE: 0.726960301399231\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slp, tas, precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6470\n",
      "Epoch [2], Iter [91/101] Loss: 0.5863\n",
      "Epoch [3], Iter [91/101] Loss: 0.5896\n",
      "Epoch [4], Iter [91/101] Loss: 0.5810\n",
      "Epoch [5], Iter [91/101] Loss: 0.5979\n",
      "Epoch [6], Iter [91/101] Loss: 0.5712\n",
      "Epoch [7], Iter [91/101] Loss: 0.5837\n",
      "Epoch [8], Iter [91/101] Loss: 0.5458\n",
      "Epoch [9], Iter [91/101] Loss: 0.5325\n",
      "Epoch [10], Iter [91/101] Loss: 0.5345\n",
      "Epoch [11], Iter [91/101] Loss: 0.5310\n",
      "Epoch [12], Iter [91/101] Loss: 0.5128\n",
      "Epoch [13], Iter [91/101] Loss: 0.5350\n",
      "Epoch [14], Iter [91/101] Loss: 0.5210\n",
      "Epoch [15], Iter [91/101] Loss: 0.5058\n",
      "Epoch [16], Iter [91/101] Loss: 0.5482\n",
      "Epoch [17], Iter [91/101] Loss: 0.4994\n",
      "Epoch [18], Iter [91/101] Loss: 0.5107\n",
      "Epoch [19], Iter [91/101] Loss: 0.4930\n",
      "Epoch [20], Iter [91/101] Loss: 0.4803\n",
      "Epoch [21], Iter [91/101] Loss: 0.4779\n",
      "Epoch [22], Iter [91/101] Loss: 0.4778\n",
      "Epoch [23], Iter [91/101] Loss: 0.4888\n",
      "Epoch [24], Iter [91/101] Loss: 0.4628\n",
      "Epoch [25], Iter [91/101] Loss: 0.4559\n",
      "Epoch [26], Iter [91/101] Loss: 0.5317\n",
      "Epoch [27], Iter [91/101] Loss: 0.4594\n",
      "Test MSE: 0.5970920920372009\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6455\n",
      "Epoch [2], Iter [91/101] Loss: 0.6412\n",
      "Epoch [3], Iter [91/101] Loss: 0.6087\n",
      "Epoch [4], Iter [91/101] Loss: 0.5582\n",
      "Epoch [5], Iter [91/101] Loss: 0.5899\n",
      "Epoch [6], Iter [91/101] Loss: 0.5865\n",
      "Epoch [7], Iter [91/101] Loss: 0.5756\n",
      "Epoch [8], Iter [91/101] Loss: 0.5495\n",
      "Epoch [9], Iter [91/101] Loss: 0.5289\n",
      "Epoch [10], Iter [91/101] Loss: 0.5404\n",
      "Epoch [11], Iter [91/101] Loss: 0.5457\n",
      "Epoch [12], Iter [91/101] Loss: 0.5106\n",
      "Epoch [13], Iter [91/101] Loss: 0.5194\n",
      "Epoch [14], Iter [91/101] Loss: 0.5291\n",
      "Epoch [15], Iter [91/101] Loss: 0.5643\n",
      "Epoch [16], Iter [91/101] Loss: 0.5199\n",
      "Epoch [17], Iter [91/101] Loss: 0.4988\n",
      "Epoch [18], Iter [91/101] Loss: 0.4882\n",
      "Epoch [19], Iter [91/101] Loss: 0.4834\n",
      "Epoch [20], Iter [91/101] Loss: 0.4963\n",
      "Epoch [21], Iter [91/101] Loss: 0.4885\n",
      "Epoch [22], Iter [91/101] Loss: 0.4531\n",
      "Epoch [23], Iter [91/101] Loss: 0.4600\n",
      "Test MSE: 0.5985498428344727\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6276\n",
      "Epoch [2], Iter [91/101] Loss: 0.6982\n",
      "Epoch [3], Iter [91/101] Loss: 0.5769\n",
      "Epoch [4], Iter [91/101] Loss: 0.5724\n",
      "Epoch [5], Iter [91/101] Loss: 0.5654\n",
      "Epoch [6], Iter [91/101] Loss: 0.5574\n",
      "Epoch [7], Iter [91/101] Loss: 0.5428\n",
      "Epoch [8], Iter [91/101] Loss: 0.5437\n",
      "Epoch [9], Iter [91/101] Loss: 0.5545\n",
      "Epoch [10], Iter [91/101] Loss: 0.5175\n",
      "Epoch [11], Iter [91/101] Loss: 0.5394\n",
      "Epoch [12], Iter [91/101] Loss: 0.5006\n",
      "Epoch [13], Iter [91/101] Loss: 0.5809\n",
      "Epoch [14], Iter [91/101] Loss: 0.4891\n",
      "Epoch [15], Iter [91/101] Loss: 0.4923\n",
      "Epoch [16], Iter [91/101] Loss: 0.5041\n",
      "Epoch [17], Iter [91/101] Loss: 0.5947\n",
      "Epoch [18], Iter [91/101] Loss: 0.4928\n",
      "Epoch [19], Iter [91/101] Loss: 0.4851\n",
      "Epoch [20], Iter [91/101] Loss: 0.4899\n",
      "Epoch [21], Iter [91/101] Loss: 0.4755\n",
      "Epoch [22], Iter [91/101] Loss: 0.4486\n",
      "Epoch [23], Iter [91/101] Loss: 0.4530\n",
      "Test MSE: 0.6098532676696777\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6448\n",
      "Epoch [2], Iter [91/101] Loss: 0.5836\n",
      "Epoch [3], Iter [91/101] Loss: 0.5828\n",
      "Epoch [4], Iter [91/101] Loss: 0.6095\n",
      "Epoch [5], Iter [91/101] Loss: 0.6038\n",
      "Epoch [6], Iter [91/101] Loss: 0.5893\n",
      "Epoch [7], Iter [91/101] Loss: 0.5764\n",
      "Epoch [8], Iter [91/101] Loss: 0.5497\n",
      "Epoch [9], Iter [91/101] Loss: 0.5378\n",
      "Epoch [10], Iter [91/101] Loss: 0.5193\n",
      "Epoch [11], Iter [91/101] Loss: 0.5317\n",
      "Epoch [12], Iter [91/101] Loss: 0.5107\n",
      "Epoch [13], Iter [91/101] Loss: 0.5210\n",
      "Epoch [14], Iter [91/101] Loss: 0.4963\n",
      "Epoch [15], Iter [91/101] Loss: 0.5166\n",
      "Epoch [16], Iter [91/101] Loss: 0.4836\n",
      "Epoch [17], Iter [91/101] Loss: 0.4891\n",
      "Epoch [18], Iter [91/101] Loss: 0.4693\n",
      "Epoch [19], Iter [91/101] Loss: 0.4764\n",
      "Epoch [20], Iter [91/101] Loss: 0.4668\n",
      "Test MSE: 0.603970468044281\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6424\n",
      "Epoch [2], Iter [91/101] Loss: 0.8640\n",
      "Epoch [3], Iter [91/101] Loss: 0.6137\n",
      "Epoch [4], Iter [91/101] Loss: 0.5856\n",
      "Epoch [5], Iter [91/101] Loss: 0.5617\n",
      "Epoch [6], Iter [91/101] Loss: 0.5567\n",
      "Epoch [7], Iter [91/101] Loss: 0.5487\n",
      "Epoch [8], Iter [91/101] Loss: 0.5082\n",
      "Epoch [9], Iter [91/101] Loss: 0.5361\n",
      "Epoch [10], Iter [91/101] Loss: 0.5079\n",
      "Epoch [11], Iter [91/101] Loss: 0.5322\n",
      "Epoch [12], Iter [91/101] Loss: 0.6109\n",
      "Epoch [13], Iter [91/101] Loss: 0.5280\n",
      "Epoch [14], Iter [91/101] Loss: 0.4988\n",
      "Epoch [15], Iter [91/101] Loss: 0.4843\n",
      "Epoch [16], Iter [91/101] Loss: 0.4864\n",
      "Epoch [17], Iter [91/101] Loss: 0.4921\n",
      "Epoch [18], Iter [91/101] Loss: 0.5059\n",
      "Epoch [19], Iter [91/101] Loss: 0.4747\n",
      "Epoch [20], Iter [91/101] Loss: 0.4538\n",
      "Epoch [21], Iter [91/101] Loss: 0.4527\n",
      "Test MSE: 0.604677140712738\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6713\n",
      "Epoch [2], Iter [91/101] Loss: 0.6202\n",
      "Epoch [3], Iter [91/101] Loss: 0.5778\n",
      "Epoch [4], Iter [91/101] Loss: 0.5832\n",
      "Epoch [5], Iter [91/101] Loss: 0.5709\n",
      "Epoch [6], Iter [91/101] Loss: 0.5476\n",
      "Epoch [7], Iter [91/101] Loss: 0.5745\n",
      "Epoch [8], Iter [91/101] Loss: 0.5173\n",
      "Epoch [9], Iter [91/101] Loss: 0.5414\n",
      "Epoch [10], Iter [91/101] Loss: 0.5386\n",
      "Epoch [11], Iter [91/101] Loss: 0.5237\n",
      "Epoch [12], Iter [91/101] Loss: 0.5140\n",
      "Epoch [13], Iter [91/101] Loss: 0.5162\n",
      "Epoch [14], Iter [91/101] Loss: 0.5080\n",
      "Epoch [15], Iter [91/101] Loss: 0.5204\n",
      "Epoch [16], Iter [91/101] Loss: 0.4909\n",
      "Epoch [17], Iter [91/101] Loss: 0.4956\n",
      "Epoch [18], Iter [91/101] Loss: 0.4796\n",
      "Epoch [19], Iter [91/101] Loss: 0.4876\n",
      "Epoch [20], Iter [91/101] Loss: 0.4838\n",
      "Epoch [21], Iter [91/101] Loss: 0.4930\n",
      "Test MSE: 0.6067414283752441\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6440\n",
      "Epoch [2], Iter [91/101] Loss: 0.6573\n",
      "Epoch [3], Iter [91/101] Loss: 0.5916\n",
      "Epoch [4], Iter [91/101] Loss: 0.5734\n",
      "Epoch [5], Iter [91/101] Loss: 0.5697\n",
      "Epoch [6], Iter [91/101] Loss: 0.5483\n",
      "Epoch [7], Iter [91/101] Loss: 0.5845\n",
      "Epoch [8], Iter [91/101] Loss: 0.5587\n",
      "Epoch [9], Iter [91/101] Loss: 0.5372\n",
      "Epoch [10], Iter [91/101] Loss: 0.5270\n",
      "Epoch [11], Iter [91/101] Loss: 0.5235\n",
      "Epoch [12], Iter [91/101] Loss: 0.5032\n",
      "Epoch [13], Iter [91/101] Loss: 0.4966\n",
      "Epoch [14], Iter [91/101] Loss: 0.5305\n",
      "Epoch [15], Iter [91/101] Loss: 0.4997\n",
      "Epoch [16], Iter [91/101] Loss: 0.6555\n",
      "Epoch [17], Iter [91/101] Loss: 0.4958\n",
      "Epoch [18], Iter [91/101] Loss: 0.4954\n",
      "Epoch [19], Iter [91/101] Loss: 0.4909\n",
      "Epoch [20], Iter [91/101] Loss: 0.4665\n",
      "Epoch [21], Iter [91/101] Loss: 0.4814\n",
      "Test MSE: 0.6037472486495972\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6240\n",
      "Epoch [2], Iter [91/101] Loss: 0.6145\n",
      "Epoch [3], Iter [91/101] Loss: 0.5799\n",
      "Epoch [4], Iter [91/101] Loss: 0.5696\n",
      "Epoch [5], Iter [91/101] Loss: 0.5795\n",
      "Epoch [6], Iter [91/101] Loss: 0.5716\n",
      "Epoch [7], Iter [91/101] Loss: 0.5517\n",
      "Epoch [8], Iter [91/101] Loss: 0.5586\n",
      "Epoch [9], Iter [91/101] Loss: 0.5257\n",
      "Epoch [10], Iter [91/101] Loss: 0.5347\n",
      "Epoch [11], Iter [91/101] Loss: 0.5867\n",
      "Epoch [12], Iter [91/101] Loss: 0.5497\n",
      "Epoch [13], Iter [91/101] Loss: 0.5091\n",
      "Epoch [14], Iter [91/101] Loss: 0.5310\n",
      "Epoch [15], Iter [91/101] Loss: 0.4975\n",
      "Epoch [16], Iter [91/101] Loss: 0.4931\n",
      "Epoch [17], Iter [91/101] Loss: 0.4980\n",
      "Epoch [18], Iter [91/101] Loss: 0.5580\n",
      "Test MSE: 0.602453351020813\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6334\n",
      "Epoch [2], Iter [91/101] Loss: 0.6106\n",
      "Epoch [3], Iter [91/101] Loss: 0.5931\n",
      "Epoch [4], Iter [91/101] Loss: 0.5883\n",
      "Epoch [5], Iter [91/101] Loss: 0.5694\n",
      "Epoch [6], Iter [91/101] Loss: 0.5491\n",
      "Epoch [7], Iter [91/101] Loss: 0.5554\n",
      "Epoch [8], Iter [91/101] Loss: 0.5376\n",
      "Epoch [9], Iter [91/101] Loss: 0.5156\n",
      "Epoch [10], Iter [91/101] Loss: 0.5260\n",
      "Epoch [11], Iter [91/101] Loss: 0.6214\n",
      "Epoch [12], Iter [91/101] Loss: 0.5249\n",
      "Epoch [13], Iter [91/101] Loss: 0.5486\n",
      "Epoch [14], Iter [91/101] Loss: 0.5315\n",
      "Epoch [15], Iter [91/101] Loss: 0.5063\n",
      "Epoch [16], Iter [91/101] Loss: 0.4979\n",
      "Epoch [17], Iter [91/101] Loss: 0.4783\n",
      "Epoch [18], Iter [91/101] Loss: 0.4850\n",
      "Epoch [19], Iter [91/101] Loss: 0.5709\n",
      "Epoch [20], Iter [91/101] Loss: 0.4959\n",
      "Epoch [21], Iter [91/101] Loss: 0.4685\n",
      "Epoch [22], Iter [91/101] Loss: 0.9873\n",
      "Epoch [23], Iter [91/101] Loss: 0.5168\n",
      "Epoch [24], Iter [91/101] Loss: 0.4307\n",
      "Epoch [25], Iter [91/101] Loss: 0.4423\n",
      "Test MSE: 0.6136957406997681\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6500\n",
      "Epoch [2], Iter [91/101] Loss: 0.6097\n",
      "Epoch [3], Iter [91/101] Loss: 0.5748\n",
      "Epoch [4], Iter [91/101] Loss: 0.5674\n",
      "Epoch [5], Iter [91/101] Loss: 0.6030\n",
      "Epoch [6], Iter [91/101] Loss: 0.5841\n",
      "Epoch [7], Iter [91/101] Loss: 0.5651\n",
      "Epoch [8], Iter [91/101] Loss: 0.5270\n",
      "Epoch [9], Iter [91/101] Loss: 0.5172\n",
      "Epoch [10], Iter [91/101] Loss: 0.5549\n",
      "Epoch [11], Iter [91/101] Loss: 0.5226\n",
      "Epoch [12], Iter [91/101] Loss: 0.5873\n",
      "Epoch [13], Iter [91/101] Loss: 0.5036\n",
      "Epoch [14], Iter [91/101] Loss: 0.5285\n",
      "Epoch [15], Iter [91/101] Loss: 0.5265\n",
      "Epoch [16], Iter [91/101] Loss: 0.5145\n",
      "Epoch [17], Iter [91/101] Loss: 0.5075\n",
      "Epoch [18], Iter [91/101] Loss: 0.4708\n",
      "Epoch [19], Iter [91/101] Loss: 0.4929\n",
      "Test MSE: 0.5945423245429993\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precip, tas, orogrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\",\n",
    "                                \"oro\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"oro\": [\"ht\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Global\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 1.0086\n",
      "Epoch [2], Iter [91/101] Loss: 0.9052\n",
      "Epoch [3], Iter [91/101] Loss: 0.7824\n",
      "Epoch [4], Iter [91/101] Loss: 0.6925\n",
      "Epoch [5], Iter [91/101] Loss: 0.6798\n",
      "Epoch [6], Iter [91/101] Loss: 0.6450\n",
      "Epoch [7], Iter [91/101] Loss: 0.6548\n",
      "Epoch [8], Iter [91/101] Loss: 0.6445\n",
      "Epoch [9], Iter [91/101] Loss: 0.6056\n",
      "Epoch [10], Iter [91/101] Loss: 0.5997\n",
      "Epoch [11], Iter [91/101] Loss: 0.6155\n",
      "Epoch [12], Iter [91/101] Loss: 0.5735\n",
      "Epoch [13], Iter [91/101] Loss: 0.5746\n",
      "Epoch [14], Iter [91/101] Loss: 0.5516\n",
      "Epoch [15], Iter [91/101] Loss: 0.5307\n",
      "Epoch [16], Iter [91/101] Loss: 0.5490\n",
      "Epoch [17], Iter [91/101] Loss: 0.5437\n",
      "Epoch [18], Iter [91/101] Loss: 0.5116\n",
      "Epoch [19], Iter [91/101] Loss: 0.5137\n",
      "Epoch [20], Iter [91/101] Loss: 0.4909\n",
      "Epoch [21], Iter [91/101] Loss: 0.4919\n",
      "Epoch [22], Iter [91/101] Loss: 0.4779\n",
      "Test MSE: 0.6164028644561768\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9626\n",
      "Epoch [2], Iter [91/101] Loss: 0.7876\n",
      "Epoch [3], Iter [91/101] Loss: 0.7422\n",
      "Epoch [4], Iter [91/101] Loss: 0.6519\n",
      "Epoch [5], Iter [91/101] Loss: 0.6604\n",
      "Epoch [6], Iter [91/101] Loss: 0.5894\n",
      "Epoch [7], Iter [91/101] Loss: 0.6081\n",
      "Epoch [8], Iter [91/101] Loss: 0.5653\n",
      "Epoch [9], Iter [91/101] Loss: 0.5609\n",
      "Epoch [10], Iter [91/101] Loss: 0.5775\n",
      "Epoch [11], Iter [91/101] Loss: 0.5493\n",
      "Epoch [12], Iter [91/101] Loss: 0.5578\n",
      "Epoch [13], Iter [91/101] Loss: 0.5329\n",
      "Epoch [14], Iter [91/101] Loss: 0.5928\n",
      "Epoch [15], Iter [91/101] Loss: 0.5232\n",
      "Epoch [16], Iter [91/101] Loss: 0.5009\n",
      "Epoch [17], Iter [91/101] Loss: 0.5070\n",
      "Epoch [18], Iter [91/101] Loss: 0.4911\n",
      "Test MSE: 0.6257789731025696\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 1.1360\n",
      "Epoch [2], Iter [91/101] Loss: 0.9790\n",
      "Epoch [3], Iter [91/101] Loss: 0.8616\n",
      "Epoch [4], Iter [91/101] Loss: 0.7590\n",
      "Epoch [5], Iter [91/101] Loss: 0.6977\n",
      "Epoch [6], Iter [91/101] Loss: 0.6761\n",
      "Epoch [7], Iter [91/101] Loss: 0.6707\n",
      "Epoch [8], Iter [91/101] Loss: 0.6131\n",
      "Epoch [9], Iter [91/101] Loss: 0.5896\n",
      "Epoch [10], Iter [91/101] Loss: 0.6092\n",
      "Epoch [11], Iter [91/101] Loss: 0.5837\n",
      "Epoch [12], Iter [91/101] Loss: 0.5479\n",
      "Epoch [13], Iter [91/101] Loss: 0.5751\n",
      "Epoch [14], Iter [91/101] Loss: 0.5364\n",
      "Epoch [15], Iter [91/101] Loss: 0.5101\n",
      "Epoch [16], Iter [91/101] Loss: 0.5338\n",
      "Epoch [17], Iter [91/101] Loss: 0.5375\n",
      "Epoch [18], Iter [91/101] Loss: 0.5153\n",
      "Epoch [19], Iter [91/101] Loss: 0.5005\n",
      "Epoch [20], Iter [91/101] Loss: 0.4909\n",
      "Epoch [21], Iter [91/101] Loss: 0.5308\n",
      "Epoch [22], Iter [91/101] Loss: 0.4990\n",
      "Epoch [23], Iter [91/101] Loss: 0.4707\n",
      "Epoch [24], Iter [91/101] Loss: 0.4687\n",
      "Epoch [25], Iter [91/101] Loss: 0.4652\n",
      "Test MSE: 0.6144933700561523\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9771\n",
      "Epoch [2], Iter [91/101] Loss: 0.8060\n",
      "Epoch [3], Iter [91/101] Loss: 0.7624\n",
      "Epoch [4], Iter [91/101] Loss: 0.7276\n",
      "Epoch [5], Iter [91/101] Loss: 0.6970\n",
      "Epoch [6], Iter [91/101] Loss: 0.6151\n",
      "Epoch [7], Iter [91/101] Loss: 0.6994\n",
      "Epoch [8], Iter [91/101] Loss: 0.6108\n",
      "Epoch [9], Iter [91/101] Loss: 0.5633\n",
      "Epoch [10], Iter [91/101] Loss: 0.5817\n",
      "Epoch [11], Iter [91/101] Loss: 0.5527\n",
      "Epoch [12], Iter [91/101] Loss: 0.5487\n",
      "Epoch [13], Iter [91/101] Loss: 0.5429\n",
      "Epoch [14], Iter [91/101] Loss: 0.5223\n",
      "Epoch [15], Iter [91/101] Loss: 0.5225\n",
      "Epoch [16], Iter [91/101] Loss: 0.5196\n",
      "Epoch [17], Iter [91/101] Loss: 0.5191\n",
      "Epoch [18], Iter [91/101] Loss: 0.5828\n",
      "Test MSE: 0.6140942573547363\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9620\n",
      "Epoch [2], Iter [91/101] Loss: 0.8990\n",
      "Epoch [3], Iter [91/101] Loss: 0.7707\n",
      "Epoch [4], Iter [91/101] Loss: 0.7142\n",
      "Epoch [5], Iter [91/101] Loss: 0.6745\n",
      "Epoch [6], Iter [91/101] Loss: 0.7171\n",
      "Epoch [7], Iter [91/101] Loss: 0.6214\n",
      "Epoch [8], Iter [91/101] Loss: 0.6258\n",
      "Epoch [9], Iter [91/101] Loss: 0.5734\n",
      "Epoch [10], Iter [91/101] Loss: 0.5987\n",
      "Epoch [11], Iter [91/101] Loss: 0.5574\n",
      "Epoch [12], Iter [91/101] Loss: 0.5584\n",
      "Epoch [13], Iter [91/101] Loss: 0.5422\n",
      "Epoch [14], Iter [91/101] Loss: 0.5447\n",
      "Epoch [15], Iter [91/101] Loss: 0.5405\n",
      "Epoch [16], Iter [91/101] Loss: 0.4976\n",
      "Epoch [17], Iter [91/101] Loss: 0.5306\n",
      "Epoch [18], Iter [91/101] Loss: 0.5038\n",
      "Epoch [19], Iter [91/101] Loss: 0.5027\n",
      "Epoch [20], Iter [91/101] Loss: 0.5076\n",
      "Epoch [21], Iter [91/101] Loss: 0.4924\n",
      "Epoch [22], Iter [91/101] Loss: 0.4996\n",
      "Test MSE: 0.6243277788162231\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9862\n",
      "Epoch [2], Iter [91/101] Loss: 0.9224\n",
      "Epoch [3], Iter [91/101] Loss: 0.7246\n",
      "Epoch [4], Iter [91/101] Loss: 0.8585\n",
      "Epoch [5], Iter [91/101] Loss: 0.6363\n",
      "Epoch [6], Iter [91/101] Loss: 0.6304\n",
      "Epoch [7], Iter [91/101] Loss: 0.5943\n",
      "Epoch [8], Iter [91/101] Loss: 0.6098\n",
      "Epoch [9], Iter [91/101] Loss: 0.5643\n",
      "Epoch [10], Iter [91/101] Loss: 0.5882\n",
      "Epoch [11], Iter [91/101] Loss: 0.5374\n",
      "Epoch [12], Iter [91/101] Loss: 0.5193\n",
      "Epoch [13], Iter [91/101] Loss: 0.5267\n",
      "Epoch [14], Iter [91/101] Loss: 0.5618\n",
      "Epoch [15], Iter [91/101] Loss: 0.5189\n",
      "Epoch [16], Iter [91/101] Loss: 0.5249\n",
      "Epoch [17], Iter [91/101] Loss: 0.5082\n",
      "Epoch [18], Iter [91/101] Loss: 0.5033\n",
      "Epoch [19], Iter [91/101] Loss: 0.5811\n",
      "Epoch [20], Iter [91/101] Loss: 0.5014\n",
      "Epoch [21], Iter [91/101] Loss: 0.4714\n",
      "Epoch [22], Iter [91/101] Loss: 0.4751\n",
      "Epoch [23], Iter [91/101] Loss: 0.4809\n",
      "Epoch [24], Iter [91/101] Loss: 0.4600\n",
      "Test MSE: 0.6068044900894165\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9948\n",
      "Epoch [2], Iter [91/101] Loss: 0.8284\n",
      "Epoch [3], Iter [91/101] Loss: 0.7354\n",
      "Epoch [4], Iter [91/101] Loss: 0.6713\n",
      "Epoch [5], Iter [91/101] Loss: 0.6860\n",
      "Epoch [6], Iter [91/101] Loss: 0.6140\n",
      "Epoch [7], Iter [91/101] Loss: 0.5960\n",
      "Epoch [8], Iter [91/101] Loss: 0.6346\n",
      "Epoch [9], Iter [91/101] Loss: 0.5626\n",
      "Epoch [10], Iter [91/101] Loss: 0.5448\n",
      "Epoch [11], Iter [91/101] Loss: 0.5463\n",
      "Epoch [12], Iter [91/101] Loss: 0.5756\n",
      "Epoch [13], Iter [91/101] Loss: 0.5315\n",
      "Epoch [14], Iter [91/101] Loss: 0.5552\n",
      "Epoch [15], Iter [91/101] Loss: 0.5213\n",
      "Epoch [16], Iter [91/101] Loss: 0.4928\n",
      "Epoch [17], Iter [91/101] Loss: 0.5338\n",
      "Epoch [18], Iter [91/101] Loss: 0.5093\n",
      "Epoch [19], Iter [91/101] Loss: 0.4944\n",
      "Epoch [20], Iter [91/101] Loss: 0.4959\n",
      "Epoch [21], Iter [91/101] Loss: 0.4793\n",
      "Epoch [22], Iter [91/101] Loss: 0.5183\n",
      "Epoch [23], Iter [91/101] Loss: 0.4593\n",
      "Epoch [24], Iter [91/101] Loss: 0.4756\n",
      "Test MSE: 0.6228976845741272\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 1.0175\n",
      "Epoch [2], Iter [91/101] Loss: 0.9863\n",
      "Epoch [3], Iter [91/101] Loss: 0.8012\n",
      "Epoch [4], Iter [91/101] Loss: 0.7135\n",
      "Epoch [5], Iter [91/101] Loss: 0.6333\n",
      "Epoch [6], Iter [91/101] Loss: 0.5993\n",
      "Epoch [7], Iter [91/101] Loss: 0.6162\n",
      "Epoch [8], Iter [91/101] Loss: 0.5620\n",
      "Epoch [9], Iter [91/101] Loss: 0.5796\n",
      "Epoch [10], Iter [91/101] Loss: 0.5634\n",
      "Epoch [11], Iter [91/101] Loss: 0.5619\n",
      "Epoch [12], Iter [91/101] Loss: 0.5434\n",
      "Epoch [13], Iter [91/101] Loss: 0.5601\n",
      "Epoch [14], Iter [91/101] Loss: 0.5332\n",
      "Epoch [15], Iter [91/101] Loss: 0.5227\n",
      "Epoch [16], Iter [91/101] Loss: 0.5350\n",
      "Epoch [17], Iter [91/101] Loss: 0.5014\n",
      "Epoch [18], Iter [91/101] Loss: 0.5195\n",
      "Epoch [19], Iter [91/101] Loss: 0.5241\n",
      "Epoch [20], Iter [91/101] Loss: 0.5049\n",
      "Epoch [21], Iter [91/101] Loss: 0.4863\n",
      "Epoch [22], Iter [91/101] Loss: 0.4778\n",
      "Epoch [23], Iter [91/101] Loss: 0.4778\n",
      "Epoch [24], Iter [91/101] Loss: 0.4725\n",
      "Epoch [25], Iter [91/101] Loss: 0.4616\n",
      "Test MSE: 0.610697865486145\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 1.0122\n",
      "Epoch [2], Iter [91/101] Loss: 0.9426\n",
      "Epoch [3], Iter [91/101] Loss: 0.7473\n",
      "Epoch [4], Iter [91/101] Loss: 0.7304\n",
      "Epoch [5], Iter [91/101] Loss: 0.6470\n",
      "Epoch [6], Iter [91/101] Loss: 0.6759\n",
      "Epoch [7], Iter [91/101] Loss: 0.5946\n",
      "Epoch [8], Iter [91/101] Loss: 0.5735\n",
      "Epoch [9], Iter [91/101] Loss: 0.5330\n",
      "Epoch [10], Iter [91/101] Loss: 0.5744\n",
      "Epoch [11], Iter [91/101] Loss: 0.5418\n",
      "Epoch [12], Iter [91/101] Loss: 0.5280\n",
      "Epoch [13], Iter [91/101] Loss: 0.5236\n",
      "Epoch [14], Iter [91/101] Loss: 0.5639\n",
      "Epoch [15], Iter [91/101] Loss: 0.5291\n",
      "Epoch [16], Iter [91/101] Loss: 0.4993\n",
      "Epoch [17], Iter [91/101] Loss: 0.5124\n",
      "Epoch [18], Iter [91/101] Loss: 0.4911\n",
      "Epoch [19], Iter [91/101] Loss: 0.4914\n",
      "Epoch [20], Iter [91/101] Loss: 0.4924\n",
      "Epoch [21], Iter [91/101] Loss: 0.4730\n",
      "Epoch [22], Iter [91/101] Loss: 0.4582\n",
      "Test MSE: 0.6151790022850037\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9314\n",
      "Epoch [2], Iter [91/101] Loss: 0.8712\n",
      "Epoch [3], Iter [91/101] Loss: 0.7919\n",
      "Epoch [4], Iter [91/101] Loss: 0.7648\n",
      "Epoch [5], Iter [91/101] Loss: 0.6445\n",
      "Epoch [6], Iter [91/101] Loss: 0.6250\n",
      "Epoch [7], Iter [91/101] Loss: 0.6657\n",
      "Epoch [8], Iter [91/101] Loss: 0.6927\n",
      "Epoch [9], Iter [91/101] Loss: 0.6100\n",
      "Epoch [10], Iter [91/101] Loss: 0.5636\n",
      "Epoch [11], Iter [91/101] Loss: 0.5552\n",
      "Epoch [12], Iter [91/101] Loss: 0.5486\n",
      "Epoch [13], Iter [91/101] Loss: 0.5250\n",
      "Epoch [14], Iter [91/101] Loss: 0.5110\n",
      "Epoch [15], Iter [91/101] Loss: 0.5340\n",
      "Epoch [16], Iter [91/101] Loss: 0.5209\n",
      "Epoch [17], Iter [91/101] Loss: 0.4903\n",
      "Epoch [18], Iter [91/101] Loss: 0.5114\n",
      "Epoch [19], Iter [91/101] Loss: 0.4670\n",
      "Epoch [20], Iter [91/101] Loss: 0.4870\n",
      "Epoch [21], Iter [91/101] Loss: 0.4896\n",
      "Epoch [22], Iter [91/101] Loss: 0.4834\n",
      "Epoch [23], Iter [91/101] Loss: 0.4645\n",
      "Test MSE: 0.6182376742362976\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4 Hyperparameter tuning\n",
    "\n",
    "Tune the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.logspace(-4,-1,20)\n",
    "runs_per_lr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (104) must match the size of tensor b (96) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10880/975072019.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RUN_NR\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LEARNING_RATE\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m  \u001b[1;31m# 0.002637 # 5e-3  # use either this or default ADAM learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0munet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mpredict_save_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_training_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\train.py\u001b[0m in \u001b[0;36mtrain_unet\u001b[1;34m(dataset_description, model_training_description, base_folder, use_tensorboard)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mdataset_description\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"GRID_TYPE\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Flat\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid grid type\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\train.py\u001b[0m in \u001b[0;36mmasked_weighted_mse_loss\u001b[1;34m(output, target, masks, weights)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \"\"\"\n\u001b[0;32m    357\u001b[0m     \u001b[1;31m# print(\"output\",output.shape, \"target\",target.shape, \"weights\", weights.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (104) must match the size of tensor b (96) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    for i in range(runs_per_lr):\n",
    "        model_training_description[\"RUN_NR\"] = i\n",
    "        model_training_description[\"LEARNING_RATE\"] = lr  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "        unet = train_unet(description, model_training_description, output_folder)\n",
    "        predict_save_unet(description, model_training_description, output_folder, unet, output_folder)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet wider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 64\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (64,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet deeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "# training parameters\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 4 # this changes compared to standard UNet\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Precipitation weighting \n",
    "\n",
    "Appendix Table A3 in thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_new\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GrouPyTorch_kernel",
   "language": "python",
   "name": "groupytorch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
