{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9776fa2c",
   "metadata": {},
   "source": [
    "# Reimplement thesis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e90607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from predict import *\n",
    "from evaluate import *\n",
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62b8e1",
   "metadata": {},
   "source": [
    "## 1) Create datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047f5b9",
   "metadata": {},
   "source": [
    "### tas, pr, oro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10fe2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\",\n",
    "                                \"oro\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"oro\": [\"ht\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624fd208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512d24d",
   "metadata": {},
   "source": [
    "## Tas, pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80769317",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02346b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7a6fe",
   "metadata": {},
   "source": [
    "### tas, pr, slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "832fa64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d667a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254bb9e",
   "metadata": {},
   "source": [
    "### tas only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b0669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "154cd2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916970ee",
   "metadata": {},
   "source": [
    "### pr only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c23b7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ab0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef46d1a",
   "metadata": {},
   "source": [
    "### slp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b7e15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027ca037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cce60",
   "metadata": {},
   "source": [
    "### pr, tas precip weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3e9ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb889c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_precip_weighted_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b338477",
   "metadata": {},
   "source": [
    "### pr, tas ico grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31b74e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42125f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'Datasets\\\\iHadCM3\\\\Interpolated\\\\isotopes_yearly_r_5_nbs_6_cons1.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12040/1541587748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_yearly_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\datasets.py\u001b[0m in \u001b[0;36mcreate_yearly_dataset\u001b[1;34m(description, dataset_folder, output_folder)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading variables\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;31m# load the selected climate variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_variables_and_timesteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;31m# split the variables into predictors and targets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\datasets.py\u001b[0m in \u001b[0;36mload_variables_and_timesteps\u001b[1;34m(description, dataset_folder)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_required_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;31m# make sure that all datasets that have a non-trivial time axis share the same calendar and units.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"GRID_TYPE\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Flat\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\datasets.py\u001b[0m in \u001b[0;36mget_required_datasets\u001b[1;34m(description, dataset_folder)\u001b[0m\n\u001b[0;32m     55\u001b[0m                                      \"{}_yearly_r_{}_nbs_5_{}.nc\".format(dst, description[\"RESOLUTION\"], description[\"INTERPOLATION\"]))\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                 \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"6_nb\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetCDF4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                 \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"5_nb\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetCDF4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnetCDF4\\_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'Datasets\\\\iHadCM3\\\\Interpolated\\\\isotopes_yearly_r_5_nbs_6_cons1.nc'"
     ]
    }
   ],
   "source": [
    "create_yearly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ea85e",
   "metadata": {},
   "source": [
    "## 2) Run experiments yearly dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed8d56",
   "metadata": {},
   "source": [
    "### 4.2.1 Modifications to flat UNet\n",
    "\n",
    "Start by selecting the tas, pr dataset without precipitation weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcf2fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19f8117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbaabda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [\"Masked_MSELoss\", \"Masked_AreaWeightedMSELoss\"]\n",
    "use_coord_conv = [False, True]\n",
    "use_cylindrical_padding = [False, True]\n",
    "n_runs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28270b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked_MSELoss False False 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6628\n",
      "Epoch [2], Iter [91/101] Loss: 0.6232\n",
      "Epoch [3], Iter [91/101] Loss: 0.6144\n",
      "Epoch [4], Iter [91/101] Loss: 0.5713\n",
      "Epoch [5], Iter [91/101] Loss: 0.5599\n",
      "Epoch [6], Iter [91/101] Loss: 0.5576\n",
      "Epoch [7], Iter [91/101] Loss: 0.5326\n",
      "Epoch [8], Iter [91/101] Loss: 0.5292\n",
      "Epoch [9], Iter [91/101] Loss: 0.5106\n",
      "Epoch [10], Iter [91/101] Loss: 0.5063\n",
      "Epoch [11], Iter [91/101] Loss: 0.4941\n",
      "Epoch [12], Iter [91/101] Loss: 0.4942\n",
      "Epoch [13], Iter [91/101] Loss: 0.4634\n",
      "Test MSE: 0.697871744632721\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6137\n",
      "Epoch [2], Iter [91/101] Loss: 0.6199\n",
      "Epoch [3], Iter [91/101] Loss: 0.5870\n",
      "Epoch [4], Iter [91/101] Loss: 0.5836\n",
      "Epoch [5], Iter [91/101] Loss: 0.5889\n",
      "Epoch [6], Iter [91/101] Loss: 0.5340\n",
      "Epoch [7], Iter [91/101] Loss: 0.5508\n",
      "Epoch [8], Iter [91/101] Loss: 0.5282\n",
      "Epoch [9], Iter [91/101] Loss: 0.5195\n",
      "Epoch [10], Iter [91/101] Loss: 0.5146\n",
      "Epoch [11], Iter [91/101] Loss: 0.4684\n",
      "Epoch [12], Iter [91/101] Loss: 0.4684\n",
      "Test MSE: 0.677980363368988\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6988\n",
      "Epoch [2], Iter [91/101] Loss: 0.5759\n",
      "Epoch [3], Iter [91/101] Loss: 0.5624\n",
      "Epoch [4], Iter [91/101] Loss: 0.6034\n",
      "Epoch [5], Iter [91/101] Loss: 0.6054\n",
      "Epoch [6], Iter [91/101] Loss: 0.5710\n",
      "Epoch [7], Iter [91/101] Loss: 0.5292\n",
      "Epoch [8], Iter [91/101] Loss: 0.5307\n",
      "Epoch [9], Iter [91/101] Loss: 0.5466\n",
      "Epoch [10], Iter [91/101] Loss: 0.5170\n",
      "Epoch [11], Iter [91/101] Loss: 0.4771\n",
      "Epoch [12], Iter [91/101] Loss: 0.4658\n",
      "Epoch [13], Iter [91/101] Loss: 0.4769\n",
      "Epoch [14], Iter [91/101] Loss: 0.4508\n",
      "Test MSE: 0.6915313005447388\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False False 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6410\n",
      "Epoch [2], Iter [91/101] Loss: 0.8372\n",
      "Epoch [3], Iter [91/101] Loss: 0.6138\n",
      "Epoch [4], Iter [91/101] Loss: 0.5498\n",
      "Epoch [5], Iter [91/101] Loss: 0.5748\n",
      "Epoch [6], Iter [91/101] Loss: 0.5723\n",
      "Epoch [7], Iter [91/101] Loss: 0.5028\n",
      "Epoch [8], Iter [91/101] Loss: 0.5222\n",
      "Epoch [9], Iter [91/101] Loss: 0.5175\n",
      "Epoch [10], Iter [91/101] Loss: 0.4981\n",
      "Epoch [11], Iter [91/101] Loss: 0.4803\n",
      "Epoch [12], Iter [91/101] Loss: 0.5011\n",
      "Epoch [13], Iter [91/101] Loss: 0.4772\n",
      "Test MSE: 0.667206883430481\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6549\n",
      "Epoch [2], Iter [91/101] Loss: 0.6087\n",
      "Epoch [3], Iter [91/101] Loss: 0.5912\n",
      "Epoch [4], Iter [91/101] Loss: 0.5849\n",
      "Epoch [5], Iter [91/101] Loss: 0.6048\n",
      "Epoch [6], Iter [91/101] Loss: 0.5548\n",
      "Epoch [7], Iter [91/101] Loss: 0.5415\n",
      "Epoch [8], Iter [91/101] Loss: 0.5228\n",
      "Epoch [9], Iter [91/101] Loss: 0.5433\n",
      "Epoch [10], Iter [91/101] Loss: 0.5225\n",
      "Epoch [11], Iter [91/101] Loss: 0.4877\n",
      "Epoch [12], Iter [91/101] Loss: 0.4623\n",
      "Epoch [13], Iter [91/101] Loss: 0.4827\n",
      "Epoch [14], Iter [91/101] Loss: 0.4742\n",
      "Epoch [15], Iter [91/101] Loss: 0.4440\n",
      "Epoch [16], Iter [91/101] Loss: 0.4510\n",
      "Epoch [17], Iter [91/101] Loss: 0.4414\n",
      "Epoch [18], Iter [91/101] Loss: 0.4357\n",
      "Test MSE: 0.6727862358093262\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7048\n",
      "Epoch [2], Iter [91/101] Loss: 0.6230\n",
      "Epoch [3], Iter [91/101] Loss: 0.6172\n",
      "Epoch [4], Iter [91/101] Loss: 0.5673\n",
      "Epoch [5], Iter [91/101] Loss: 0.5805\n",
      "Epoch [6], Iter [91/101] Loss: 0.5434\n",
      "Epoch [7], Iter [91/101] Loss: 0.5492\n",
      "Epoch [8], Iter [91/101] Loss: 0.5189\n",
      "Epoch [9], Iter [91/101] Loss: 0.5346\n",
      "Epoch [10], Iter [91/101] Loss: 0.5371\n",
      "Epoch [11], Iter [91/101] Loss: 0.5151\n",
      "Epoch [12], Iter [91/101] Loss: 0.4842\n",
      "Epoch [13], Iter [91/101] Loss: 0.4742\n",
      "Epoch [14], Iter [91/101] Loss: 0.4983\n",
      "Epoch [15], Iter [91/101] Loss: 0.5200\n",
      "Epoch [16], Iter [91/101] Loss: 0.4644\n",
      "Epoch [17], Iter [91/101] Loss: 0.4452\n",
      "Epoch [18], Iter [91/101] Loss: 0.4419\n",
      "Test MSE: 0.6913966536521912\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6926\n",
      "Epoch [2], Iter [91/101] Loss: 0.5964\n",
      "Epoch [3], Iter [91/101] Loss: 0.5889\n",
      "Epoch [4], Iter [91/101] Loss: 0.6003\n",
      "Epoch [5], Iter [91/101] Loss: 0.5845\n",
      "Epoch [6], Iter [91/101] Loss: 0.5694\n",
      "Epoch [7], Iter [91/101] Loss: 0.5691\n",
      "Epoch [8], Iter [91/101] Loss: 0.5445\n",
      "Epoch [9], Iter [91/101] Loss: 0.5238\n",
      "Epoch [10], Iter [91/101] Loss: 0.5160\n",
      "Epoch [11], Iter [91/101] Loss: 0.5050\n",
      "Epoch [12], Iter [91/101] Loss: 0.5065\n",
      "Epoch [13], Iter [91/101] Loss: 0.4955\n",
      "Epoch [14], Iter [91/101] Loss: 0.4622\n",
      "Epoch [15], Iter [91/101] Loss: 0.4658\n",
      "Epoch [16], Iter [91/101] Loss: 0.4647\n",
      "Epoch [17], Iter [91/101] Loss: 0.4748\n",
      "Epoch [18], Iter [91/101] Loss: 0.4413\n",
      "Epoch [19], Iter [91/101] Loss: 0.4440\n",
      "Test MSE: 0.8481761813163757\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss False True 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6695\n",
      "Epoch [2], Iter [91/101] Loss: 0.6409\n",
      "Epoch [3], Iter [91/101] Loss: 0.6286\n",
      "Epoch [4], Iter [91/101] Loss: 0.5905\n",
      "Epoch [5], Iter [91/101] Loss: 0.5644\n",
      "Epoch [6], Iter [91/101] Loss: 0.5415\n",
      "Epoch [7], Iter [91/101] Loss: 0.5603\n",
      "Epoch [8], Iter [91/101] Loss: 0.5400\n",
      "Epoch [9], Iter [91/101] Loss: 0.5088\n",
      "Epoch [10], Iter [91/101] Loss: 0.5012\n",
      "Epoch [11], Iter [91/101] Loss: 0.5066\n",
      "Epoch [12], Iter [91/101] Loss: 0.4884\n",
      "Epoch [13], Iter [91/101] Loss: 0.4854\n",
      "Epoch [14], Iter [91/101] Loss: 0.5525\n",
      "Epoch [15], Iter [91/101] Loss: 0.4701\n",
      "Test MSE: 0.6870294213294983\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6372\n",
      "Epoch [2], Iter [91/101] Loss: 0.6294\n",
      "Epoch [3], Iter [91/101] Loss: 0.6226\n",
      "Epoch [4], Iter [91/101] Loss: 0.5851\n",
      "Epoch [5], Iter [91/101] Loss: 0.6022\n",
      "Epoch [6], Iter [91/101] Loss: 0.5944\n",
      "Epoch [7], Iter [91/101] Loss: 0.5631\n",
      "Epoch [8], Iter [91/101] Loss: 0.5576\n",
      "Epoch [9], Iter [91/101] Loss: 0.5429\n",
      "Epoch [10], Iter [91/101] Loss: 0.5572\n",
      "Epoch [11], Iter [91/101] Loss: 0.5283\n",
      "Epoch [12], Iter [91/101] Loss: 0.5182\n",
      "Epoch [13], Iter [91/101] Loss: 0.5221\n",
      "Epoch [14], Iter [91/101] Loss: 0.4903\n",
      "Epoch [15], Iter [91/101] Loss: 0.5008\n",
      "Epoch [16], Iter [91/101] Loss: 0.4684\n",
      "Epoch [17], Iter [91/101] Loss: 0.4928\n",
      "Epoch [18], Iter [91/101] Loss: 0.4816\n",
      "Epoch [19], Iter [91/101] Loss: 0.4758\n",
      "Epoch [20], Iter [91/101] Loss: 0.5214\n",
      "Test MSE: 0.6626791954040527\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6465\n",
      "Epoch [2], Iter [91/101] Loss: 0.6760\n",
      "Epoch [3], Iter [91/101] Loss: 0.6242\n",
      "Epoch [4], Iter [91/101] Loss: 0.5880\n",
      "Epoch [5], Iter [91/101] Loss: 0.6028\n",
      "Epoch [6], Iter [91/101] Loss: 0.5838\n",
      "Epoch [7], Iter [91/101] Loss: 0.5562\n",
      "Epoch [8], Iter [91/101] Loss: 0.5762\n",
      "Epoch [9], Iter [91/101] Loss: 0.5930\n",
      "Epoch [10], Iter [91/101] Loss: 0.5629\n",
      "Epoch [11], Iter [91/101] Loss: 0.5458\n",
      "Epoch [12], Iter [91/101] Loss: 0.5289\n",
      "Epoch [13], Iter [91/101] Loss: 0.5076\n",
      "Epoch [14], Iter [91/101] Loss: 0.5149\n",
      "Epoch [15], Iter [91/101] Loss: 0.5023\n",
      "Epoch [16], Iter [91/101] Loss: 0.4845\n",
      "Epoch [17], Iter [91/101] Loss: 0.4735\n",
      "Epoch [18], Iter [91/101] Loss: 0.5306\n",
      "Epoch [19], Iter [91/101] Loss: 0.4766\n",
      "Epoch [20], Iter [91/101] Loss: 0.4692\n",
      "Epoch [21], Iter [91/101] Loss: 0.4475\n",
      "Epoch [22], Iter [91/101] Loss: 0.4783\n",
      "Test MSE: 0.7246212363243103\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6810\n",
      "Epoch [2], Iter [91/101] Loss: 0.6882\n",
      "Epoch [3], Iter [91/101] Loss: 0.6078\n",
      "Epoch [4], Iter [91/101] Loss: 0.5598\n",
      "Epoch [5], Iter [91/101] Loss: 0.5846\n",
      "Epoch [6], Iter [91/101] Loss: 0.6068\n",
      "Epoch [7], Iter [91/101] Loss: 1.1814\n",
      "Epoch [8], Iter [91/101] Loss: 0.5209\n",
      "Epoch [9], Iter [91/101] Loss: 0.5332\n",
      "Epoch [10], Iter [91/101] Loss: 0.5087\n",
      "Epoch [11], Iter [91/101] Loss: 0.5204\n",
      "Test MSE: 0.670082688331604\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True False 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6452\n",
      "Epoch [2], Iter [91/101] Loss: 0.6186\n",
      "Epoch [3], Iter [91/101] Loss: 0.6466\n",
      "Epoch [4], Iter [91/101] Loss: 0.6104\n",
      "Epoch [5], Iter [91/101] Loss: 0.5786\n",
      "Epoch [6], Iter [91/101] Loss: 0.5609\n",
      "Epoch [7], Iter [91/101] Loss: 0.5726\n",
      "Epoch [8], Iter [91/101] Loss: 0.5495\n",
      "Epoch [9], Iter [91/101] Loss: 0.5412\n",
      "Epoch [10], Iter [91/101] Loss: 0.5591\n",
      "Epoch [11], Iter [91/101] Loss: 0.5250\n",
      "Epoch [12], Iter [91/101] Loss: 0.4907\n",
      "Epoch [13], Iter [91/101] Loss: 0.5047\n",
      "Epoch [14], Iter [91/101] Loss: 0.4928\n",
      "Epoch [15], Iter [91/101] Loss: 0.4813\n",
      "Epoch [16], Iter [91/101] Loss: 0.5037\n",
      "Epoch [17], Iter [91/101] Loss: 0.4687\n",
      "Test MSE: 0.6261389851570129\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8396\n",
      "Epoch [2], Iter [91/101] Loss: 0.6128\n",
      "Epoch [3], Iter [91/101] Loss: 0.6081\n",
      "Epoch [4], Iter [91/101] Loss: 0.5997\n",
      "Epoch [5], Iter [91/101] Loss: 0.5600\n",
      "Epoch [6], Iter [91/101] Loss: 0.6106\n",
      "Epoch [7], Iter [91/101] Loss: 0.5675\n",
      "Epoch [8], Iter [91/101] Loss: 0.5320\n",
      "Epoch [9], Iter [91/101] Loss: 0.5362\n",
      "Epoch [10], Iter [91/101] Loss: 0.5430\n",
      "Epoch [11], Iter [91/101] Loss: 0.5504\n",
      "Epoch [12], Iter [91/101] Loss: 0.5150\n",
      "Epoch [13], Iter [91/101] Loss: 0.5247\n",
      "Epoch [14], Iter [91/101] Loss: 0.5036\n",
      "Epoch [15], Iter [91/101] Loss: 0.5155\n",
      "Epoch [16], Iter [91/101] Loss: 0.5032\n",
      "Epoch [17], Iter [91/101] Loss: 0.4951\n",
      "Epoch [18], Iter [91/101] Loss: 0.5218\n",
      "Epoch [19], Iter [91/101] Loss: 0.4834\n",
      "Epoch [20], Iter [91/101] Loss: 0.4641\n",
      "Epoch [21], Iter [91/101] Loss: 0.4674\n",
      "Test MSE: 0.6917014718055725\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6497\n",
      "Epoch [2], Iter [91/101] Loss: 0.6512\n",
      "Epoch [3], Iter [91/101] Loss: 0.6191\n",
      "Epoch [4], Iter [91/101] Loss: 0.5768\n",
      "Epoch [5], Iter [91/101] Loss: 0.5682\n",
      "Epoch [6], Iter [91/101] Loss: 0.5728\n",
      "Epoch [7], Iter [91/101] Loss: 0.5580\n",
      "Epoch [8], Iter [91/101] Loss: 0.6350\n",
      "Epoch [9], Iter [91/101] Loss: 0.5594\n",
      "Epoch [10], Iter [91/101] Loss: 0.5530\n",
      "Epoch [11], Iter [91/101] Loss: 0.5534\n",
      "Epoch [12], Iter [91/101] Loss: 0.5470\n",
      "Epoch [13], Iter [91/101] Loss: 0.5345\n",
      "Epoch [14], Iter [91/101] Loss: 0.5520\n",
      "Epoch [15], Iter [91/101] Loss: 0.5289\n",
      "Test MSE: 0.6497945189476013\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6668\n",
      "Epoch [2], Iter [91/101] Loss: 0.6235\n",
      "Epoch [3], Iter [91/101] Loss: 0.6519\n",
      "Epoch [4], Iter [91/101] Loss: 0.6115\n",
      "Epoch [5], Iter [91/101] Loss: 0.6004\n",
      "Epoch [6], Iter [91/101] Loss: 0.6139\n",
      "Epoch [7], Iter [91/101] Loss: 0.5709\n",
      "Epoch [8], Iter [91/101] Loss: 0.5914\n",
      "Epoch [9], Iter [91/101] Loss: 0.5785\n",
      "Epoch [10], Iter [91/101] Loss: 0.5337\n",
      "Epoch [11], Iter [91/101] Loss: 0.5500\n",
      "Epoch [12], Iter [91/101] Loss: 0.5171\n",
      "Epoch [13], Iter [91/101] Loss: 0.5417\n",
      "Epoch [14], Iter [91/101] Loss: 0.5254\n",
      "Epoch [15], Iter [91/101] Loss: 0.5136\n",
      "Epoch [16], Iter [91/101] Loss: 0.4932\n",
      "Epoch [17], Iter [91/101] Loss: 0.4941\n",
      "Epoch [18], Iter [91/101] Loss: 0.5183\n",
      "Epoch [19], Iter [91/101] Loss: 0.4979\n",
      "Test MSE: 0.6641145348548889\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_MSELoss True True 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6545\n",
      "Epoch [2], Iter [91/101] Loss: 0.6328\n",
      "Epoch [3], Iter [91/101] Loss: 0.6252\n",
      "Epoch [4], Iter [91/101] Loss: 0.6128\n",
      "Epoch [5], Iter [91/101] Loss: 0.5905\n",
      "Epoch [6], Iter [91/101] Loss: 0.5560\n",
      "Epoch [7], Iter [91/101] Loss: 0.5825\n",
      "Epoch [8], Iter [91/101] Loss: 0.5650\n",
      "Epoch [9], Iter [91/101] Loss: 0.5562\n",
      "Epoch [10], Iter [91/101] Loss: 0.5524\n",
      "Epoch [11], Iter [91/101] Loss: 0.5332\n",
      "Epoch [12], Iter [91/101] Loss: 0.5144\n",
      "Epoch [13], Iter [91/101] Loss: 0.5311\n",
      "Epoch [14], Iter [91/101] Loss: 0.5464\n",
      "Epoch [15], Iter [91/101] Loss: 0.5164\n",
      "Epoch [16], Iter [91/101] Loss: 0.5691\n",
      "Epoch [17], Iter [91/101] Loss: 0.4768\n",
      "Epoch [18], Iter [91/101] Loss: 0.5015\n",
      "Epoch [19], Iter [91/101] Loss: 1.0360\n",
      "Epoch [20], Iter [91/101] Loss: 0.4790\n",
      "Epoch [21], Iter [91/101] Loss: 0.4630\n",
      "Test MSE: 0.7128632068634033\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6475\n",
      "Epoch [2], Iter [91/101] Loss: 0.6283\n",
      "Epoch [3], Iter [91/101] Loss: 0.6149\n",
      "Epoch [4], Iter [91/101] Loss: 0.6104\n",
      "Epoch [5], Iter [91/101] Loss: 0.5785\n",
      "Epoch [6], Iter [91/101] Loss: 0.5129\n",
      "Epoch [7], Iter [91/101] Loss: 0.5281\n",
      "Epoch [8], Iter [91/101] Loss: 0.5216\n",
      "Epoch [9], Iter [91/101] Loss: 0.4850\n",
      "Epoch [10], Iter [91/101] Loss: 0.4953\n",
      "Epoch [11], Iter [91/101] Loss: 0.4778\n",
      "Epoch [12], Iter [91/101] Loss: 0.4907\n",
      "Epoch [13], Iter [91/101] Loss: 0.4527\n",
      "Epoch [14], Iter [91/101] Loss: 0.4764\n",
      "Epoch [15], Iter [91/101] Loss: 0.4347\n",
      "Epoch [16], Iter [91/101] Loss: 0.4495\n",
      "Test MSE: 0.6760199069976807\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6500\n",
      "Epoch [2], Iter [91/101] Loss: 0.6170\n",
      "Epoch [3], Iter [91/101] Loss: 0.5833\n",
      "Epoch [4], Iter [91/101] Loss: 0.5747\n",
      "Epoch [5], Iter [91/101] Loss: 0.5556\n",
      "Epoch [6], Iter [91/101] Loss: 0.5445\n",
      "Epoch [7], Iter [91/101] Loss: 0.5540\n",
      "Epoch [8], Iter [91/101] Loss: 0.5351\n",
      "Epoch [9], Iter [91/101] Loss: 0.5210\n",
      "Epoch [10], Iter [91/101] Loss: 0.4907\n",
      "Epoch [11], Iter [91/101] Loss: 0.4980\n",
      "Epoch [12], Iter [91/101] Loss: 0.4850\n",
      "Epoch [13], Iter [91/101] Loss: 0.4609\n",
      "Epoch [14], Iter [91/101] Loss: 0.4379\n",
      "Epoch [15], Iter [91/101] Loss: 0.4558\n",
      "Epoch [16], Iter [91/101] Loss: 0.4416\n",
      "Epoch [17], Iter [91/101] Loss: 0.4052\n",
      "Epoch [18], Iter [91/101] Loss: 0.9242\n",
      "Test MSE: 0.6516020894050598\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6381\n",
      "Epoch [2], Iter [91/101] Loss: 0.6315\n",
      "Epoch [3], Iter [91/101] Loss: 0.7111\n",
      "Epoch [4], Iter [91/101] Loss: 0.5296\n",
      "Epoch [5], Iter [91/101] Loss: 0.5746\n",
      "Epoch [6], Iter [91/101] Loss: 0.5522\n",
      "Epoch [7], Iter [91/101] Loss: 0.5533\n",
      "Epoch [8], Iter [91/101] Loss: 0.5108\n",
      "Epoch [9], Iter [91/101] Loss: 0.5655\n",
      "Epoch [10], Iter [91/101] Loss: 0.5009\n",
      "Epoch [11], Iter [91/101] Loss: 0.4804\n",
      "Epoch [12], Iter [91/101] Loss: 0.4927\n",
      "Epoch [13], Iter [91/101] Loss: 0.4656\n",
      "Epoch [14], Iter [91/101] Loss: 0.4492\n",
      "Test MSE: 0.6611409187316895\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False False 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6696\n",
      "Epoch [2], Iter [91/101] Loss: 0.6290\n",
      "Epoch [3], Iter [91/101] Loss: 0.5910\n",
      "Epoch [4], Iter [91/101] Loss: 0.5567\n",
      "Epoch [5], Iter [91/101] Loss: 0.5533\n",
      "Epoch [6], Iter [91/101] Loss: 0.5716\n",
      "Epoch [7], Iter [91/101] Loss: 0.5322\n",
      "Epoch [8], Iter [91/101] Loss: 0.5052\n",
      "Epoch [9], Iter [91/101] Loss: 0.4964\n",
      "Epoch [10], Iter [91/101] Loss: 0.4863\n",
      "Epoch [11], Iter [91/101] Loss: 0.4845\n",
      "Epoch [12], Iter [91/101] Loss: 0.5568\n",
      "Epoch [13], Iter [91/101] Loss: 0.4743\n",
      "Epoch [14], Iter [91/101] Loss: 0.4388\n",
      "Test MSE: 0.6659254431724548\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False True 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7225\n",
      "Epoch [2], Iter [91/101] Loss: 0.6909\n",
      "Epoch [3], Iter [91/101] Loss: 0.7288\n",
      "Epoch [4], Iter [91/101] Loss: 0.5933\n",
      "Epoch [5], Iter [91/101] Loss: 0.5595\n",
      "Epoch [6], Iter [91/101] Loss: 0.5500\n",
      "Epoch [7], Iter [91/101] Loss: 0.5185\n",
      "Epoch [8], Iter [91/101] Loss: 0.6101\n",
      "Epoch [9], Iter [91/101] Loss: 0.4990\n",
      "Epoch [10], Iter [91/101] Loss: 0.5118\n",
      "Epoch [11], Iter [91/101] Loss: 0.4877\n",
      "Epoch [12], Iter [91/101] Loss: 0.4966\n",
      "Epoch [13], Iter [91/101] Loss: 0.4681\n",
      "Epoch [14], Iter [91/101] Loss: 0.4586\n",
      "Epoch [15], Iter [91/101] Loss: 0.4608\n",
      "Test MSE: 0.6440669298171997\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False True 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6616\n",
      "Epoch [2], Iter [91/101] Loss: 0.6138\n",
      "Epoch [3], Iter [91/101] Loss: 0.5647\n",
      "Epoch [4], Iter [91/101] Loss: 0.5440\n",
      "Epoch [5], Iter [91/101] Loss: 0.5591\n",
      "Epoch [6], Iter [91/101] Loss: 0.5328\n",
      "Epoch [7], Iter [91/101] Loss: 0.5278\n",
      "Epoch [8], Iter [91/101] Loss: 0.5974\n",
      "Epoch [9], Iter [91/101] Loss: 0.5034\n",
      "Epoch [10], Iter [91/101] Loss: 0.5200\n",
      "Epoch [11], Iter [91/101] Loss: 0.4958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Iter [91/101] Loss: 0.4875\n",
      "Epoch [13], Iter [91/101] Loss: 0.4809\n",
      "Epoch [14], Iter [91/101] Loss: 0.5074\n",
      "Test MSE: 0.8342309594154358\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False True 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6379\n",
      "Epoch [2], Iter [91/101] Loss: 0.6285\n",
      "Epoch [3], Iter [91/101] Loss: 0.6275\n",
      "Epoch [4], Iter [91/101] Loss: 0.5652\n",
      "Epoch [5], Iter [91/101] Loss: 0.5689\n",
      "Epoch [6], Iter [91/101] Loss: 0.5494\n",
      "Epoch [7], Iter [91/101] Loss: 0.5351\n",
      "Epoch [8], Iter [91/101] Loss: 0.5220\n",
      "Epoch [9], Iter [91/101] Loss: 0.5082\n",
      "Epoch [10], Iter [91/101] Loss: 0.4947\n",
      "Epoch [11], Iter [91/101] Loss: 0.4833\n",
      "Epoch [12], Iter [91/101] Loss: 0.4848\n",
      "Epoch [13], Iter [91/101] Loss: 0.4813\n",
      "Epoch [14], Iter [91/101] Loss: 0.4425\n",
      "Epoch [15], Iter [91/101] Loss: 0.4674\n",
      "Epoch [16], Iter [91/101] Loss: 0.4634\n",
      "Test MSE: 0.6621280312538147\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss False True 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6457\n",
      "Epoch [2], Iter [91/101] Loss: 0.6351\n",
      "Epoch [3], Iter [91/101] Loss: 0.6031\n",
      "Epoch [4], Iter [91/101] Loss: 0.5872\n",
      "Epoch [5], Iter [91/101] Loss: 0.5391\n",
      "Epoch [6], Iter [91/101] Loss: 0.5503\n",
      "Epoch [7], Iter [91/101] Loss: 0.5265\n",
      "Epoch [8], Iter [91/101] Loss: 0.5225\n",
      "Epoch [9], Iter [91/101] Loss: 0.5224\n",
      "Epoch [10], Iter [91/101] Loss: 0.5150\n",
      "Epoch [11], Iter [91/101] Loss: 0.4808\n",
      "Epoch [12], Iter [91/101] Loss: 0.5093\n",
      "Epoch [13], Iter [91/101] Loss: 0.4923\n",
      "Epoch [14], Iter [91/101] Loss: 0.4615\n",
      "Epoch [15], Iter [91/101] Loss: 0.4452\n",
      "Epoch [16], Iter [91/101] Loss: 0.4469\n",
      "Epoch [17], Iter [91/101] Loss: 0.4338\n",
      "Epoch [18], Iter [91/101] Loss: 0.4197\n",
      "Epoch [19], Iter [91/101] Loss: 0.4078\n",
      "Test MSE: 0.8009379506111145\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss True False 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7055\n",
      "Epoch [2], Iter [91/101] Loss: 0.6582\n",
      "Epoch [3], Iter [91/101] Loss: 0.5856\n",
      "Epoch [4], Iter [91/101] Loss: 0.6126\n",
      "Epoch [5], Iter [91/101] Loss: 0.5970\n",
      "Epoch [6], Iter [91/101] Loss: 1.2273\n",
      "Epoch [7], Iter [91/101] Loss: 0.5694\n",
      "Epoch [8], Iter [91/101] Loss: 0.5503\n",
      "Epoch [9], Iter [91/101] Loss: 0.5395\n",
      "Epoch [10], Iter [91/101] Loss: 0.5461\n",
      "Epoch [11], Iter [91/101] Loss: 0.5124\n",
      "Epoch [12], Iter [91/101] Loss: 0.5222\n",
      "Epoch [13], Iter [91/101] Loss: 0.5117\n",
      "Epoch [14], Iter [91/101] Loss: 0.5177\n",
      "Epoch [15], Iter [91/101] Loss: 0.5009\n",
      "Epoch [16], Iter [91/101] Loss: 0.4735\n",
      "Test MSE: 0.6252280473709106\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss True False 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6523\n",
      "Epoch [2], Iter [91/101] Loss: 0.6126\n",
      "Epoch [3], Iter [91/101] Loss: 0.6047\n",
      "Epoch [4], Iter [91/101] Loss: 0.5947\n",
      "Epoch [5], Iter [91/101] Loss: 0.5377\n",
      "Epoch [6], Iter [91/101] Loss: 0.5431\n",
      "Epoch [7], Iter [91/101] Loss: 0.5292\n",
      "Epoch [8], Iter [91/101] Loss: 0.5582\n",
      "Epoch [9], Iter [91/101] Loss: 0.5317\n",
      "Epoch [10], Iter [91/101] Loss: 0.5460\n",
      "Epoch [11], Iter [91/101] Loss: 0.5422\n",
      "Epoch [12], Iter [91/101] Loss: 0.4780\n",
      "Epoch [13], Iter [91/101] Loss: 1.1081\n",
      "Epoch [14], Iter [91/101] Loss: 0.4698\n",
      "Epoch [15], Iter [91/101] Loss: 0.5100\n",
      "Epoch [16], Iter [91/101] Loss: 0.5446\n",
      "Epoch [17], Iter [91/101] Loss: 0.5093\n",
      "Epoch [18], Iter [91/101] Loss: 0.4738\n",
      "Test MSE: 0.6606146097183228\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss True False 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6259\n",
      "Epoch [2], Iter [91/101] Loss: 0.6078\n",
      "Epoch [3], Iter [91/101] Loss: 0.6117\n",
      "Epoch [4], Iter [91/101] Loss: 0.5684\n",
      "Epoch [5], Iter [91/101] Loss: 0.5763\n",
      "Epoch [6], Iter [91/101] Loss: 0.5728\n",
      "Epoch [7], Iter [91/101] Loss: 0.5503\n",
      "Epoch [8], Iter [91/101] Loss: 0.5534\n",
      "Epoch [9], Iter [91/101] Loss: 0.5417\n",
      "Epoch [10], Iter [91/101] Loss: 0.5336\n",
      "Epoch [11], Iter [91/101] Loss: 0.5193\n",
      "Epoch [12], Iter [91/101] Loss: 0.4944\n",
      "Epoch [13], Iter [91/101] Loss: 0.4941\n",
      "Epoch [14], Iter [91/101] Loss: 0.5059\n",
      "Epoch [15], Iter [91/101] Loss: 0.4901\n",
      "Epoch [16], Iter [91/101] Loss: 0.4709\n",
      "Test MSE: 0.6901370882987976\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss True False 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6069\n",
      "Epoch [2], Iter [91/101] Loss: 0.6088\n",
      "Epoch [3], Iter [91/101] Loss: 0.6795\n",
      "Epoch [4], Iter [91/101] Loss: 0.6053\n",
      "Epoch [5], Iter [91/101] Loss: 0.5644\n",
      "Epoch [6], Iter [91/101] Loss: 0.5749\n",
      "Epoch [7], Iter [91/101] Loss: 0.5441\n",
      "Epoch [8], Iter [91/101] Loss: 0.5400\n",
      "Epoch [9], Iter [91/101] Loss: 0.5325\n",
      "Epoch [10], Iter [91/101] Loss: 0.5400\n",
      "Epoch [11], Iter [91/101] Loss: 0.4993\n",
      "Epoch [12], Iter [91/101] Loss: 0.5081\n",
      "Epoch [13], Iter [91/101] Loss: 0.5051\n",
      "Epoch [14], Iter [91/101] Loss: 0.4902\n",
      "Epoch [15], Iter [91/101] Loss: 0.4707\n",
      "Epoch [16], Iter [91/101] Loss: 0.5009\n",
      "Epoch [17], Iter [91/101] Loss: 0.4722\n",
      "Epoch [18], Iter [91/101] Loss: 0.4674\n",
      "Epoch [19], Iter [91/101] Loss: 0.4684\n",
      "Epoch [20], Iter [91/101] Loss: 0.4381\n",
      "Test MSE: 0.6500015258789062\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss True True 0\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7105\n",
      "Epoch [2], Iter [91/101] Loss: 0.6532\n",
      "Epoch [3], Iter [91/101] Loss: 0.5809\n",
      "Epoch [4], Iter [91/101] Loss: 0.5969\n",
      "Epoch [5], Iter [91/101] Loss: 0.5989\n",
      "Epoch [6], Iter [91/101] Loss: 0.5601\n",
      "Epoch [7], Iter [91/101] Loss: 0.5500\n",
      "Epoch [8], Iter [91/101] Loss: 0.7080\n",
      "Epoch [9], Iter [91/101] Loss: 0.5854\n",
      "Epoch [10], Iter [91/101] Loss: 0.5470\n",
      "Epoch [11], Iter [91/101] Loss: 0.5319\n",
      "Epoch [12], Iter [91/101] Loss: 0.5047\n",
      "Epoch [13], Iter [91/101] Loss: 0.5413\n",
      "Epoch [14], Iter [91/101] Loss: 0.5275\n",
      "Epoch [15], Iter [91/101] Loss: 0.5189\n",
      "Epoch [16], Iter [91/101] Loss: 0.4925\n",
      "Epoch [17], Iter [91/101] Loss: 0.4797\n",
      "Epoch [18], Iter [91/101] Loss: 0.4756\n",
      "Epoch [19], Iter [91/101] Loss: 0.4906\n",
      "Epoch [20], Iter [91/101] Loss: 0.4716\n",
      "Epoch [21], Iter [91/101] Loss: 0.4950\n",
      "Epoch [22], Iter [91/101] Loss: 0.4859\n",
      "Epoch [23], Iter [91/101] Loss: 0.4797\n",
      "Epoch [24], Iter [91/101] Loss: 0.4557\n",
      "Epoch [25], Iter [91/101] Loss: 0.4535\n",
      "Test MSE: 0.6540716886520386\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss True True 1\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6602\n",
      "Epoch [2], Iter [91/101] Loss: 0.6390\n",
      "Epoch [3], Iter [91/101] Loss: 0.5630\n",
      "Epoch [4], Iter [91/101] Loss: 0.5685\n",
      "Epoch [5], Iter [91/101] Loss: 0.5804\n",
      "Epoch [6], Iter [91/101] Loss: 0.5477\n",
      "Epoch [7], Iter [91/101] Loss: 0.5590\n",
      "Epoch [8], Iter [91/101] Loss: 0.5398\n",
      "Epoch [9], Iter [91/101] Loss: 0.5179\n",
      "Epoch [10], Iter [91/101] Loss: 0.5361\n",
      "Epoch [11], Iter [91/101] Loss: 0.5177\n",
      "Epoch [12], Iter [91/101] Loss: 0.5172\n",
      "Epoch [13], Iter [91/101] Loss: 0.5152\n",
      "Epoch [14], Iter [91/101] Loss: 0.4862\n",
      "Epoch [15], Iter [91/101] Loss: 0.5003\n",
      "Epoch [16], Iter [91/101] Loss: 0.4863\n",
      "Epoch [17], Iter [91/101] Loss: 0.4851\n",
      "Test MSE: 0.6644803285598755\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss True True 2\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6757\n",
      "Epoch [2], Iter [91/101] Loss: 0.6474\n",
      "Epoch [3], Iter [91/101] Loss: 0.6074\n",
      "Epoch [4], Iter [91/101] Loss: 0.5750\n",
      "Epoch [5], Iter [91/101] Loss: 0.6063\n",
      "Epoch [6], Iter [91/101] Loss: 0.5650\n",
      "Epoch [7], Iter [91/101] Loss: 0.5780\n",
      "Epoch [8], Iter [91/101] Loss: 0.5386\n",
      "Epoch [9], Iter [91/101] Loss: 0.5488\n",
      "Epoch [10], Iter [91/101] Loss: 0.5511\n",
      "Epoch [11], Iter [91/101] Loss: 0.5211\n",
      "Epoch [12], Iter [91/101] Loss: 0.5324\n",
      "Epoch [13], Iter [91/101] Loss: 0.5158\n",
      "Epoch [14], Iter [91/101] Loss: 0.5370\n",
      "Epoch [15], Iter [91/101] Loss: 0.4930\n",
      "Epoch [16], Iter [91/101] Loss: 0.5046\n",
      "Epoch [17], Iter [91/101] Loss: 0.4754\n",
      "Test MSE: 0.6460735201835632\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Masked_AreaWeightedMSELoss True True 3\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6482\n",
      "Epoch [2], Iter [91/101] Loss: 0.6396\n",
      "Epoch [3], Iter [91/101] Loss: 0.5826\n",
      "Epoch [4], Iter [91/101] Loss: 0.6260\n",
      "Epoch [5], Iter [91/101] Loss: 0.5905\n",
      "Epoch [6], Iter [91/101] Loss: 0.5729\n",
      "Epoch [7], Iter [91/101] Loss: 0.5606\n",
      "Epoch [8], Iter [91/101] Loss: 0.5839\n",
      "Epoch [9], Iter [91/101] Loss: 0.5450\n",
      "Epoch [10], Iter [91/101] Loss: 0.5158\n",
      "Epoch [11], Iter [91/101] Loss: 0.5449\n",
      "Epoch [12], Iter [91/101] Loss: 0.5436\n",
      "Epoch [13], Iter [91/101] Loss: 0.5012\n",
      "Epoch [14], Iter [91/101] Loss: 0.5146\n",
      "Epoch [15], Iter [91/101] Loss: 0.4939\n",
      "Epoch [16], Iter [91/101] Loss: 0.4852\n",
      "Test MSE: 0.6315173506736755\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for l in loss:\n",
    "    for c_conv in use_coord_conv:\n",
    "        for c_pad in use_cylindrical_padding:\n",
    "            for i in range(n_runs):\n",
    "                print(l, c_conv, c_pad, i)\n",
    "                model_training_description[\"USE_CYLINDRICAL_PADDING\"] = c_pad\n",
    "                model_training_description[\"USE_COORD_CONV\"] = c_conv\n",
    "                model_training_description[\"LOSS\"] = l  # \"MSELoss\" # \"AreaWeightedMSELoss\"\n",
    "                model_training_description[\"RUN_NR\"] = i\n",
    "                unet = train_unet(description, model_training_description, output_folder)\n",
    "                predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e757923",
   "metadata": {},
   "source": [
    "### 4.2.2 Comparing results for different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf587d",
   "metadata": {},
   "source": [
    "We will need to implement the interpolation before we can reproduce the whole table. Until then: Only compare ico architectures on ico grid and flat architectures on flat grid.\n",
    "\n",
    "Results for modified and unmodified flat UNet are already obtained in last cell.(4.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad454efa",
   "metadata": {},
   "source": [
    "### Flat grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974f0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eef2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "model_training_description[\"N_PC_PREDICTORS\"] = 450\n",
    "model_training_description[\"N_PC_TARGETS\"] = 300\n",
    "model_training_description[\"REGTYPE\"] = \"lasso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f45df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pca, pca_targets, model = train_pca(description, model_training_description, output_folder)\n",
    "print(\"finished training\")\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445d9ae",
   "metadata": {},
   "source": [
    "### New linreg baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab67592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"LinReg_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3705d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "models = train_linreg_pixelwise(description, model_training_description, output_folder)\n",
    "predict_save_linreg_pixelwise(description, model_training_description, output_folder, models, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5010b",
   "metadata": {},
   "source": [
    "### New PCA-reg baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29a5689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1a00b",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning, compute 50x50 logarithmic grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a934508",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 50)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62ba9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results: N_PC_IN: 230 N_PC_OUT: 184, R2_mean, validationset: [0.17195202581952]81952]]]]\n",
      "Retrain including validation set.\n",
      "Result on test set: [0.1212945783438279]\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89376fae",
   "metadata": {},
   "source": [
    "### New Random-forest baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069fbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"RandomForest_Pixelwise\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = False\n",
    "# model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e193661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  3.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 19.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed: 20.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "model = train_random_forest_pixelwise(description, model_training_description, output_folder, verbose=3, n_jobs=-1)\n",
    "predict_save_randomforest_pixelwise(description, model_training_description, output_folder, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc721a",
   "metadata": {},
   "source": [
    "### Icosahedral grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e59925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Ico\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"RESOLUTION\"] = 5\n",
    "description[\"INTERPOLATE_CORNERS\"] = True\n",
    "description[\"INTERPOLATION\"] = \"cons1\"\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca80ed",
   "metadata": {},
   "source": [
    "Ico baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe580f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"PCA_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = False\n",
    "\n",
    "model_training_description[\"REGTYPE\"] = \"linreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff5ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_range = np.logspace(np.log10(3), np.log10(700), 20)\n",
    "n_pc_in, n_pc_out = np.meshgrid(n_pc_range, n_pc_range)\n",
    "n_pc_in = n_pc_in.flatten().astype(\"int\")\n",
    "n_pc_out = n_pc_out.flatten().astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1780209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results: N_PC_IN: 222 N_PC_OUT: 166, R2_mean, validationset: [0.19301843]\n",
      "Retrain including validation set.\n",
      "Result on test set: [0.2131577]\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from train_tune_pca import train_tune_pca\n",
    "pca, pca_targets, model = train_tune_pca(description, model_training_description, output_folder, \\\n",
    "                                                    n_pc_in=n_pc_in, n_pc_out=n_pc_out)\n",
    "\n",
    "predict_save_pca(description, model_training_description, output_folder, pca, pca_targets, model, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98681c",
   "metadata": {},
   "source": [
    "Ico UNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327ecbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Ico\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = IcoBatchNorm2d # torch.nn.BatchNorm2d\n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "model_training_description[\"LOSS\"] = \"MSELoss\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab785a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8002\n",
      "Epoch [2], Iter [91/100] Loss: 0.7010\n",
      "Epoch [3], Iter [91/100] Loss: 0.6858\n",
      "Epoch [4], Iter [91/100] Loss: 0.6467\n",
      "Epoch [5], Iter [91/100] Loss: 0.6227\n",
      "Epoch [6], Iter [91/100] Loss: 0.5836\n",
      "Epoch [7], Iter [91/100] Loss: 0.5944\n",
      "Epoch [8], Iter [91/100] Loss: 0.5605\n",
      "Epoch [9], Iter [91/100] Loss: 0.5584\n",
      "Epoch [10], Iter [91/100] Loss: 0.5482\n",
      "Epoch [11], Iter [91/100] Loss: 0.5457\n",
      "Epoch [12], Iter [91/100] Loss: 0.5198\n",
      "Epoch [13], Iter [91/100] Loss: 0.5154\n",
      "Epoch [14], Iter [91/100] Loss: 0.4617\n",
      "Epoch [15], Iter [91/100] Loss: 0.4886\n",
      "Test MSE: 0.6205584406852722\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8139\n",
      "Epoch [2], Iter [91/100] Loss: 0.7641\n",
      "Epoch [3], Iter [91/100] Loss: 0.6952\n",
      "Epoch [4], Iter [91/100] Loss: 0.6451\n",
      "Epoch [5], Iter [91/100] Loss: 0.6408\n",
      "Epoch [6], Iter [91/100] Loss: 0.5781\n",
      "Epoch [7], Iter [91/100] Loss: 0.5841\n",
      "Epoch [8], Iter [91/100] Loss: 0.5611\n",
      "Epoch [9], Iter [91/100] Loss: 0.5457\n",
      "Epoch [10], Iter [91/100] Loss: 0.5736\n",
      "Epoch [11], Iter [91/100] Loss: 0.5576\n",
      "Epoch [12], Iter [91/100] Loss: 0.5357\n",
      "Epoch [13], Iter [91/100] Loss: 0.4989\n",
      "Epoch [14], Iter [91/100] Loss: 0.4991\n",
      "Epoch [15], Iter [91/100] Loss: 0.4625\n",
      "Epoch [16], Iter [91/100] Loss: 0.4734\n",
      "Epoch [17], Iter [91/100] Loss: 0.4541\n",
      "Epoch [18], Iter [91/100] Loss: 0.4204\n",
      "Test MSE: 0.6036093235015869\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8109\n",
      "Epoch [2], Iter [91/100] Loss: 0.9515\n",
      "Epoch [3], Iter [91/100] Loss: 0.6567\n",
      "Epoch [4], Iter [91/100] Loss: 0.6586\n",
      "Epoch [5], Iter [91/100] Loss: 0.6293\n",
      "Epoch [6], Iter [91/100] Loss: 0.6275\n",
      "Epoch [7], Iter [91/100] Loss: 0.5998\n",
      "Epoch [8], Iter [91/100] Loss: 0.6059\n",
      "Epoch [9], Iter [91/100] Loss: 0.5747\n",
      "Epoch [10], Iter [91/100] Loss: 0.6368\n",
      "Epoch [11], Iter [91/100] Loss: 0.6088\n",
      "Epoch [12], Iter [91/100] Loss: 0.5472\n",
      "Epoch [13], Iter [91/100] Loss: 0.5291\n",
      "Epoch [14], Iter [91/100] Loss: 0.5267\n",
      "Epoch [15], Iter [91/100] Loss: 0.4906\n",
      "Epoch [16], Iter [91/100] Loss: 0.4958\n",
      "Epoch [17], Iter [91/100] Loss: 0.4504\n",
      "Epoch [18], Iter [91/100] Loss: 0.4594\n",
      "Test MSE: 0.6061821579933167\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7795\n",
      "Epoch [2], Iter [91/100] Loss: 0.6496\n",
      "Epoch [3], Iter [91/100] Loss: 0.6517\n",
      "Epoch [4], Iter [91/100] Loss: 0.6112\n",
      "Epoch [5], Iter [91/100] Loss: 0.6200\n",
      "Epoch [6], Iter [91/100] Loss: 0.5809\n",
      "Epoch [7], Iter [91/100] Loss: 0.5852\n",
      "Epoch [8], Iter [91/100] Loss: 0.5862\n",
      "Epoch [9], Iter [91/100] Loss: 0.5328\n",
      "Epoch [10], Iter [91/100] Loss: 0.5322\n",
      "Epoch [11], Iter [91/100] Loss: 0.4972\n",
      "Epoch [12], Iter [91/100] Loss: 0.4970\n",
      "Epoch [13], Iter [91/100] Loss: 0.4769\n",
      "Epoch [14], Iter [91/100] Loss: 0.4572\n",
      "Epoch [15], Iter [91/100] Loss: 0.4422\n",
      "Epoch [16], Iter [91/100] Loss: 0.4211\n",
      "Epoch [17], Iter [91/100] Loss: 0.4267\n",
      "Epoch [18], Iter [91/100] Loss: 0.3964\n",
      "Epoch [19], Iter [91/100] Loss: 0.4121\n",
      "Epoch [20], Iter [91/100] Loss: 0.3742\n",
      "Test MSE: 0.61271733045578\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8088\n",
      "Epoch [2], Iter [91/100] Loss: 0.7485\n",
      "Epoch [3], Iter [91/100] Loss: 0.6600\n",
      "Epoch [4], Iter [91/100] Loss: 0.6366\n",
      "Epoch [5], Iter [91/100] Loss: 0.6186\n",
      "Epoch [6], Iter [91/100] Loss: 0.6289\n",
      "Epoch [7], Iter [91/100] Loss: 0.5833\n",
      "Epoch [8], Iter [91/100] Loss: 0.5597\n",
      "Epoch [9], Iter [91/100] Loss: 0.5618\n",
      "Epoch [10], Iter [91/100] Loss: 0.5417\n",
      "Epoch [11], Iter [91/100] Loss: 0.5228\n",
      "Epoch [12], Iter [91/100] Loss: 0.5411\n",
      "Epoch [13], Iter [91/100] Loss: 0.5027\n",
      "Epoch [14], Iter [91/100] Loss: 0.4826\n",
      "Epoch [15], Iter [91/100] Loss: 0.4789\n",
      "Epoch [16], Iter [91/100] Loss: 0.4532\n",
      "Epoch [17], Iter [91/100] Loss: 0.4330\n",
      "Test MSE: 0.6188255548477173\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8405\n",
      "Epoch [2], Iter [91/100] Loss: 0.7518\n",
      "Epoch [3], Iter [91/100] Loss: 0.6849\n",
      "Epoch [4], Iter [91/100] Loss: 0.6672\n",
      "Epoch [5], Iter [91/100] Loss: 0.6788\n",
      "Epoch [6], Iter [91/100] Loss: 0.6013\n",
      "Epoch [7], Iter [91/100] Loss: 0.6264\n",
      "Epoch [8], Iter [91/100] Loss: 0.5895\n",
      "Epoch [9], Iter [91/100] Loss: 0.5728\n",
      "Epoch [10], Iter [91/100] Loss: 0.5805\n",
      "Epoch [11], Iter [91/100] Loss: 0.5251\n",
      "Epoch [12], Iter [91/100] Loss: 0.5329\n",
      "Epoch [13], Iter [91/100] Loss: 0.5177\n",
      "Epoch [14], Iter [91/100] Loss: 0.5007\n",
      "Epoch [15], Iter [91/100] Loss: 0.5055\n",
      "Test MSE: 0.6143753528594971\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7826\n",
      "Epoch [2], Iter [91/100] Loss: 0.7020\n",
      "Epoch [3], Iter [91/100] Loss: 0.6604\n",
      "Epoch [4], Iter [91/100] Loss: 0.6584\n",
      "Epoch [5], Iter [91/100] Loss: 0.6225\n",
      "Epoch [6], Iter [91/100] Loss: 0.6474\n",
      "Epoch [7], Iter [91/100] Loss: 0.6194\n",
      "Epoch [8], Iter [91/100] Loss: 0.5620\n",
      "Epoch [9], Iter [91/100] Loss: 0.5951\n",
      "Epoch [10], Iter [91/100] Loss: 0.5588\n",
      "Epoch [11], Iter [91/100] Loss: 0.5365\n",
      "Epoch [12], Iter [91/100] Loss: 0.5369\n",
      "Epoch [13], Iter [91/100] Loss: 0.5078\n",
      "Epoch [14], Iter [91/100] Loss: 0.5081\n",
      "Epoch [15], Iter [91/100] Loss: 0.4728\n",
      "Epoch [16], Iter [91/100] Loss: 0.4566\n",
      "Epoch [17], Iter [91/100] Loss: 0.4441\n",
      "Epoch [18], Iter [91/100] Loss: 0.4187\n",
      "Epoch [19], Iter [91/100] Loss: 0.4109\n",
      "Epoch [20], Iter [91/100] Loss: 0.3946\n",
      "Epoch [21], Iter [91/100] Loss: 0.3915\n",
      "Epoch [22], Iter [91/100] Loss: 0.3792\n",
      "Test MSE: 0.617697536945343\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8621\n",
      "Epoch [2], Iter [91/100] Loss: 0.7585\n",
      "Epoch [3], Iter [91/100] Loss: 0.8161\n",
      "Epoch [4], Iter [91/100] Loss: 0.6521\n",
      "Epoch [5], Iter [91/100] Loss: 0.7031\n",
      "Epoch [6], Iter [91/100] Loss: 0.6340\n",
      "Epoch [7], Iter [91/100] Loss: 0.6009\n",
      "Epoch [8], Iter [91/100] Loss: 0.6335\n",
      "Epoch [9], Iter [91/100] Loss: 0.6052\n",
      "Epoch [10], Iter [91/100] Loss: 0.6015\n",
      "Epoch [11], Iter [91/100] Loss: 0.5676\n",
      "Epoch [12], Iter [91/100] Loss: 0.5679\n",
      "Epoch [13], Iter [91/100] Loss: 0.5479\n",
      "Epoch [14], Iter [91/100] Loss: 0.5287\n",
      "Epoch [15], Iter [91/100] Loss: 0.5364\n",
      "Epoch [16], Iter [91/100] Loss: 0.5551\n",
      "Epoch [17], Iter [91/100] Loss: 0.5104\n",
      "Epoch [18], Iter [91/100] Loss: 0.5157\n",
      "Epoch [19], Iter [91/100] Loss: 0.5818\n",
      "Test MSE: 0.6075485348701477\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8608\n",
      "Epoch [2], Iter [91/100] Loss: 0.8255\n",
      "Epoch [3], Iter [91/100] Loss: 0.7386\n",
      "Epoch [4], Iter [91/100] Loss: 0.6809\n",
      "Epoch [5], Iter [91/100] Loss: 0.6546\n",
      "Epoch [6], Iter [91/100] Loss: 0.6738\n",
      "Epoch [7], Iter [91/100] Loss: 0.6426\n",
      "Epoch [8], Iter [91/100] Loss: 0.6527\n",
      "Epoch [9], Iter [91/100] Loss: 0.5891\n",
      "Epoch [10], Iter [91/100] Loss: 0.6246\n",
      "Epoch [11], Iter [91/100] Loss: 0.5857\n",
      "Epoch [12], Iter [91/100] Loss: 0.5792\n",
      "Epoch [13], Iter [91/100] Loss: 0.5474\n",
      "Epoch [14], Iter [91/100] Loss: 0.5524\n",
      "Epoch [15], Iter [91/100] Loss: 0.5501\n",
      "Epoch [16], Iter [91/100] Loss: 0.5123\n",
      "Epoch [17], Iter [91/100] Loss: 0.5098\n",
      "Epoch [18], Iter [91/100] Loss: 0.5057\n",
      "Epoch [19], Iter [91/100] Loss: 0.5053\n",
      "Epoch [20], Iter [91/100] Loss: 0.4465\n",
      "Epoch [21], Iter [91/100] Loss: 0.4704\n",
      "Epoch [22], Iter [91/100] Loss: 0.4439\n",
      "Test MSE: 0.5933142900466919\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.8753\n",
      "Epoch [2], Iter [91/100] Loss: 0.7273\n",
      "Epoch [3], Iter [91/100] Loss: 0.7898\n",
      "Epoch [4], Iter [91/100] Loss: 0.6106\n",
      "Epoch [5], Iter [91/100] Loss: 0.6479\n",
      "Epoch [6], Iter [91/100] Loss: 0.6549\n",
      "Epoch [7], Iter [91/100] Loss: 0.6294\n",
      "Epoch [8], Iter [91/100] Loss: 0.6073\n",
      "Epoch [9], Iter [91/100] Loss: 0.5766\n",
      "Epoch [10], Iter [91/100] Loss: 0.5767\n",
      "Epoch [11], Iter [91/100] Loss: 0.5585\n",
      "Epoch [12], Iter [91/100] Loss: 0.5341\n",
      "Epoch [13], Iter [91/100] Loss: 0.5228\n",
      "Epoch [14], Iter [91/100] Loss: 0.5418\n",
      "Epoch [15], Iter [91/100] Loss: 0.5169\n",
      "Epoch [16], Iter [91/100] Loss: 0.4921\n",
      "Epoch [17], Iter [91/100] Loss: 0.4672\n",
      "Epoch [18], Iter [91/100] Loss: 0.4421\n",
      "Epoch [19], Iter [91/100] Loss: 0.4162\n",
      "Epoch [20], Iter [91/100] Loss: 0.4052\n",
      "Epoch [21], Iter [91/100] Loss: 0.4145\n",
      "Test MSE: 0.5961763858795166\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9f8e0",
   "metadata": {},
   "source": [
    "## 4.2.3 COMPARING RESULTS FOR DIFFERENT PREDICTOR COMBINATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4b0c3",
   "metadata": {},
   "source": [
    "### tas only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40ff3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9bacf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8394\n",
      "Epoch [2], Iter [91/101] Loss: 0.7628\n",
      "Epoch [3], Iter [91/101] Loss: 0.7747\n",
      "Epoch [4], Iter [91/101] Loss: 0.6802\n",
      "Epoch [5], Iter [91/101] Loss: 0.7179\n",
      "Epoch [6], Iter [91/101] Loss: 0.7440\n",
      "Epoch [7], Iter [91/101] Loss: 0.6962\n",
      "Epoch [8], Iter [91/101] Loss: 0.6661\n",
      "Epoch [9], Iter [91/101] Loss: 0.6903\n",
      "Epoch [10], Iter [91/101] Loss: 0.6455\n",
      "Epoch [11], Iter [91/101] Loss: 0.6522\n",
      "Epoch [12], Iter [91/101] Loss: 0.6438\n",
      "Epoch [13], Iter [91/101] Loss: 0.6564\n",
      "Epoch [14], Iter [91/101] Loss: 0.6296\n",
      "Epoch [15], Iter [91/101] Loss: 0.6375\n",
      "Epoch [16], Iter [91/101] Loss: 0.5864\n",
      "Epoch [17], Iter [91/101] Loss: 0.5764\n",
      "Epoch [18], Iter [91/101] Loss: 0.6116\n",
      "Epoch [19], Iter [91/101] Loss: 0.5854\n",
      "Epoch [20], Iter [91/101] Loss: 0.5697\n",
      "Test MSE: 0.8802059292793274\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7478\n",
      "Epoch [2], Iter [91/101] Loss: 0.7648\n",
      "Epoch [3], Iter [91/101] Loss: 0.7943\n",
      "Epoch [4], Iter [91/101] Loss: 0.7531\n",
      "Epoch [5], Iter [91/101] Loss: 0.7246\n",
      "Epoch [6], Iter [91/101] Loss: 0.6825\n",
      "Epoch [7], Iter [91/101] Loss: 0.6963\n",
      "Epoch [8], Iter [91/101] Loss: 0.6902\n",
      "Epoch [9], Iter [91/101] Loss: 0.6949\n",
      "Epoch [10], Iter [91/101] Loss: 0.7400\n",
      "Epoch [11], Iter [91/101] Loss: 0.6784\n",
      "Epoch [12], Iter [91/101] Loss: 0.6420\n",
      "Epoch [13], Iter [91/101] Loss: 0.6192\n",
      "Epoch [14], Iter [91/101] Loss: 0.6070\n",
      "Epoch [15], Iter [91/101] Loss: 0.6085\n",
      "Epoch [16], Iter [91/101] Loss: 0.6087\n",
      "Epoch [17], Iter [91/101] Loss: 0.6301\n",
      "Epoch [18], Iter [91/101] Loss: 0.5869\n",
      "Test MSE: 0.8360264301300049\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8483\n",
      "Epoch [2], Iter [91/101] Loss: 0.7368\n",
      "Epoch [3], Iter [91/101] Loss: 0.8234\n",
      "Epoch [4], Iter [91/101] Loss: 0.7439\n",
      "Epoch [5], Iter [91/101] Loss: 0.8051\n",
      "Epoch [6], Iter [91/101] Loss: 0.7203\n",
      "Epoch [7], Iter [91/101] Loss: 0.6564\n",
      "Epoch [8], Iter [91/101] Loss: 0.6742\n",
      "Epoch [9], Iter [91/101] Loss: 0.6925\n",
      "Epoch [10], Iter [91/101] Loss: 0.6573\n",
      "Epoch [11], Iter [91/101] Loss: 0.6836\n",
      "Epoch [12], Iter [91/101] Loss: 0.6402\n",
      "Epoch [13], Iter [91/101] Loss: 0.6020\n",
      "Epoch [14], Iter [91/101] Loss: 0.6267\n",
      "Epoch [15], Iter [91/101] Loss: 0.6263\n",
      "Epoch [16], Iter [91/101] Loss: 0.5896\n",
      "Epoch [17], Iter [91/101] Loss: 0.5925\n",
      "Test MSE: 0.8676066994667053\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7607\n",
      "Epoch [2], Iter [91/101] Loss: 0.8225\n",
      "Epoch [3], Iter [91/101] Loss: 0.7542\n",
      "Epoch [4], Iter [91/101] Loss: 0.7186\n",
      "Epoch [5], Iter [91/101] Loss: 0.7465\n",
      "Epoch [6], Iter [91/101] Loss: 0.7834\n",
      "Epoch [7], Iter [91/101] Loss: 0.7126\n",
      "Epoch [8], Iter [91/101] Loss: 0.6624\n",
      "Epoch [9], Iter [91/101] Loss: 0.6611\n",
      "Epoch [10], Iter [91/101] Loss: 0.6671\n",
      "Epoch [11], Iter [91/101] Loss: 0.6285\n",
      "Epoch [12], Iter [91/101] Loss: 0.6721\n",
      "Epoch [13], Iter [91/101] Loss: 0.6371\n",
      "Epoch [14], Iter [91/101] Loss: 0.6487\n",
      "Epoch [15], Iter [91/101] Loss: 0.5956\n",
      "Epoch [16], Iter [91/101] Loss: 0.6076\n",
      "Epoch [17], Iter [91/101] Loss: 0.5812\n",
      "Epoch [18], Iter [91/101] Loss: 0.5829\n",
      "Test MSE: 0.8168020844459534\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f174e",
   "metadata": {},
   "source": [
    "### precip only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82472aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23f2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce2\\-0x474f701d9a1cf0d7-0x3f3842cab6568e1c_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8347\n",
      "Epoch [2], Iter [91/101] Loss: 0.6473\n",
      "Epoch [3], Iter [91/101] Loss: 0.6521\n",
      "Epoch [4], Iter [91/101] Loss: 0.7158\n",
      "Epoch [5], Iter [91/101] Loss: 0.6269\n",
      "Epoch [6], Iter [91/101] Loss: 0.6163\n",
      "Epoch [7], Iter [91/101] Loss: 0.6173\n",
      "Epoch [8], Iter [91/101] Loss: 0.6744\n",
      "Epoch [9], Iter [91/101] Loss: 0.5790\n",
      "Epoch [10], Iter [91/101] Loss: 0.6041\n",
      "Epoch [11], Iter [91/101] Loss: 0.5722\n",
      "Epoch [12], Iter [91/101] Loss: 0.6055\n",
      "Epoch [13], Iter [91/101] Loss: 0.5720\n",
      "Epoch [14], Iter [91/101] Loss: 0.5649\n",
      "Epoch [15], Iter [91/101] Loss: 0.5517\n",
      "Epoch [16], Iter [91/101] Loss: 0.5626\n",
      "Epoch [17], Iter [91/101] Loss: 0.5193\n",
      "Epoch [18], Iter [91/101] Loss: 0.5570\n",
      "Epoch [19], Iter [91/101] Loss: 0.5527\n",
      "Epoch [20], Iter [91/101] Loss: 0.5323\n",
      "Epoch [21], Iter [91/101] Loss: 0.4994\n",
      "Test MSE: 0.675542414188385\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce2\\-0x474f701d9a1cf0d7-0x7a88af6a854dc503_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7071\n",
      "Epoch [2], Iter [91/101] Loss: 0.6809\n",
      "Epoch [3], Iter [91/101] Loss: 0.6418\n",
      "Epoch [4], Iter [91/101] Loss: 0.6401\n",
      "Epoch [5], Iter [91/101] Loss: 0.5941\n",
      "Epoch [6], Iter [91/101] Loss: 0.6090\n",
      "Epoch [7], Iter [91/101] Loss: 0.5981\n",
      "Epoch [8], Iter [91/101] Loss: 1.2938\n",
      "Epoch [9], Iter [91/101] Loss: 0.6331\n",
      "Epoch [10], Iter [91/101] Loss: 0.5963\n",
      "Epoch [11], Iter [91/101] Loss: 0.6059\n",
      "Epoch [12], Iter [91/101] Loss: 0.5841\n",
      "Epoch [13], Iter [91/101] Loss: 0.5860\n",
      "Epoch [14], Iter [91/101] Loss: 0.5599\n",
      "Epoch [15], Iter [91/101] Loss: 0.5596\n",
      "Epoch [16], Iter [91/101] Loss: 0.5537\n",
      "Epoch [17], Iter [91/101] Loss: 0.5680\n",
      "Epoch [18], Iter [91/101] Loss: 0.5440\n",
      "Test MSE: 0.6645519137382507\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce2\\-0x474f701d9a1cf0d7-0x4df92eaaaa6726ed_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7074\n",
      "Epoch [2], Iter [91/101] Loss: 0.7362\n",
      "Epoch [3], Iter [91/101] Loss: 0.6782\n",
      "Epoch [4], Iter [91/101] Loss: 0.6065\n",
      "Epoch [5], Iter [91/101] Loss: 0.7453\n",
      "Epoch [6], Iter [91/101] Loss: 0.6250\n",
      "Epoch [7], Iter [91/101] Loss: 0.6003\n",
      "Epoch [8], Iter [91/101] Loss: 0.6134\n",
      "Epoch [9], Iter [91/101] Loss: 0.5367\n",
      "Epoch [10], Iter [91/101] Loss: 0.6065\n",
      "Epoch [11], Iter [91/101] Loss: 0.5776\n",
      "Epoch [12], Iter [91/101] Loss: 0.5598\n",
      "Epoch [13], Iter [91/101] Loss: 0.5598\n",
      "Epoch [14], Iter [91/101] Loss: 0.5484\n",
      "Epoch [15], Iter [91/101] Loss: 0.5427\n",
      "Epoch [16], Iter [91/101] Loss: 0.5329\n",
      "Epoch [17], Iter [91/101] Loss: 0.5445\n",
      "Epoch [18], Iter [91/101] Loss: 0.5179\n",
      "Epoch [19], Iter [91/101] Loss: 0.5522\n",
      "Test MSE: 0.667618989944458\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "To open tensorboard, run tensorboard --logdir=Output/Reproduce2\\-0x474f701d9a1cf0d70x718d0b717cb49eba_log\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7051\n",
      "Epoch [2], Iter [91/101] Loss: 0.7594\n",
      "Epoch [3], Iter [91/101] Loss: 0.6224\n",
      "Epoch [4], Iter [91/101] Loss: 0.6586\n",
      "Epoch [5], Iter [91/101] Loss: 0.6555\n",
      "Epoch [6], Iter [91/101] Loss: 0.6302\n",
      "Epoch [7], Iter [91/101] Loss: 0.6014\n",
      "Epoch [8], Iter [91/101] Loss: 0.5946\n",
      "Epoch [9], Iter [91/101] Loss: 0.6267\n",
      "Epoch [10], Iter [91/101] Loss: 0.5810\n",
      "Epoch [11], Iter [91/101] Loss: 0.5957\n",
      "Epoch [12], Iter [91/101] Loss: 0.5664\n",
      "Epoch [13], Iter [91/101] Loss: 0.5928\n",
      "Epoch [14], Iter [91/101] Loss: 0.5809\n",
      "Epoch [15], Iter [91/101] Loss: 0.5937\n",
      "Epoch [16], Iter [91/101] Loss: 0.5730\n",
      "Epoch [17], Iter [91/101] Loss: 0.5369\n",
      "Epoch [18], Iter [91/101] Loss: 0.5553\n",
      "Epoch [19], Iter [91/101] Loss: 0.5446\n",
      "Epoch [20], Iter [91/101] Loss: 0.5102\n",
      "Epoch [21], Iter [91/101] Loss: 0.4901\n",
      "Epoch [22], Iter [91/101] Loss: 0.4984\n",
      "Epoch [23], Iter [91/101] Loss: 0.4853\n",
      "Epoch [24], Iter [91/101] Loss: 0.4532\n",
      "Test MSE: 0.669654130935669\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder, use_tensorboard=True)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd59a25",
   "metadata": {},
   "source": [
    "### slp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9d6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "312560f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8404\n",
      "Epoch [2], Iter [91/101] Loss: 0.7666\n",
      "Epoch [3], Iter [91/101] Loss: 0.7299\n",
      "Epoch [4], Iter [91/101] Loss: 0.6857\n",
      "Epoch [5], Iter [91/101] Loss: 0.6709\n",
      "Epoch [6], Iter [91/101] Loss: 0.6884\n",
      "Epoch [7], Iter [91/101] Loss: 0.6361\n",
      "Epoch [8], Iter [91/101] Loss: 0.6545\n",
      "Epoch [9], Iter [91/101] Loss: 0.6630\n",
      "Epoch [10], Iter [91/101] Loss: 0.6214\n",
      "Epoch [11], Iter [91/101] Loss: 0.6685\n",
      "Epoch [12], Iter [91/101] Loss: 0.5850\n",
      "Epoch [13], Iter [91/101] Loss: 0.6255\n",
      "Epoch [14], Iter [91/101] Loss: 0.6009\n",
      "Epoch [15], Iter [91/101] Loss: 0.6100\n",
      "Epoch [16], Iter [91/101] Loss: 1.2371\n",
      "Epoch [17], Iter [91/101] Loss: 0.5761\n",
      "Epoch [18], Iter [91/101] Loss: 0.5362\n",
      "Epoch [19], Iter [91/101] Loss: 0.5554\n",
      "Epoch [20], Iter [91/101] Loss: 0.5478\n",
      "Epoch [21], Iter [91/101] Loss: 0.5547\n",
      "Epoch [22], Iter [91/101] Loss: 0.5314\n",
      "Epoch [23], Iter [91/101] Loss: 0.5735\n",
      "Epoch [24], Iter [91/101] Loss: 0.5301\n",
      "Test MSE: 0.7179846167564392\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8068\n",
      "Epoch [2], Iter [91/101] Loss: 0.7531\n",
      "Epoch [3], Iter [91/101] Loss: 0.7235\n",
      "Epoch [4], Iter [91/101] Loss: 0.6946\n",
      "Epoch [5], Iter [91/101] Loss: 0.6530\n",
      "Epoch [6], Iter [91/101] Loss: 0.6885\n",
      "Epoch [7], Iter [91/101] Loss: 0.6332\n",
      "Epoch [8], Iter [91/101] Loss: 0.6452\n",
      "Epoch [9], Iter [91/101] Loss: 0.6543\n",
      "Epoch [10], Iter [91/101] Loss: 0.5924\n",
      "Epoch [11], Iter [91/101] Loss: 0.6109\n",
      "Epoch [12], Iter [91/101] Loss: 0.6212\n",
      "Epoch [13], Iter [91/101] Loss: 0.6157\n",
      "Epoch [14], Iter [91/101] Loss: 0.5641\n",
      "Epoch [15], Iter [91/101] Loss: 0.5944\n",
      "Epoch [16], Iter [91/101] Loss: 0.5857\n",
      "Epoch [17], Iter [91/101] Loss: 0.5763\n",
      "Epoch [18], Iter [91/101] Loss: 0.5582\n",
      "Epoch [19], Iter [91/101] Loss: 0.5643\n",
      "Epoch [20], Iter [91/101] Loss: 0.5460\n",
      "Epoch [21], Iter [91/101] Loss: 0.5515\n",
      "Test MSE: 0.7194775342941284\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7415\n",
      "Epoch [2], Iter [91/101] Loss: 0.7582\n",
      "Epoch [3], Iter [91/101] Loss: 0.7097\n",
      "Epoch [4], Iter [91/101] Loss: 0.7108\n",
      "Epoch [5], Iter [91/101] Loss: 0.7201\n",
      "Epoch [6], Iter [91/101] Loss: 0.6513\n",
      "Epoch [7], Iter [91/101] Loss: 0.6568\n",
      "Epoch [8], Iter [91/101] Loss: 0.6297\n",
      "Epoch [9], Iter [91/101] Loss: 0.6209\n",
      "Epoch [10], Iter [91/101] Loss: 0.6222\n",
      "Epoch [11], Iter [91/101] Loss: 0.5864\n",
      "Epoch [12], Iter [91/101] Loss: 0.5889\n",
      "Epoch [13], Iter [91/101] Loss: 0.5880\n",
      "Epoch [14], Iter [91/101] Loss: 0.5836\n",
      "Epoch [15], Iter [91/101] Loss: 0.5771\n",
      "Epoch [16], Iter [91/101] Loss: 0.5613\n",
      "Epoch [17], Iter [91/101] Loss: 0.5719\n",
      "Test MSE: 0.7191644906997681\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 1.5948\n",
      "Epoch [2], Iter [91/101] Loss: 0.7269\n",
      "Epoch [3], Iter [91/101] Loss: 0.7699\n",
      "Epoch [4], Iter [91/101] Loss: 0.6770\n",
      "Epoch [5], Iter [91/101] Loss: 0.6782\n",
      "Epoch [6], Iter [91/101] Loss: 0.6531\n",
      "Epoch [7], Iter [91/101] Loss: 0.6572\n",
      "Epoch [8], Iter [91/101] Loss: 0.6383\n",
      "Epoch [9], Iter [91/101] Loss: 0.6191\n",
      "Epoch [10], Iter [91/101] Loss: 0.6176\n",
      "Epoch [11], Iter [91/101] Loss: 0.6360\n",
      "Epoch [12], Iter [91/101] Loss: 0.5927\n",
      "Epoch [13], Iter [91/101] Loss: 0.6272\n",
      "Epoch [14], Iter [91/101] Loss: 0.5791\n",
      "Epoch [15], Iter [91/101] Loss: 0.5611\n",
      "Epoch [16], Iter [91/101] Loss: 0.5762\n",
      "Test MSE: 0.718652069568634\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c2dba",
   "metadata": {},
   "source": [
    "### slp, tas, precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6767544",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c45f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6061\n",
      "Epoch [2], Iter [91/101] Loss: 0.5838\n",
      "Epoch [3], Iter [91/101] Loss: 0.6173\n",
      "Epoch [4], Iter [91/101] Loss: 0.5389\n",
      "Epoch [5], Iter [91/101] Loss: 0.5581\n",
      "Epoch [6], Iter [91/101] Loss: 0.5726\n",
      "Epoch [7], Iter [91/101] Loss: 0.5606\n",
      "Epoch [8], Iter [91/101] Loss: 0.6610\n",
      "Epoch [9], Iter [91/101] Loss: 0.5263\n",
      "Epoch [10], Iter [91/101] Loss: 0.5271\n",
      "Epoch [11], Iter [91/101] Loss: 0.5047\n",
      "Epoch [12], Iter [91/101] Loss: 0.5039\n",
      "Epoch [13], Iter [91/101] Loss: 0.4989\n",
      "Epoch [14], Iter [91/101] Loss: 0.5052\n",
      "Epoch [15], Iter [91/101] Loss: 0.4858\n",
      "Epoch [16], Iter [91/101] Loss: 0.5147\n",
      "Epoch [17], Iter [91/101] Loss: 0.4867\n",
      "Epoch [18], Iter [91/101] Loss: 0.4631\n",
      "Epoch [19], Iter [91/101] Loss: 0.4762\n",
      "Epoch [20], Iter [91/101] Loss: 0.4817\n",
      "Test MSE: 0.6211734414100647\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7027\n",
      "Epoch [2], Iter [91/101] Loss: 0.6425\n",
      "Epoch [3], Iter [91/101] Loss: 0.6553\n",
      "Epoch [4], Iter [91/101] Loss: 0.5710\n",
      "Epoch [5], Iter [91/101] Loss: 0.5387\n",
      "Epoch [6], Iter [91/101] Loss: 0.5487\n",
      "Epoch [7], Iter [91/101] Loss: 0.5620\n",
      "Epoch [8], Iter [91/101] Loss: 0.6364\n",
      "Epoch [9], Iter [91/101] Loss: 0.5383\n",
      "Epoch [10], Iter [91/101] Loss: 0.5246\n",
      "Epoch [11], Iter [91/101] Loss: 0.5430\n",
      "Epoch [12], Iter [91/101] Loss: 0.5450\n",
      "Epoch [13], Iter [91/101] Loss: 0.5026\n",
      "Epoch [14], Iter [91/101] Loss: 0.4995\n",
      "Epoch [15], Iter [91/101] Loss: 0.5317\n",
      "Epoch [16], Iter [91/101] Loss: 0.4886\n",
      "Epoch [17], Iter [91/101] Loss: 0.4746\n",
      "Epoch [18], Iter [91/101] Loss: 0.4505\n",
      "Epoch [19], Iter [91/101] Loss: 0.4793\n",
      "Epoch [20], Iter [91/101] Loss: 0.4575\n",
      "Epoch [21], Iter [91/101] Loss: 0.4471\n",
      "Epoch [22], Iter [91/101] Loss: 0.4462\n",
      "Epoch [23], Iter [91/101] Loss: 0.4644\n",
      "Epoch [24], Iter [91/101] Loss: 0.4468\n",
      "Epoch [25], Iter [91/101] Loss: 0.4448\n",
      "Epoch [26], Iter [91/101] Loss: 0.4890\n",
      "Epoch [27], Iter [91/101] Loss: 0.4173\n",
      "Epoch [28], Iter [91/101] Loss: 0.4938\n",
      "Test MSE: 0.6579325795173645\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6550\n",
      "Epoch [2], Iter [91/101] Loss: 0.6134\n",
      "Epoch [3], Iter [91/101] Loss: 0.5867\n",
      "Epoch [4], Iter [91/101] Loss: 0.5759\n",
      "Epoch [5], Iter [91/101] Loss: 0.5681\n",
      "Epoch [6], Iter [91/101] Loss: 0.5549\n",
      "Epoch [7], Iter [91/101] Loss: 0.5509\n",
      "Epoch [8], Iter [91/101] Loss: 0.5302\n",
      "Epoch [9], Iter [91/101] Loss: 0.5398\n",
      "Epoch [10], Iter [91/101] Loss: 0.5185\n",
      "Epoch [11], Iter [91/101] Loss: 0.5067\n",
      "Epoch [12], Iter [91/101] Loss: 0.5209\n",
      "Epoch [13], Iter [91/101] Loss: 0.4909\n",
      "Epoch [14], Iter [91/101] Loss: 0.4861\n",
      "Epoch [15], Iter [91/101] Loss: 0.5891\n",
      "Epoch [16], Iter [91/101] Loss: 0.4976\n",
      "Epoch [17], Iter [91/101] Loss: 0.4726\n",
      "Epoch [18], Iter [91/101] Loss: 0.4914\n",
      "Epoch [19], Iter [91/101] Loss: 0.4661\n",
      "Epoch [20], Iter [91/101] Loss: 0.4718\n",
      "Epoch [21], Iter [91/101] Loss: 0.4668\n",
      "Test MSE: 0.6100334525108337\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6078\n",
      "Epoch [2], Iter [91/101] Loss: 0.6281\n",
      "Epoch [3], Iter [91/101] Loss: 0.6040\n",
      "Epoch [4], Iter [91/101] Loss: 0.5588\n",
      "Epoch [5], Iter [91/101] Loss: 0.5788\n",
      "Epoch [6], Iter [91/101] Loss: 0.5532\n",
      "Epoch [7], Iter [91/101] Loss: 0.5286\n",
      "Epoch [8], Iter [91/101] Loss: 0.5559\n",
      "Epoch [9], Iter [91/101] Loss: 0.5282\n",
      "Epoch [10], Iter [91/101] Loss: 0.5475\n",
      "Epoch [11], Iter [91/101] Loss: 0.5162\n",
      "Epoch [12], Iter [91/101] Loss: 0.5215\n",
      "Epoch [13], Iter [91/101] Loss: 0.5099\n",
      "Epoch [14], Iter [91/101] Loss: 0.4965\n",
      "Epoch [15], Iter [91/101] Loss: 0.5132\n",
      "Epoch [16], Iter [91/101] Loss: 0.5843\n",
      "Epoch [17], Iter [91/101] Loss: 0.4729\n",
      "Epoch [18], Iter [91/101] Loss: 0.4865\n",
      "Epoch [19], Iter [91/101] Loss: 0.4596\n",
      "Epoch [20], Iter [91/101] Loss: 0.4589\n",
      "Epoch [21], Iter [91/101] Loss: 0.4640\n",
      "Epoch [22], Iter [91/101] Loss: 0.4668\n",
      "Epoch [23], Iter [91/101] Loss: 0.4593\n",
      "Epoch [24], Iter [91/101] Loss: 0.4349\n",
      "Test MSE: 0.5900517702102661\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e44dc4",
   "metadata": {},
   "source": [
    "### precip, tas, orogrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f10ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\",\n",
    "                                \"oro\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"oro\": [\"ht\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Global\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30178260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9712\n",
      "Epoch [2], Iter [91/101] Loss: 0.9724\n",
      "Epoch [3], Iter [91/101] Loss: 0.8527\n",
      "Epoch [4], Iter [91/101] Loss: 0.7844\n",
      "Epoch [5], Iter [91/101] Loss: 0.6688\n",
      "Epoch [6], Iter [91/101] Loss: 0.6634\n",
      "Epoch [7], Iter [91/101] Loss: 0.6283\n",
      "Epoch [8], Iter [91/101] Loss: 0.6281\n",
      "Epoch [9], Iter [91/101] Loss: 0.5591\n",
      "Epoch [10], Iter [91/101] Loss: 0.5722\n",
      "Epoch [11], Iter [91/101] Loss: 0.5366\n",
      "Epoch [12], Iter [91/101] Loss: 0.5625\n",
      "Epoch [13], Iter [91/101] Loss: 0.5466\n",
      "Epoch [14], Iter [91/101] Loss: 0.5352\n",
      "Epoch [15], Iter [91/101] Loss: 0.5261\n",
      "Epoch [16], Iter [91/101] Loss: 0.4990\n",
      "Epoch [17], Iter [91/101] Loss: 0.4983\n",
      "Epoch [18], Iter [91/101] Loss: 0.4843\n",
      "Epoch [19], Iter [91/101] Loss: 0.4887\n",
      "Epoch [20], Iter [91/101] Loss: 0.4706\n",
      "Epoch [21], Iter [91/101] Loss: 0.4699\n",
      "Epoch [22], Iter [91/101] Loss: 0.4762\n",
      "Test MSE: 0.6222823262214661\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9481\n",
      "Epoch [2], Iter [91/101] Loss: 1.0484\n",
      "Epoch [3], Iter [91/101] Loss: 0.8093\n",
      "Epoch [4], Iter [91/101] Loss: 0.7059\n",
      "Epoch [5], Iter [91/101] Loss: 0.6813\n",
      "Epoch [6], Iter [91/101] Loss: 0.6409\n",
      "Epoch [7], Iter [91/101] Loss: 0.6403\n",
      "Epoch [8], Iter [91/101] Loss: 0.5942\n",
      "Epoch [9], Iter [91/101] Loss: 0.5579\n",
      "Epoch [10], Iter [91/101] Loss: 0.5420\n",
      "Epoch [11], Iter [91/101] Loss: 0.5359\n",
      "Epoch [12], Iter [91/101] Loss: 0.5284\n",
      "Epoch [13], Iter [91/101] Loss: 0.5113\n",
      "Epoch [14], Iter [91/101] Loss: 0.5141\n",
      "Epoch [15], Iter [91/101] Loss: 0.5034\n",
      "Epoch [16], Iter [91/101] Loss: 0.5148\n",
      "Epoch [17], Iter [91/101] Loss: 0.5159\n",
      "Epoch [18], Iter [91/101] Loss: 0.4928\n",
      "Epoch [19], Iter [91/101] Loss: 0.5014\n",
      "Epoch [20], Iter [91/101] Loss: 0.4818\n",
      "Test MSE: 0.6173062920570374\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9085\n",
      "Epoch [2], Iter [91/101] Loss: 0.8696\n",
      "Epoch [3], Iter [91/101] Loss: 0.7148\n",
      "Epoch [4], Iter [91/101] Loss: 0.7140\n",
      "Epoch [5], Iter [91/101] Loss: 0.6223\n",
      "Epoch [6], Iter [91/101] Loss: 0.7400\n",
      "Epoch [7], Iter [91/101] Loss: 0.5356\n",
      "Epoch [8], Iter [91/101] Loss: 0.5705\n",
      "Epoch [9], Iter [91/101] Loss: 0.5321\n",
      "Epoch [10], Iter [91/101] Loss: 0.5434\n",
      "Epoch [11], Iter [91/101] Loss: 0.5764\n",
      "Epoch [12], Iter [91/101] Loss: 0.5389\n",
      "Epoch [13], Iter [91/101] Loss: 0.5290\n",
      "Epoch [14], Iter [91/101] Loss: 0.5146\n",
      "Epoch [15], Iter [91/101] Loss: 0.4912\n",
      "Epoch [16], Iter [91/101] Loss: 0.4940\n",
      "Epoch [17], Iter [91/101] Loss: 0.5024\n",
      "Epoch [18], Iter [91/101] Loss: 0.4884\n",
      "Epoch [19], Iter [91/101] Loss: 0.4797\n",
      "Epoch [20], Iter [91/101] Loss: 0.4837\n",
      "Epoch [21], Iter [91/101] Loss: 0.4712\n",
      "Epoch [22], Iter [91/101] Loss: 0.4628\n",
      "Epoch [23], Iter [91/101] Loss: 0.4462\n",
      "Epoch [24], Iter [91/101] Loss: 0.4587\n",
      "Test MSE: 0.6155632138252258\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.9144\n",
      "Epoch [2], Iter [91/101] Loss: 0.9166\n",
      "Epoch [3], Iter [91/101] Loss: 0.7126\n",
      "Epoch [4], Iter [91/101] Loss: 0.6395\n",
      "Epoch [5], Iter [91/101] Loss: 0.6988\n",
      "Epoch [6], Iter [91/101] Loss: 0.6102\n",
      "Epoch [7], Iter [91/101] Loss: 0.6074\n",
      "Epoch [8], Iter [91/101] Loss: 0.6011\n",
      "Epoch [9], Iter [91/101] Loss: 0.5752\n",
      "Epoch [10], Iter [91/101] Loss: 0.5669\n",
      "Epoch [11], Iter [91/101] Loss: 0.5472\n",
      "Epoch [12], Iter [91/101] Loss: 0.5398\n",
      "Epoch [13], Iter [91/101] Loss: 0.5188\n",
      "Epoch [14], Iter [91/101] Loss: 0.5350\n",
      "Epoch [15], Iter [91/101] Loss: 0.5015\n",
      "Epoch [16], Iter [91/101] Loss: 0.5141\n",
      "Epoch [17], Iter [91/101] Loss: 0.5115\n",
      "Epoch [18], Iter [91/101] Loss: 0.5410\n",
      "Epoch [19], Iter [91/101] Loss: 0.4727\n",
      "Epoch [20], Iter [91/101] Loss: 0.4785\n",
      "Epoch [21], Iter [91/101] Loss: 0.4339\n",
      "Test MSE: 0.640643298625946\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0c94d",
   "metadata": {},
   "source": [
    "## 4.2.4 Hyperparameter tuning\n",
    "\n",
    "Tune the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3397989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.logspace(-4,-1,20)\n",
    "runs_per_lr = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f62766",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6642\n",
      "Epoch [2], Iter [91/101] Loss: 0.6228\n",
      "Epoch [3], Iter [91/101] Loss: 0.5527\n",
      "Epoch [4], Iter [91/101] Loss: 0.5784\n",
      "Epoch [5], Iter [91/101] Loss: 0.5651\n",
      "Epoch [6], Iter [91/101] Loss: 0.5615\n",
      "Epoch [7], Iter [91/101] Loss: 0.5509\n",
      "Epoch [8], Iter [91/101] Loss: 0.5918\n",
      "Epoch [9], Iter [91/101] Loss: 0.5837\n",
      "Epoch [10], Iter [91/101] Loss: 0.5024\n",
      "Epoch [11], Iter [91/101] Loss: 0.5093\n",
      "Epoch [12], Iter [91/101] Loss: 0.5549\n",
      "Epoch [13], Iter [91/101] Loss: 0.5004\n",
      "Epoch [14], Iter [91/101] Loss: 0.4949\n",
      "Epoch [15], Iter [91/101] Loss: 0.4841\n",
      "Test MSE: 0.6431035399436951\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6370\n",
      "Epoch [2], Iter [91/101] Loss: 0.6160\n",
      "Epoch [3], Iter [91/101] Loss: 0.5846\n",
      "Epoch [4], Iter [91/101] Loss: 0.5406\n",
      "Epoch [5], Iter [91/101] Loss: 0.5908\n",
      "Epoch [6], Iter [91/101] Loss: 0.5682\n",
      "Epoch [7], Iter [91/101] Loss: 0.5513\n",
      "Epoch [8], Iter [91/101] Loss: 0.5242\n",
      "Epoch [9], Iter [91/101] Loss: 0.5628\n",
      "Epoch [10], Iter [91/101] Loss: 0.4883\n",
      "Epoch [11], Iter [91/101] Loss: 0.5202\n",
      "Epoch [12], Iter [91/101] Loss: 0.5080\n",
      "Epoch [13], Iter [91/101] Loss: 0.5111\n",
      "Epoch [14], Iter [91/101] Loss: 0.5084\n",
      "Epoch [15], Iter [91/101] Loss: 0.4902\n",
      "Epoch [16], Iter [91/101] Loss: 0.4909\n",
      "Epoch [17], Iter [91/101] Loss: 0.4736\n",
      "Epoch [18], Iter [91/101] Loss: 0.4800\n",
      "Epoch [19], Iter [91/101] Loss: 0.4572\n",
      "Epoch [20], Iter [91/101] Loss: 0.4816\n",
      "Epoch [21], Iter [91/101] Loss: 0.4932\n",
      "Epoch [22], Iter [91/101] Loss: 0.4599\n",
      "Test MSE: 0.5965799689292908\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6603\n",
      "Epoch [2], Iter [91/101] Loss: 0.6015\n",
      "Epoch [3], Iter [91/101] Loss: 0.5682\n",
      "Epoch [4], Iter [91/101] Loss: 0.5606\n",
      "Epoch [5], Iter [91/101] Loss: 0.5514\n",
      "Epoch [6], Iter [91/101] Loss: 0.5582\n",
      "Epoch [7], Iter [91/101] Loss: 0.5403\n",
      "Epoch [8], Iter [91/101] Loss: 0.6165\n",
      "Epoch [9], Iter [91/101] Loss: 0.5109\n",
      "Epoch [10], Iter [91/101] Loss: 0.5177\n",
      "Epoch [11], Iter [91/101] Loss: 0.5165\n",
      "Epoch [12], Iter [91/101] Loss: 0.4950\n",
      "Epoch [13], Iter [91/101] Loss: 0.5010\n",
      "Epoch [14], Iter [91/101] Loss: 0.5104\n",
      "Epoch [15], Iter [91/101] Loss: 0.4916\n",
      "Epoch [16], Iter [91/101] Loss: 0.4827\n",
      "Epoch [17], Iter [91/101] Loss: 0.5656\n",
      "Epoch [18], Iter [91/101] Loss: 0.5239\n",
      "Epoch [19], Iter [91/101] Loss: 0.4839\n",
      "Epoch [20], Iter [91/101] Loss: 0.4715\n",
      "Test MSE: 0.6348835825920105\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6012\n",
      "Epoch [2], Iter [91/101] Loss: 0.6249\n",
      "Epoch [3], Iter [91/101] Loss: 0.6235\n",
      "Epoch [4], Iter [91/101] Loss: 0.5573\n",
      "Epoch [5], Iter [91/101] Loss: 0.5673\n",
      "Epoch [6], Iter [91/101] Loss: 0.5873\n",
      "Epoch [7], Iter [91/101] Loss: 0.5367\n",
      "Epoch [8], Iter [91/101] Loss: 0.5437\n",
      "Epoch [9], Iter [91/101] Loss: 0.4989\n",
      "Epoch [10], Iter [91/101] Loss: 0.5237\n",
      "Epoch [11], Iter [91/101] Loss: 0.4880\n",
      "Epoch [12], Iter [91/101] Loss: 0.5362\n",
      "Epoch [13], Iter [91/101] Loss: 0.4964\n",
      "Epoch [14], Iter [91/101] Loss: 0.4804\n",
      "Epoch [15], Iter [91/101] Loss: 0.5388\n",
      "Test MSE: 0.6251792311668396\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6564\n",
      "Epoch [2], Iter [91/101] Loss: 0.6123\n",
      "Epoch [3], Iter [91/101] Loss: 0.5777\n",
      "Epoch [4], Iter [91/101] Loss: 0.6242\n",
      "Epoch [5], Iter [91/101] Loss: 0.5459\n",
      "Epoch [6], Iter [91/101] Loss: 0.5516\n",
      "Epoch [7], Iter [91/101] Loss: 0.5567\n",
      "Epoch [8], Iter [91/101] Loss: 0.5386\n",
      "Epoch [9], Iter [91/101] Loss: 0.5335\n",
      "Epoch [10], Iter [91/101] Loss: 0.5211\n",
      "Epoch [11], Iter [91/101] Loss: 0.5176\n",
      "Epoch [12], Iter [91/101] Loss: 0.5269\n",
      "Epoch [13], Iter [91/101] Loss: 0.4894\n",
      "Epoch [14], Iter [91/101] Loss: 0.5605\n",
      "Epoch [15], Iter [91/101] Loss: 0.4646\n",
      "Epoch [16], Iter [91/101] Loss: 0.4895\n",
      "Epoch [17], Iter [91/101] Loss: 0.4913\n",
      "Epoch [18], Iter [91/101] Loss: 0.4928\n",
      "Epoch [19], Iter [91/101] Loss: 0.4669\n",
      "Epoch [20], Iter [91/101] Loss: 0.4562\n",
      "Epoch [21], Iter [91/101] Loss: 0.4807\n",
      "Epoch [22], Iter [91/101] Loss: 0.4482\n",
      "Epoch [23], Iter [91/101] Loss: 0.4329\n",
      "Epoch [24], Iter [91/101] Loss: 0.4246\n",
      "Test MSE: 0.6069739460945129\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7636\n",
      "Epoch [2], Iter [91/101] Loss: 0.5915\n",
      "Epoch [3], Iter [91/101] Loss: 0.5854\n",
      "Epoch [4], Iter [91/101] Loss: 0.5288\n",
      "Epoch [5], Iter [91/101] Loss: 0.5613\n",
      "Epoch [6], Iter [91/101] Loss: 0.5542\n",
      "Epoch [7], Iter [91/101] Loss: 0.5497\n",
      "Epoch [8], Iter [91/101] Loss: 0.5130\n",
      "Epoch [9], Iter [91/101] Loss: 0.5251\n",
      "Epoch [10], Iter [91/101] Loss: 0.5099\n",
      "Epoch [11], Iter [91/101] Loss: 0.5050\n",
      "Epoch [12], Iter [91/101] Loss: 0.5705\n",
      "Epoch [13], Iter [91/101] Loss: 0.5100\n",
      "Epoch [14], Iter [91/101] Loss: 0.4887\n",
      "Epoch [15], Iter [91/101] Loss: 0.4951\n",
      "Epoch [16], Iter [91/101] Loss: 0.4808\n",
      "Epoch [17], Iter [91/101] Loss: 0.4780\n",
      "Epoch [18], Iter [91/101] Loss: 0.5219\n",
      "Epoch [19], Iter [91/101] Loss: 0.4746\n",
      "Epoch [20], Iter [91/101] Loss: 0.4469\n",
      "Epoch [21], Iter [91/101] Loss: 0.4523\n",
      "Epoch [22], Iter [91/101] Loss: 0.4506\n",
      "Epoch [23], Iter [91/101] Loss: 0.4454\n",
      "Epoch [24], Iter [91/101] Loss: 0.4488\n",
      "Test MSE: 0.6111662983894348\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6499\n",
      "Epoch [2], Iter [91/101] Loss: 0.5815\n",
      "Epoch [3], Iter [91/101] Loss: 0.5810\n",
      "Epoch [4], Iter [91/101] Loss: 0.5843\n",
      "Epoch [5], Iter [91/101] Loss: 0.5671\n",
      "Epoch [6], Iter [91/101] Loss: 0.5445\n",
      "Epoch [7], Iter [91/101] Loss: 0.5552\n",
      "Epoch [8], Iter [91/101] Loss: 0.5493\n",
      "Epoch [9], Iter [91/101] Loss: 0.5060\n",
      "Epoch [10], Iter [91/101] Loss: 0.5334\n",
      "Epoch [11], Iter [91/101] Loss: 0.5199\n",
      "Epoch [12], Iter [91/101] Loss: 0.5101\n",
      "Epoch [13], Iter [91/101] Loss: 0.4786\n",
      "Epoch [14], Iter [91/101] Loss: 0.4988\n",
      "Epoch [15], Iter [91/101] Loss: 0.4931\n",
      "Epoch [16], Iter [91/101] Loss: 0.5509\n",
      "Epoch [17], Iter [91/101] Loss: 0.4708\n",
      "Epoch [18], Iter [91/101] Loss: 0.4624\n",
      "Epoch [19], Iter [91/101] Loss: 0.4741\n",
      "Epoch [20], Iter [91/101] Loss: 0.4739\n",
      "Epoch [21], Iter [91/101] Loss: 0.5380\n",
      "Test MSE: 0.6182892322540283\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6127\n",
      "Epoch [2], Iter [91/101] Loss: 0.6012\n",
      "Epoch [3], Iter [91/101] Loss: 0.5679\n",
      "Epoch [4], Iter [91/101] Loss: 0.5457\n",
      "Epoch [5], Iter [91/101] Loss: 0.5668\n",
      "Epoch [6], Iter [91/101] Loss: 0.5666\n",
      "Epoch [7], Iter [91/101] Loss: 0.5306\n",
      "Epoch [8], Iter [91/101] Loss: 0.5498\n",
      "Epoch [9], Iter [91/101] Loss: 0.5092\n",
      "Epoch [10], Iter [91/101] Loss: 0.5367\n",
      "Epoch [11], Iter [91/101] Loss: 0.4998\n",
      "Epoch [12], Iter [91/101] Loss: 0.4908\n",
      "Epoch [13], Iter [91/101] Loss: 0.4829\n",
      "Test MSE: 0.6498109102249146\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7758\n",
      "Epoch [2], Iter [91/101] Loss: 0.6247\n",
      "Epoch [3], Iter [91/101] Loss: 0.8387\n",
      "Epoch [4], Iter [91/101] Loss: 0.5790\n",
      "Epoch [5], Iter [91/101] Loss: 0.5524\n",
      "Epoch [6], Iter [91/101] Loss: 0.5277\n",
      "Epoch [7], Iter [91/101] Loss: 0.5384\n",
      "Epoch [8], Iter [91/101] Loss: 0.5299\n",
      "Epoch [9], Iter [91/101] Loss: 0.5224\n",
      "Epoch [10], Iter [91/101] Loss: 0.5146\n",
      "Epoch [11], Iter [91/101] Loss: 0.4866\n",
      "Epoch [12], Iter [91/101] Loss: 0.5009\n",
      "Epoch [13], Iter [91/101] Loss: 0.4984\n",
      "Epoch [14], Iter [91/101] Loss: 0.4809\n",
      "Epoch [15], Iter [91/101] Loss: 0.4895\n",
      "Epoch [16], Iter [91/101] Loss: 0.4759\n",
      "Epoch [17], Iter [91/101] Loss: 0.4869\n",
      "Epoch [18], Iter [91/101] Loss: 0.4640\n",
      "Epoch [19], Iter [91/101] Loss: 0.4681\n",
      "Epoch [20], Iter [91/101] Loss: 0.4551\n",
      "Epoch [21], Iter [91/101] Loss: 0.4382\n",
      "Epoch [22], Iter [91/101] Loss: 0.4403\n",
      "Epoch [23], Iter [91/101] Loss: 0.4471\n",
      "Epoch [24], Iter [91/101] Loss: 0.4281\n",
      "Test MSE: 0.6262602210044861\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6611\n",
      "Epoch [2], Iter [91/101] Loss: 0.6036\n",
      "Epoch [3], Iter [91/101] Loss: 0.5951\n",
      "Epoch [4], Iter [91/101] Loss: 0.5535\n",
      "Epoch [5], Iter [91/101] Loss: 0.5947\n",
      "Epoch [6], Iter [91/101] Loss: 0.5316\n",
      "Epoch [7], Iter [91/101] Loss: 0.5922\n",
      "Epoch [8], Iter [91/101] Loss: 0.5472\n",
      "Epoch [9], Iter [91/101] Loss: 0.5046\n",
      "Epoch [10], Iter [91/101] Loss: 0.5093\n",
      "Epoch [11], Iter [91/101] Loss: 0.4960\n",
      "Epoch [12], Iter [91/101] Loss: 1.1149\n",
      "Epoch [13], Iter [91/101] Loss: 0.5041\n",
      "Epoch [14], Iter [91/101] Loss: 0.4890\n",
      "Epoch [15], Iter [91/101] Loss: 0.4858\n",
      "Epoch [16], Iter [91/101] Loss: 0.5039\n",
      "Epoch [17], Iter [91/101] Loss: 0.5290\n",
      "Epoch [18], Iter [91/101] Loss: 0.4912\n",
      "Epoch [19], Iter [91/101] Loss: 0.9823\n",
      "Epoch [20], Iter [91/101] Loss: 0.4835\n",
      "Epoch [21], Iter [91/101] Loss: 0.4523\n",
      "Epoch [22], Iter [91/101] Loss: 0.4669\n",
      "Epoch [23], Iter [91/101] Loss: 0.4355\n",
      "Test MSE: 0.6157366633415222\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6260\n",
      "Epoch [2], Iter [91/101] Loss: 0.6173\n",
      "Epoch [3], Iter [91/101] Loss: 0.5743\n",
      "Epoch [4], Iter [91/101] Loss: 0.6052\n",
      "Epoch [5], Iter [91/101] Loss: 0.5493\n",
      "Epoch [6], Iter [91/101] Loss: 0.5419\n",
      "Epoch [7], Iter [91/101] Loss: 0.5477\n",
      "Epoch [8], Iter [91/101] Loss: 0.5296\n",
      "Epoch [9], Iter [91/101] Loss: 0.5309\n",
      "Epoch [10], Iter [91/101] Loss: 0.5235\n",
      "Epoch [11], Iter [91/101] Loss: 1.1165\n",
      "Epoch [12], Iter [91/101] Loss: 0.5030\n",
      "Epoch [13], Iter [91/101] Loss: 0.5236\n",
      "Epoch [14], Iter [91/101] Loss: 0.4973\n",
      "Epoch [15], Iter [91/101] Loss: 0.4961\n",
      "Epoch [16], Iter [91/101] Loss: 0.4893\n",
      "Epoch [17], Iter [91/101] Loss: 0.4770\n",
      "Epoch [18], Iter [91/101] Loss: 0.4791\n",
      "Epoch [19], Iter [91/101] Loss: 0.4746\n",
      "Epoch [20], Iter [91/101] Loss: 0.4534\n",
      "Epoch [21], Iter [91/101] Loss: 0.4728\n",
      "Epoch [22], Iter [91/101] Loss: 0.4390\n",
      "Test MSE: 0.5977613925933838\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6205\n",
      "Epoch [2], Iter [91/101] Loss: 0.6254\n",
      "Epoch [3], Iter [91/101] Loss: 0.5973\n",
      "Epoch [4], Iter [91/101] Loss: 0.5631\n",
      "Epoch [5], Iter [91/101] Loss: 0.5616\n",
      "Epoch [6], Iter [91/101] Loss: 0.5407\n",
      "Epoch [7], Iter [91/101] Loss: 0.5076\n",
      "Epoch [8], Iter [91/101] Loss: 0.5654\n",
      "Epoch [9], Iter [91/101] Loss: 0.5411\n",
      "Epoch [10], Iter [91/101] Loss: 0.5205\n",
      "Epoch [11], Iter [91/101] Loss: 0.5343\n",
      "Epoch [12], Iter [91/101] Loss: 0.5053\n",
      "Epoch [13], Iter [91/101] Loss: 0.5164\n",
      "Epoch [14], Iter [91/101] Loss: 0.4888\n",
      "Epoch [15], Iter [91/101] Loss: 0.4881\n",
      "Epoch [16], Iter [91/101] Loss: 0.4817\n",
      "Epoch [17], Iter [91/101] Loss: 0.5170\n",
      "Epoch [18], Iter [91/101] Loss: 0.5381\n",
      "Epoch [19], Iter [91/101] Loss: 0.5004\n",
      "Epoch [20], Iter [91/101] Loss: 0.4691\n",
      "Epoch [21], Iter [91/101] Loss: 0.4547\n",
      "Epoch [22], Iter [91/101] Loss: 0.4667\n",
      "Epoch [23], Iter [91/101] Loss: 0.4264\n",
      "Test MSE: 0.617375373840332\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6486\n",
      "Epoch [2], Iter [91/101] Loss: 0.6203\n",
      "Epoch [3], Iter [91/101] Loss: 0.5790\n",
      "Epoch [4], Iter [91/101] Loss: 0.5682\n",
      "Epoch [5], Iter [91/101] Loss: 0.5303\n",
      "Epoch [6], Iter [91/101] Loss: 0.5698\n",
      "Epoch [7], Iter [91/101] Loss: 0.5232\n",
      "Epoch [8], Iter [91/101] Loss: 0.5244\n",
      "Epoch [9], Iter [91/101] Loss: 0.5442\n",
      "Epoch [10], Iter [91/101] Loss: 0.5708\n",
      "Epoch [11], Iter [91/101] Loss: 0.5020\n",
      "Epoch [12], Iter [91/101] Loss: 0.5209\n",
      "Epoch [13], Iter [91/101] Loss: 0.5093\n",
      "Epoch [14], Iter [91/101] Loss: 0.4954\n",
      "Epoch [15], Iter [91/101] Loss: 0.4984\n",
      "Epoch [16], Iter [91/101] Loss: 0.4693\n",
      "Epoch [17], Iter [91/101] Loss: 0.4701\n",
      "Epoch [18], Iter [91/101] Loss: 0.4661\n",
      "Epoch [19], Iter [91/101] Loss: 0.4694\n",
      "Test MSE: 0.6593647003173828\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6222\n",
      "Epoch [2], Iter [91/101] Loss: 0.5996\n",
      "Epoch [3], Iter [91/101] Loss: 0.6167\n",
      "Epoch [4], Iter [91/101] Loss: 0.5773\n",
      "Epoch [5], Iter [91/101] Loss: 0.5504\n",
      "Epoch [6], Iter [91/101] Loss: 0.5525\n",
      "Epoch [7], Iter [91/101] Loss: 0.5476\n",
      "Epoch [8], Iter [91/101] Loss: 0.5381\n",
      "Epoch [9], Iter [91/101] Loss: 0.5323\n",
      "Epoch [10], Iter [91/101] Loss: 0.5035\n",
      "Epoch [11], Iter [91/101] Loss: 0.5415\n",
      "Epoch [12], Iter [91/101] Loss: 0.5551\n",
      "Epoch [13], Iter [91/101] Loss: 0.4997\n",
      "Epoch [14], Iter [91/101] Loss: 0.4930\n",
      "Epoch [15], Iter [91/101] Loss: 0.4984\n",
      "Epoch [16], Iter [91/101] Loss: 0.5038\n",
      "Epoch [17], Iter [91/101] Loss: 0.5042\n",
      "Epoch [18], Iter [91/101] Loss: 0.5025\n",
      "Test MSE: 0.612114429473877\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6697\n",
      "Epoch [2], Iter [91/101] Loss: 0.6128\n",
      "Epoch [3], Iter [91/101] Loss: 0.5927\n",
      "Epoch [4], Iter [91/101] Loss: 0.5562\n",
      "Epoch [5], Iter [91/101] Loss: 0.5599\n",
      "Epoch [6], Iter [91/101] Loss: 0.5470\n",
      "Epoch [7], Iter [91/101] Loss: 0.5302\n",
      "Epoch [8], Iter [91/101] Loss: 0.5276\n",
      "Epoch [9], Iter [91/101] Loss: 0.5630\n",
      "Epoch [10], Iter [91/101] Loss: 0.5142\n",
      "Epoch [11], Iter [91/101] Loss: 0.5012\n",
      "Epoch [12], Iter [91/101] Loss: 0.5294\n",
      "Epoch [13], Iter [91/101] Loss: 0.4924\n",
      "Epoch [14], Iter [91/101] Loss: 0.5106\n",
      "Epoch [15], Iter [91/101] Loss: 0.4753\n",
      "Epoch [16], Iter [91/101] Loss: 0.5035\n",
      "Epoch [17], Iter [91/101] Loss: 0.4805\n",
      "Epoch [18], Iter [91/101] Loss: 0.4920\n",
      "Epoch [19], Iter [91/101] Loss: 0.4631\n",
      "Epoch [20], Iter [91/101] Loss: 0.4494\n",
      "Epoch [21], Iter [91/101] Loss: 0.4706\n",
      "Epoch [22], Iter [91/101] Loss: 0.4526\n",
      "Test MSE: 0.6267555952072144\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6244\n",
      "Epoch [2], Iter [91/101] Loss: 0.6117\n",
      "Epoch [3], Iter [91/101] Loss: 0.5795\n",
      "Epoch [4], Iter [91/101] Loss: 0.5546\n",
      "Epoch [5], Iter [91/101] Loss: 0.5645\n",
      "Epoch [6], Iter [91/101] Loss: 0.5599\n",
      "Epoch [7], Iter [91/101] Loss: 0.5202\n",
      "Epoch [8], Iter [91/101] Loss: 0.5499\n",
      "Epoch [9], Iter [91/101] Loss: 0.5548\n",
      "Epoch [10], Iter [91/101] Loss: 0.5105\n",
      "Epoch [11], Iter [91/101] Loss: 0.4968\n",
      "Epoch [12], Iter [91/101] Loss: 0.5341\n",
      "Epoch [13], Iter [91/101] Loss: 0.4979\n",
      "Epoch [14], Iter [91/101] Loss: 0.5175\n",
      "Epoch [15], Iter [91/101] Loss: 0.4827\n",
      "Epoch [16], Iter [91/101] Loss: 0.4905\n",
      "Epoch [17], Iter [91/101] Loss: 0.4922\n",
      "Epoch [18], Iter [91/101] Loss: 0.4490\n",
      "Epoch [19], Iter [91/101] Loss: 0.4591\n",
      "Epoch [20], Iter [91/101] Loss: 0.4525\n",
      "Test MSE: 0.6147188544273376\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6478\n",
      "Epoch [2], Iter [91/101] Loss: 0.5793\n",
      "Epoch [3], Iter [91/101] Loss: 0.6155\n",
      "Epoch [4], Iter [91/101] Loss: 0.5594\n",
      "Epoch [5], Iter [91/101] Loss: 0.5967\n",
      "Epoch [6], Iter [91/101] Loss: 0.5309\n",
      "Epoch [7], Iter [91/101] Loss: 0.5373\n",
      "Epoch [8], Iter [91/101] Loss: 0.5459\n",
      "Epoch [9], Iter [91/101] Loss: 0.5471\n",
      "Epoch [10], Iter [91/101] Loss: 0.5109\n",
      "Epoch [11], Iter [91/101] Loss: 0.5082\n",
      "Epoch [12], Iter [91/101] Loss: 0.5271\n",
      "Epoch [13], Iter [91/101] Loss: 0.5153\n",
      "Epoch [14], Iter [91/101] Loss: 0.4905\n",
      "Epoch [15], Iter [91/101] Loss: 0.5184\n",
      "Test MSE: 0.6204400062561035\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6247\n",
      "Epoch [2], Iter [91/101] Loss: 0.6128\n",
      "Epoch [3], Iter [91/101] Loss: 0.5855\n",
      "Epoch [4], Iter [91/101] Loss: 0.5482\n",
      "Epoch [5], Iter [91/101] Loss: 0.5613\n",
      "Epoch [6], Iter [91/101] Loss: 0.5425\n",
      "Epoch [7], Iter [91/101] Loss: 0.5634\n",
      "Epoch [8], Iter [91/101] Loss: 0.5244\n",
      "Epoch [9], Iter [91/101] Loss: 0.5472\n",
      "Epoch [10], Iter [91/101] Loss: 0.5258\n",
      "Epoch [11], Iter [91/101] Loss: 0.5378\n",
      "Epoch [12], Iter [91/101] Loss: 0.5088\n",
      "Epoch [13], Iter [91/101] Loss: 0.5058\n",
      "Epoch [14], Iter [91/101] Loss: 0.5183\n",
      "Epoch [15], Iter [91/101] Loss: 0.5078\n",
      "Epoch [16], Iter [91/101] Loss: 0.4720\n",
      "Epoch [17], Iter [91/101] Loss: 0.4766\n",
      "Epoch [18], Iter [91/101] Loss: 0.4871\n",
      "Epoch [19], Iter [91/101] Loss: 0.4654\n",
      "Epoch [20], Iter [91/101] Loss: 0.4670\n",
      "Test MSE: 0.6165999174118042\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6533\n",
      "Epoch [2], Iter [91/101] Loss: 0.6134\n",
      "Epoch [3], Iter [91/101] Loss: 0.5812\n",
      "Epoch [4], Iter [91/101] Loss: 0.5536\n",
      "Epoch [5], Iter [91/101] Loss: 0.5609\n",
      "Epoch [6], Iter [91/101] Loss: 0.5500\n",
      "Epoch [7], Iter [91/101] Loss: 0.5587\n",
      "Epoch [8], Iter [91/101] Loss: 0.5631\n",
      "Epoch [9], Iter [91/101] Loss: 0.5517\n",
      "Epoch [10], Iter [91/101] Loss: 0.5165\n",
      "Epoch [11], Iter [91/101] Loss: 0.5797\n",
      "Epoch [12], Iter [91/101] Loss: 0.5249\n",
      "Epoch [13], Iter [91/101] Loss: 0.5013\n",
      "Epoch [14], Iter [91/101] Loss: 0.4968\n",
      "Epoch [15], Iter [91/101] Loss: 0.5278\n",
      "Test MSE: 0.6332415342330933\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6328\n",
      "Epoch [2], Iter [91/101] Loss: 0.6054\n",
      "Epoch [3], Iter [91/101] Loss: 0.6151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Iter [91/101] Loss: 0.5674\n",
      "Epoch [5], Iter [91/101] Loss: 0.5360\n",
      "Epoch [6], Iter [91/101] Loss: 0.5622\n",
      "Epoch [7], Iter [91/101] Loss: 0.5279\n",
      "Epoch [8], Iter [91/101] Loss: 0.5259\n",
      "Epoch [9], Iter [91/101] Loss: 0.5437\n",
      "Epoch [10], Iter [91/101] Loss: 0.5486\n",
      "Epoch [11], Iter [91/101] Loss: 0.5335\n",
      "Epoch [12], Iter [91/101] Loss: 0.4967\n",
      "Epoch [13], Iter [91/101] Loss: 0.4795\n",
      "Epoch [14], Iter [91/101] Loss: 0.4933\n",
      "Epoch [15], Iter [91/101] Loss: 0.4677\n",
      "Epoch [16], Iter [91/101] Loss: 0.5052\n",
      "Epoch [17], Iter [91/101] Loss: 0.4858\n",
      "Epoch [18], Iter [91/101] Loss: 0.4606\n",
      "Epoch [19], Iter [91/101] Loss: 0.4624\n",
      "Epoch [20], Iter [91/101] Loss: 0.4581\n",
      "Epoch [21], Iter [91/101] Loss: 0.4642\n",
      "Test MSE: 0.6130402088165283\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6540\n",
      "Epoch [2], Iter [91/101] Loss: 0.6132\n",
      "Epoch [3], Iter [91/101] Loss: 0.6634\n",
      "Epoch [4], Iter [91/101] Loss: 0.5856\n",
      "Epoch [5], Iter [91/101] Loss: 0.5700\n",
      "Epoch [6], Iter [91/101] Loss: 0.5230\n",
      "Epoch [7], Iter [91/101] Loss: 0.5194\n",
      "Epoch [8], Iter [91/101] Loss: 0.5401\n",
      "Epoch [9], Iter [91/101] Loss: 0.5191\n",
      "Epoch [10], Iter [91/101] Loss: 0.5408\n",
      "Epoch [11], Iter [91/101] Loss: 0.5081\n",
      "Epoch [12], Iter [91/101] Loss: 0.5301\n",
      "Epoch [13], Iter [91/101] Loss: 0.5004\n",
      "Epoch [14], Iter [91/101] Loss: 0.5202\n",
      "Epoch [15], Iter [91/101] Loss: 0.4870\n",
      "Epoch [16], Iter [91/101] Loss: 0.4898\n",
      "Epoch [17], Iter [91/101] Loss: 0.4920\n",
      "Epoch [18], Iter [91/101] Loss: 0.4647\n",
      "Epoch [19], Iter [91/101] Loss: 0.4512\n",
      "Epoch [20], Iter [91/101] Loss: 0.4457\n",
      "Test MSE: 0.6222957968711853\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6314\n",
      "Epoch [2], Iter [91/101] Loss: 0.5991\n",
      "Epoch [3], Iter [91/101] Loss: 0.6010\n",
      "Epoch [4], Iter [91/101] Loss: 0.5583\n",
      "Epoch [5], Iter [91/101] Loss: 0.5420\n",
      "Epoch [6], Iter [91/101] Loss: 0.5567\n",
      "Epoch [7], Iter [91/101] Loss: 0.5156\n",
      "Epoch [8], Iter [91/101] Loss: 0.5464\n",
      "Epoch [9], Iter [91/101] Loss: 0.5888\n",
      "Epoch [10], Iter [91/101] Loss: 0.5172\n",
      "Epoch [11], Iter [91/101] Loss: 0.5173\n",
      "Epoch [12], Iter [91/101] Loss: 0.5768\n",
      "Epoch [13], Iter [91/101] Loss: 0.5109\n",
      "Epoch [14], Iter [91/101] Loss: 0.4796\n",
      "Epoch [15], Iter [91/101] Loss: 0.5075\n",
      "Epoch [16], Iter [91/101] Loss: 0.4981\n",
      "Epoch [17], Iter [91/101] Loss: 0.4816\n",
      "Epoch [18], Iter [91/101] Loss: 0.4638\n",
      "Epoch [19], Iter [91/101] Loss: 0.4718\n",
      "Epoch [20], Iter [91/101] Loss: 0.4378\n",
      "Test MSE: 0.6123600006103516\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6020\n",
      "Epoch [2], Iter [91/101] Loss: 0.6193\n",
      "Epoch [3], Iter [91/101] Loss: 0.6049\n",
      "Epoch [4], Iter [91/101] Loss: 0.5694\n",
      "Epoch [5], Iter [91/101] Loss: 0.5513\n",
      "Epoch [6], Iter [91/101] Loss: 0.5684\n",
      "Epoch [7], Iter [91/101] Loss: 0.5490\n",
      "Epoch [8], Iter [91/101] Loss: 0.5381\n",
      "Epoch [9], Iter [91/101] Loss: 0.5451\n",
      "Epoch [10], Iter [91/101] Loss: 0.6355\n",
      "Epoch [11], Iter [91/101] Loss: 0.5091\n",
      "Epoch [12], Iter [91/101] Loss: 0.5073\n",
      "Epoch [13], Iter [91/101] Loss: 0.5023\n",
      "Epoch [14], Iter [91/101] Loss: 0.5041\n",
      "Epoch [15], Iter [91/101] Loss: 0.4868\n",
      "Epoch [16], Iter [91/101] Loss: 0.4880\n",
      "Epoch [17], Iter [91/101] Loss: 0.4933\n",
      "Epoch [18], Iter [91/101] Loss: 0.4677\n",
      "Epoch [19], Iter [91/101] Loss: 0.4602\n",
      "Epoch [20], Iter [91/101] Loss: 0.5178\n",
      "Epoch [21], Iter [91/101] Loss: 0.4666\n",
      "Epoch [22], Iter [91/101] Loss: 0.4444\n",
      "Epoch [23], Iter [91/101] Loss: 0.4450\n",
      "Epoch [24], Iter [91/101] Loss: 0.4396\n",
      "Epoch [25], Iter [91/101] Loss: 0.4396\n",
      "Test MSE: 0.6143922209739685\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6263\n",
      "Epoch [2], Iter [91/101] Loss: 0.6876\n",
      "Epoch [3], Iter [91/101] Loss: 0.5862\n",
      "Epoch [4], Iter [91/101] Loss: 0.5596\n",
      "Epoch [5], Iter [91/101] Loss: 0.5712\n",
      "Epoch [6], Iter [91/101] Loss: 0.5567\n",
      "Epoch [7], Iter [91/101] Loss: 0.5154\n",
      "Epoch [8], Iter [91/101] Loss: 0.5229\n",
      "Epoch [9], Iter [91/101] Loss: 0.5099\n",
      "Epoch [10], Iter [91/101] Loss: 0.5053\n",
      "Epoch [11], Iter [91/101] Loss: 0.5151\n",
      "Epoch [12], Iter [91/101] Loss: 0.5054\n",
      "Epoch [13], Iter [91/101] Loss: 0.5116\n",
      "Epoch [14], Iter [91/101] Loss: 0.5002\n",
      "Epoch [15], Iter [91/101] Loss: 0.4988\n",
      "Epoch [16], Iter [91/101] Loss: 0.4950\n",
      "Epoch [17], Iter [91/101] Loss: 0.4857\n",
      "Epoch [18], Iter [91/101] Loss: 0.5100\n",
      "Epoch [19], Iter [91/101] Loss: 0.4600\n",
      "Epoch [20], Iter [91/101] Loss: 0.4471\n",
      "Epoch [21], Iter [91/101] Loss: 0.4794\n",
      "Epoch [22], Iter [91/101] Loss: 0.4902\n",
      "Epoch [23], Iter [91/101] Loss: 0.4652\n",
      "Epoch [24], Iter [91/101] Loss: 0.4467\n",
      "Test MSE: 0.6272680163383484\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6444\n",
      "Epoch [2], Iter [91/101] Loss: 0.5923\n",
      "Epoch [3], Iter [91/101] Loss: 0.7119\n",
      "Epoch [4], Iter [91/101] Loss: 0.5467\n",
      "Epoch [5], Iter [91/101] Loss: 0.5460\n",
      "Epoch [6], Iter [91/101] Loss: 0.5476\n",
      "Epoch [7], Iter [91/101] Loss: 0.5569\n",
      "Epoch [8], Iter [91/101] Loss: 0.5460\n",
      "Epoch [9], Iter [91/101] Loss: 0.5287\n",
      "Epoch [10], Iter [91/101] Loss: 0.5119\n",
      "Epoch [11], Iter [91/101] Loss: 0.5019\n",
      "Epoch [12], Iter [91/101] Loss: 0.5441\n",
      "Epoch [13], Iter [91/101] Loss: 0.5032\n",
      "Epoch [14], Iter [91/101] Loss: 0.5103\n",
      "Epoch [15], Iter [91/101] Loss: 0.5081\n",
      "Epoch [16], Iter [91/101] Loss: 0.4846\n",
      "Epoch [17], Iter [91/101] Loss: 0.4948\n",
      "Epoch [18], Iter [91/101] Loss: 0.4717\n",
      "Test MSE: 0.6200335621833801\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7447\n",
      "Epoch [2], Iter [91/101] Loss: 0.6025\n",
      "Epoch [3], Iter [91/101] Loss: 0.5783\n",
      "Epoch [4], Iter [91/101] Loss: 0.6668\n",
      "Epoch [5], Iter [91/101] Loss: 0.5507\n",
      "Epoch [6], Iter [91/101] Loss: 0.6495\n",
      "Epoch [7], Iter [91/101] Loss: 0.5389\n",
      "Epoch [8], Iter [91/101] Loss: 0.5080\n",
      "Epoch [9], Iter [91/101] Loss: 0.5310\n",
      "Epoch [10], Iter [91/101] Loss: 0.5190\n",
      "Epoch [11], Iter [91/101] Loss: 0.5019\n",
      "Epoch [12], Iter [91/101] Loss: 0.5033\n",
      "Epoch [13], Iter [91/101] Loss: 0.5494\n",
      "Epoch [14], Iter [91/101] Loss: 0.5084\n",
      "Epoch [15], Iter [91/101] Loss: 0.5181\n",
      "Epoch [16], Iter [91/101] Loss: 0.5007\n",
      "Epoch [17], Iter [91/101] Loss: 0.4641\n",
      "Epoch [18], Iter [91/101] Loss: 0.4715\n",
      "Epoch [19], Iter [91/101] Loss: 0.4583\n",
      "Epoch [20], Iter [91/101] Loss: 0.4508\n",
      "Test MSE: 0.6002222299575806\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6365\n",
      "Epoch [2], Iter [91/101] Loss: 0.6722\n",
      "Epoch [3], Iter [91/101] Loss: 0.5791\n",
      "Epoch [4], Iter [91/101] Loss: 0.5800\n",
      "Epoch [5], Iter [91/101] Loss: 0.6227\n",
      "Epoch [6], Iter [91/101] Loss: 0.5482\n",
      "Epoch [7], Iter [91/101] Loss: 0.6007\n",
      "Epoch [8], Iter [91/101] Loss: 0.5554\n",
      "Epoch [9], Iter [91/101] Loss: 0.5200\n",
      "Epoch [10], Iter [91/101] Loss: 0.5092\n",
      "Epoch [11], Iter [91/101] Loss: 0.5202\n",
      "Epoch [12], Iter [91/101] Loss: 0.4947\n",
      "Epoch [13], Iter [91/101] Loss: 0.5217\n",
      "Epoch [14], Iter [91/101] Loss: 0.5093\n",
      "Epoch [15], Iter [91/101] Loss: 0.5039\n",
      "Epoch [16], Iter [91/101] Loss: 0.4923\n",
      "Epoch [17], Iter [91/101] Loss: 0.4725\n",
      "Epoch [18], Iter [91/101] Loss: 0.4941\n",
      "Epoch [19], Iter [91/101] Loss: 0.4807\n",
      "Epoch [20], Iter [91/101] Loss: 0.4866\n",
      "Test MSE: 0.6296055316925049\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6706\n",
      "Epoch [2], Iter [91/101] Loss: 0.5785\n",
      "Epoch [3], Iter [91/101] Loss: 0.5701\n",
      "Epoch [4], Iter [91/101] Loss: 0.5775\n",
      "Epoch [5], Iter [91/101] Loss: 0.5740\n",
      "Epoch [6], Iter [91/101] Loss: 0.5835\n",
      "Epoch [7], Iter [91/101] Loss: 0.5427\n",
      "Epoch [8], Iter [91/101] Loss: 0.5465\n",
      "Epoch [9], Iter [91/101] Loss: 0.5078\n",
      "Epoch [10], Iter [91/101] Loss: 0.5819\n",
      "Epoch [11], Iter [91/101] Loss: 0.4985\n",
      "Epoch [12], Iter [91/101] Loss: 0.4899\n",
      "Epoch [13], Iter [91/101] Loss: 0.5313\n",
      "Epoch [14], Iter [91/101] Loss: 0.5076\n",
      "Epoch [15], Iter [91/101] Loss: 0.4876\n",
      "Epoch [16], Iter [91/101] Loss: 0.4905\n",
      "Epoch [17], Iter [91/101] Loss: 0.4789\n",
      "Epoch [18], Iter [91/101] Loss: 0.4788\n",
      "Epoch [19], Iter [91/101] Loss: 0.4694\n",
      "Epoch [20], Iter [91/101] Loss: 0.4712\n",
      "Test MSE: 0.6221182346343994\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6190\n",
      "Epoch [2], Iter [91/101] Loss: 0.6307\n",
      "Epoch [3], Iter [1/101] Loss: 0.5787"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    for i in range(runs_per_lr):\n",
    "        model_training_description[\"RUN_NR\"] = i\n",
    "        model_training_description[\"LEARNING_RATE\"] = lr  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "        unet = train_unet(description, model_training_description, output_folder)\n",
    "        predict_save_unet(description, model_training_description, output_folder, unet, output_folder)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e07ce6",
   "metadata": {},
   "source": [
    "### UNet wider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ca1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Global\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 64\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (64,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a096e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6207\n",
      "Epoch [2], Iter [91/101] Loss: 0.6178\n",
      "Epoch [3], Iter [91/101] Loss: 0.6260\n",
      "Epoch [4], Iter [91/101] Loss: 0.5908\n",
      "Epoch [5], Iter [91/101] Loss: 0.5610\n",
      "Epoch [6], Iter [91/101] Loss: 0.5430\n",
      "Epoch [7], Iter [91/101] Loss: 0.5602\n",
      "Epoch [8], Iter [91/101] Loss: 0.5936\n",
      "Epoch [9], Iter [91/101] Loss: 0.5816\n",
      "Epoch [10], Iter [91/101] Loss: 0.5734\n",
      "Epoch [11], Iter [91/101] Loss: 0.5202\n",
      "Epoch [12], Iter [91/101] Loss: 0.5225\n",
      "Epoch [13], Iter [91/101] Loss: 0.4944\n",
      "Epoch [14], Iter [91/101] Loss: 0.4949\n",
      "Epoch [15], Iter [91/101] Loss: 0.4842\n",
      "Epoch [16], Iter [91/101] Loss: 0.4755\n",
      "Epoch [17], Iter [91/101] Loss: 0.4930\n",
      "Epoch [18], Iter [91/101] Loss: 0.4458\n",
      "Epoch [19], Iter [91/101] Loss: 0.4499\n",
      "Epoch [20], Iter [91/101] Loss: 0.4367\n",
      "Epoch [21], Iter [91/101] Loss: 0.4019\n",
      "Test MSE: 0.5897901654243469\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8047\n",
      "Epoch [2], Iter [91/101] Loss: 0.6867\n",
      "Epoch [3], Iter [91/101] Loss: 0.6129\n",
      "Epoch [4], Iter [91/101] Loss: 1.2872\n",
      "Epoch [5], Iter [91/101] Loss: 0.5638\n",
      "Epoch [6], Iter [91/101] Loss: 0.5342\n",
      "Epoch [7], Iter [91/101] Loss: 0.5364\n",
      "Epoch [8], Iter [91/101] Loss: 0.5572\n",
      "Epoch [9], Iter [91/101] Loss: 0.4997\n",
      "Epoch [10], Iter [91/101] Loss: 0.5502\n",
      "Epoch [11], Iter [91/101] Loss: 0.5382\n",
      "Epoch [12], Iter [91/101] Loss: 0.5132\n",
      "Epoch [13], Iter [91/101] Loss: 0.5013\n",
      "Epoch [14], Iter [91/101] Loss: 0.4942\n",
      "Epoch [15], Iter [91/101] Loss: 0.4711\n",
      "Epoch [16], Iter [91/101] Loss: 0.4829\n",
      "Epoch [17], Iter [91/101] Loss: 0.4719\n",
      "Epoch [18], Iter [91/101] Loss: 0.4661\n",
      "Test MSE: 0.618500828742981\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6277\n",
      "Epoch [2], Iter [91/101] Loss: 0.5952\n",
      "Epoch [3], Iter [91/101] Loss: 0.5819\n",
      "Epoch [4], Iter [91/101] Loss: 0.5551\n",
      "Epoch [5], Iter [91/101] Loss: 0.5599\n",
      "Epoch [6], Iter [91/101] Loss: 0.5501\n",
      "Epoch [7], Iter [91/101] Loss: 0.5525\n",
      "Epoch [8], Iter [91/101] Loss: 0.5478\n",
      "Epoch [9], Iter [91/101] Loss: 1.1875\n",
      "Epoch [10], Iter [91/101] Loss: 0.5370\n",
      "Epoch [11], Iter [91/101] Loss: 0.5181\n",
      "Epoch [12], Iter [91/101] Loss: 0.5127\n",
      "Epoch [13], Iter [91/101] Loss: 0.5026\n",
      "Epoch [14], Iter [91/101] Loss: 0.4763\n",
      "Epoch [15], Iter [91/101] Loss: 0.5640\n",
      "Epoch [16], Iter [91/101] Loss: 0.4572\n",
      "Test MSE: 0.5964110493659973\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6438\n",
      "Epoch [2], Iter [91/101] Loss: 0.5778\n",
      "Epoch [3], Iter [91/101] Loss: 0.5907\n",
      "Epoch [4], Iter [91/101] Loss: 0.5610\n",
      "Epoch [5], Iter [91/101] Loss: 0.5807\n",
      "Epoch [6], Iter [91/101] Loss: 0.5564\n",
      "Epoch [7], Iter [91/101] Loss: 0.5413\n",
      "Epoch [8], Iter [91/101] Loss: 0.5297\n",
      "Epoch [9], Iter [91/101] Loss: 0.5225\n",
      "Epoch [10], Iter [91/101] Loss: 0.4774\n",
      "Epoch [11], Iter [91/101] Loss: 0.5879\n",
      "Epoch [12], Iter [91/101] Loss: 0.5650\n",
      "Epoch [13], Iter [91/101] Loss: 0.4856\n",
      "Epoch [14], Iter [91/101] Loss: 0.5026\n",
      "Epoch [15], Iter [91/101] Loss: 0.5070\n",
      "Epoch [16], Iter [91/101] Loss: 0.4662\n",
      "Epoch [17], Iter [91/101] Loss: 0.4665\n",
      "Epoch [18], Iter [91/101] Loss: 0.4501\n",
      "Epoch [19], Iter [91/101] Loss: 0.4354\n",
      "Epoch [20], Iter [91/101] Loss: 0.4295\n",
      "Epoch [21], Iter [91/101] Loss: 0.4285\n",
      "Epoch [22], Iter [91/101] Loss: 0.4128\n",
      "Test MSE: 0.6175037026405334\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d7f23",
   "metadata": {},
   "source": [
    "### UNet deeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed8b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\",\n",
    "                                \"slp\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"],\n",
    "                                      \"slp\": [\"p\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "# training parameters\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 4 # this changes compared to standard UNet\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,128,128)\n",
    "\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dfb85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6509\n",
      "Epoch [2], Iter [91/101] Loss: 0.6202\n",
      "Epoch [3], Iter [91/101] Loss: 0.6386\n",
      "Epoch [4], Iter [91/101] Loss: 0.5396\n",
      "Epoch [5], Iter [91/101] Loss: 0.5562\n",
      "Epoch [6], Iter [91/101] Loss: 0.5562\n",
      "Epoch [7], Iter [91/101] Loss: 0.5151\n",
      "Epoch [8], Iter [91/101] Loss: 0.5440\n",
      "Epoch [9], Iter [91/101] Loss: 0.5852\n",
      "Epoch [10], Iter [91/101] Loss: 0.5315\n",
      "Epoch [11], Iter [91/101] Loss: 0.6047\n",
      "Epoch [12], Iter [91/101] Loss: 0.4912\n",
      "Epoch [13], Iter [91/101] Loss: 0.5016\n",
      "Epoch [14], Iter [91/101] Loss: 0.4977\n",
      "Epoch [15], Iter [91/101] Loss: 0.4906\n",
      "Epoch [16], Iter [91/101] Loss: 0.4956\n",
      "Epoch [17], Iter [91/101] Loss: 0.5354\n",
      "Epoch [18], Iter [91/101] Loss: 0.4716\n",
      "Epoch [19], Iter [91/101] Loss: 0.4529\n",
      "Epoch [20], Iter [91/101] Loss: 0.4577\n",
      "Epoch [21], Iter [91/101] Loss: 0.4471\n",
      "Epoch [22], Iter [91/101] Loss: 0.4941\n",
      "Epoch [23], Iter [91/101] Loss: 0.4153\n",
      "Epoch [24], Iter [91/101] Loss: 0.4186\n",
      "Test MSE: 0.5881031155586243\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6595\n",
      "Epoch [2], Iter [91/101] Loss: 0.6124\n",
      "Epoch [3], Iter [91/101] Loss: 0.5736\n",
      "Epoch [4], Iter [91/101] Loss: 0.5595\n",
      "Epoch [5], Iter [91/101] Loss: 0.5399\n",
      "Epoch [6], Iter [91/101] Loss: 0.5300\n",
      "Epoch [7], Iter [91/101] Loss: 0.5618\n",
      "Epoch [8], Iter [91/101] Loss: 0.5709\n",
      "Epoch [9], Iter [91/101] Loss: 0.5081\n",
      "Epoch [10], Iter [91/101] Loss: 0.5073\n",
      "Epoch [11], Iter [91/101] Loss: 0.5075\n",
      "Epoch [12], Iter [91/101] Loss: 0.4956\n",
      "Epoch [13], Iter [91/101] Loss: 0.4888\n",
      "Epoch [14], Iter [91/101] Loss: 0.5040\n",
      "Epoch [15], Iter [91/101] Loss: 0.4675\n",
      "Epoch [16], Iter [91/101] Loss: 0.4745\n",
      "Epoch [17], Iter [91/101] Loss: 0.4667\n",
      "Epoch [18], Iter [91/101] Loss: 0.4692\n",
      "Epoch [19], Iter [91/101] Loss: 0.5011\n",
      "Test MSE: 0.6036540865898132\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6015\n",
      "Epoch [2], Iter [91/101] Loss: 0.6359\n",
      "Epoch [3], Iter [91/101] Loss: 0.5684\n",
      "Epoch [4], Iter [91/101] Loss: 0.5707\n",
      "Epoch [5], Iter [91/101] Loss: 0.5393\n",
      "Epoch [6], Iter [91/101] Loss: 0.5573\n",
      "Epoch [7], Iter [91/101] Loss: 0.5346\n",
      "Epoch [8], Iter [91/101] Loss: 0.5418\n",
      "Epoch [9], Iter [91/101] Loss: 0.5261\n",
      "Epoch [10], Iter [91/101] Loss: 0.5288\n",
      "Epoch [11], Iter [91/101] Loss: 0.5301\n",
      "Epoch [12], Iter [91/101] Loss: 0.5343\n",
      "Epoch [13], Iter [91/101] Loss: 0.4740\n",
      "Epoch [14], Iter [91/101] Loss: 0.4830\n",
      "Epoch [15], Iter [91/101] Loss: 0.4876\n",
      "Epoch [16], Iter [91/101] Loss: 0.5210\n",
      "Epoch [17], Iter [91/101] Loss: 0.4937\n",
      "Epoch [18], Iter [91/101] Loss: 0.4922\n",
      "Epoch [19], Iter [91/101] Loss: 0.4782\n",
      "Test MSE: 0.6287088394165039\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.5835\n",
      "Epoch [2], Iter [91/101] Loss: 0.5711\n",
      "Epoch [3], Iter [91/101] Loss: 0.5898\n",
      "Epoch [4], Iter [91/101] Loss: 0.5745\n",
      "Epoch [5], Iter [91/101] Loss: 0.5776\n",
      "Epoch [6], Iter [91/101] Loss: 0.5396\n",
      "Epoch [7], Iter [91/101] Loss: 0.5345\n",
      "Epoch [8], Iter [91/101] Loss: 0.5458\n",
      "Epoch [9], Iter [91/101] Loss: 0.5341\n",
      "Epoch [10], Iter [91/101] Loss: 0.5448\n",
      "Epoch [11], Iter [91/101] Loss: 0.5117\n",
      "Epoch [12], Iter [91/101] Loss: 0.5012\n",
      "Epoch [13], Iter [91/101] Loss: 0.5104\n",
      "Epoch [14], Iter [91/101] Loss: 0.5146\n",
      "Epoch [15], Iter [91/101] Loss: 0.5424\n",
      "Epoch [16], Iter [91/101] Loss: 0.4914\n",
      "Epoch [17], Iter [91/101] Loss: 0.4957\n",
      "Epoch [18], Iter [91/101] Loss: 0.4716\n",
      "Epoch [19], Iter [91/101] Loss: 0.4709\n",
      "Epoch [20], Iter [91/101] Loss: 0.4850\n",
      "Epoch [21], Iter [91/101] Loss: 0.4616\n",
      "Test MSE: 0.6186230778694153\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da09b1",
   "metadata": {},
   "source": [
    "# 3) Precipitation weighting \n",
    "\n",
    "Appendix Table A3 in thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "515a8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce2\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"TEST_FRACTION\"] = 0.1\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = True\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"YEARLY\"\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dad9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.5642\n",
      "Epoch [2], Iter [91/101] Loss: 0.5930\n",
      "Epoch [3], Iter [91/101] Loss: 0.5555\n",
      "Epoch [4], Iter [91/101] Loss: 0.4887\n",
      "Epoch [5], Iter [91/101] Loss: 0.4905\n",
      "Epoch [6], Iter [91/101] Loss: 0.4803\n",
      "Epoch [7], Iter [91/101] Loss: 0.4543\n",
      "Epoch [8], Iter [91/101] Loss: 0.4846\n",
      "Epoch [9], Iter [91/101] Loss: 0.4564\n",
      "Epoch [10], Iter [91/101] Loss: 0.4895\n",
      "Epoch [11], Iter [91/101] Loss: 0.4830\n",
      "Epoch [12], Iter [91/101] Loss: 0.4396\n",
      "Test MSE: 1.0260807275772095\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6570\n",
      "Epoch [2], Iter [91/101] Loss: 0.5964\n",
      "Epoch [3], Iter [91/101] Loss: 0.5498\n",
      "Epoch [4], Iter [91/101] Loss: 0.5645\n",
      "Epoch [5], Iter [91/101] Loss: 0.5100\n",
      "Epoch [6], Iter [91/101] Loss: 0.4743\n",
      "Epoch [7], Iter [91/101] Loss: 0.5094\n",
      "Epoch [8], Iter [91/101] Loss: 0.4771\n",
      "Epoch [9], Iter [91/101] Loss: 0.5067\n",
      "Epoch [10], Iter [91/101] Loss: 0.4961\n",
      "Epoch [11], Iter [91/101] Loss: 0.4557\n",
      "Epoch [12], Iter [91/101] Loss: 0.4431\n",
      "Epoch [13], Iter [91/101] Loss: 0.4353\n",
      "Epoch [14], Iter [91/101] Loss: 0.4482\n",
      "Epoch [15], Iter [91/101] Loss: 0.4525\n",
      "Epoch [16], Iter [91/101] Loss: 0.4277\n",
      "Epoch [17], Iter [91/101] Loss: 0.4323\n",
      "Epoch [18], Iter [91/101] Loss: 0.4630\n",
      "Epoch [19], Iter [91/101] Loss: 0.4522\n",
      "Epoch [20], Iter [91/101] Loss: 0.4227\n",
      "Epoch [21], Iter [91/101] Loss: 0.4068\n",
      "Epoch [22], Iter [91/101] Loss: 0.4094\n",
      "Epoch [23], Iter [91/101] Loss: 0.4197\n",
      "Epoch [24], Iter [91/101] Loss: 0.3957\n",
      "Epoch [25], Iter [91/101] Loss: 0.3679\n",
      "Epoch [26], Iter [91/101] Loss: 0.4145\n",
      "Test MSE: 1.6169921159744263\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6233\n",
      "Epoch [2], Iter [91/101] Loss: 0.5447\n",
      "Epoch [3], Iter [91/101] Loss: 0.5647\n",
      "Epoch [4], Iter [91/101] Loss: 0.6601\n",
      "Epoch [5], Iter [91/101] Loss: 0.4957\n",
      "Epoch [6], Iter [91/101] Loss: 0.5322\n",
      "Epoch [7], Iter [91/101] Loss: 0.5080\n",
      "Epoch [8], Iter [91/101] Loss: 0.5181\n",
      "Epoch [9], Iter [91/101] Loss: 2.7922\n",
      "Epoch [10], Iter [91/101] Loss: 0.4777\n",
      "Epoch [11], Iter [91/101] Loss: 0.4607\n",
      "Epoch [12], Iter [91/101] Loss: 0.4687\n",
      "Epoch [13], Iter [91/101] Loss: 0.4504\n",
      "Epoch [14], Iter [91/101] Loss: 0.4855\n",
      "Epoch [15], Iter [91/101] Loss: 0.4366\n",
      "Epoch [16], Iter [91/101] Loss: 0.4242\n",
      "Epoch [17], Iter [91/101] Loss: 0.4748\n",
      "Epoch [18], Iter [91/101] Loss: 0.4373\n",
      "Epoch [19], Iter [91/101] Loss: 0.4910\n",
      "Epoch [20], Iter [91/101] Loss: 0.4357\n",
      "Epoch [21], Iter [91/101] Loss: 0.4503\n",
      "Epoch [22], Iter [91/101] Loss: 0.4232\n",
      "Epoch [23], Iter [91/101] Loss: 0.4351\n",
      "Epoch [24], Iter [91/101] Loss: 0.4097\n",
      "Epoch [25], Iter [91/101] Loss: 0.4050\n",
      "Epoch [26], Iter [91/101] Loss: 0.3878\n",
      "Epoch [27], Iter [91/101] Loss: 0.4157\n",
      "Epoch [28], Iter [91/101] Loss: 0.4098\n",
      "Epoch [29], Iter [91/101] Loss: 0.3764\n",
      "Test MSE: 0.8627781867980957\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6620\n",
      "Epoch [2], Iter [91/101] Loss: 0.5769\n",
      "Epoch [3], Iter [91/101] Loss: 0.5239\n",
      "Epoch [4], Iter [91/101] Loss: 0.5365\n",
      "Epoch [5], Iter [91/101] Loss: 0.5124\n",
      "Epoch [6], Iter [91/101] Loss: 0.5062\n",
      "Epoch [7], Iter [91/101] Loss: 0.5173\n",
      "Epoch [8], Iter [91/101] Loss: 0.5081\n",
      "Epoch [9], Iter [91/101] Loss: 0.4956\n",
      "Epoch [10], Iter [91/101] Loss: 0.4779\n",
      "Epoch [11], Iter [91/101] Loss: 0.4977\n",
      "Epoch [12], Iter [91/101] Loss: 0.4693\n",
      "Epoch [13], Iter [91/101] Loss: 0.4728\n",
      "Epoch [14], Iter [91/101] Loss: 0.4809\n",
      "Epoch [15], Iter [91/101] Loss: 0.4572\n",
      "Epoch [16], Iter [91/101] Loss: 0.4329\n",
      "Epoch [17], Iter [91/101] Loss: 0.7524\n",
      "Epoch [18], Iter [91/101] Loss: 0.4253\n",
      "Epoch [19], Iter [91/101] Loss: 0.5733\n",
      "Epoch [20], Iter [91/101] Loss: 0.4536\n",
      "Epoch [21], Iter [91/101] Loss: 0.4509\n",
      "Epoch [22], Iter [91/101] Loss: 0.4179\n",
      "Epoch [23], Iter [91/101] Loss: 0.4253\n",
      "Test MSE: 0.8492376208305359\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09a06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
