{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the classical baseline for different number of principle components.\n",
    "\n",
    "Was done as an ablation experiment in the Appendix of the thesis. Results are evaluated in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import netCDF4\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from icosahedron import Icosahedron, rand_rotation_icosahedron, rand_rotation_matrix, plot_voronoi, plot_voronoi_charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset for given specifications we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dictionary to store information on the training procedure and the used model\n",
    "model_training_description = {}\n",
    "model_training_description[\"MODELTYPE\"] = \"Classical_flat_PCA\" \n",
    "\n",
    "#  automatically load dataset given specifications\n",
    "PREFIX = \"HadCM3-flat\"\n",
    "DO_SHUFFLE = False\n",
    "DSET_NR = 1\n",
    "\n",
    "ALL_VARIABLES = np.sort([\"temp_1\", \"precip\", \"dO18\"])#,\"p\"])\n",
    "\n",
    "shuffle_dict = {True:\"shuffle\", False:\"no-shuffle\"}\n",
    "corners_dict = {True: \"interp-corners\", False: \"zero-fill-corners\"}\n",
    "\n",
    "DATASET_FOLDER = \"{}_{}_{}_\".format(PREFIX, shuffle_dict[DO_SHUFFLE], DSET_NR)\n",
    "DATASET_FOLDER = DATASET_FOLDER + \"-\".join(ALL_VARIABLES)\n",
    "\n",
    "DIRECTORY_DATASETS_INTERPOLATED = \"Datasets/Interpolated/\"  # directory where the interpolated dataset is stored\n",
    "DIRECTORY_IMAGES = \"Images/\"  # directory where we want to store plots produced by the notebook\n",
    "DIRECTORY_OUTPUTS = \"Output/Compare_n_pc\"  # directory where the want to store the outpout\n",
    "\n",
    "DATASET_FOLDER = os.path.join(DIRECTORY_OUTPUTS, DATASET_FOLDER)\n",
    "\n",
    "if not os.path.exists(DATASET_FOLDER):\n",
    "    raise OSError(\"There exists no folder for the given specifications\")\n",
    "\n",
    "# load the dataset from its directory\n",
    "DATASET = os.path.join(DATASET_FOLDER, \"dataset.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select what standardization we want for each of the variables.\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = (\"Pixelwise\", \"Pixelwise\")\n",
    "model_training_description[\"S_MODE_TARGETS\"] = (\"Pixelwise\",)\n",
    "model_training_description[\"REGTYPE\"] = \"lasso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, s_mode_predictors, s_mode_targets):\n",
    "    \"\"\"\n",
    "    Load data from file specified by path, prepare loaders with batchsize batch_size.\n",
    "    Standardize the data with the given standardization mode\n",
    "    \"\"\"\n",
    "\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    train_predictors = dataset[\"train\"][\"predictors\"].astype(np.float32)\n",
    "    train_targets = dataset[\"train\"][\"targets\"].astype(np.float32)\n",
    "    test_predictors = dataset[\"test\"][\"predictors\"].astype(np.float32)\n",
    "    test_targets = dataset[\"test\"][\"targets\"].astype(np.float32)\n",
    "    \n",
    "    n_predictors = train_predictors.shape[1]\n",
    "    n_targets = train_targets.shape[1]\n",
    "    w = train_predictors.shape[-1]\n",
    "    h = train_predictors.shape[-2]\n",
    "    \n",
    "    # assert that standardize mode has one element for each variable.\n",
    "    assert len(s_mode_predictors) == n_predictors\n",
    "    assert len(s_mode_targets) == n_targets\n",
    "    assert all([mode in [\"None\", \"Pixelwise\", \"Global_mean_pixelwise_std\", \"Pixelwise_mean_global_std\", \"Global\"] for mode in s_mode_predictors]) \n",
    "    assert all([mode in [\"None\", \"Pixelwise\", \"Global_mean_pixelwise_std\", \"Pixelwise_mean_global_std\", \"Global\"] for mode in s_mode_targets]) \n",
    "\n",
    "\n",
    "    # predictors: \n",
    "    for i, mode in enumerate(s_mode_predictors):    \n",
    "        if mode == \"Global\": # Global normalization: Use same standard deviation for each pixel\n",
    "            mean = np.mean(train_predictors[:,i,...],axis=(0,1,2), keepdims=True)\n",
    "            std = np.mean(torch.std(train_predictors[:,i,...], axis=(0), keepdims=True), axis=(1,2), keepdims=True)\n",
    "            std[std==0] = 1 # avoid dividing by zero\n",
    "            \n",
    "        elif mode == \"Global_mean_local_std\": # Subtract the global mean, but divide by local standard deviation\n",
    "            mean = np.mean(train_predictors[:,i,...],axis=(0,1,2), keepdims=True)\n",
    "            std = np.std(train_predictors[:,i,...],axis=(0), keepdims=True)\n",
    "            std[std==0] = 1 # avoid dividing by zero\n",
    "            \n",
    "        elif mode == \"Pixelwise_mean_global_std\": # Subtract the global mean, but divide by local standard deviation\n",
    "            mean = np.mean(train_predictors[:,i,...],axis=(0), keepdims=True)\n",
    "            std = np.mean(torch.std(train_predictors[:,i,...], axis=(0), keepdims=True), axis=(1,2), keepdims=True)\n",
    "            std[std==0] = 1 # avoid dividing by zero\n",
    "            \n",
    "        elif mode == \"Pixelwise\":  # Subtract pixelwise mean and ivide each pixel by its own standard deviation\n",
    "            mean = np.mean(train_predictors[:,i,...], axis=(0), keepdims=True)\n",
    "            std = np.std(train_predictors[:,i,...],axis=(0), keepdims=True)                             \n",
    "            std[std==0] = 1 # avoid dividing by zero\n",
    "\n",
    "        train_predictors[:,i,...] = (train_predictors[:,i,...] - mean)/std\n",
    "        test_predictors[:,i,...] = (test_predictors[:,i,...] - mean)/std\n",
    "\n",
    "    # targets: \n",
    "    for i, mode in enumerate(s_mode_targets):\n",
    "        if mode == \"Global\": # Global normalization: Use same standard deviation for each pixel\n",
    "            mean = np.mean(train_targets[:,i,...], axis=(0,1,2), keepdims=True)\n",
    "            std = np.mean(torch.std(train_targets[:,i,...], axis=(0), keepdims=True), axis=(1,2), keepdims=True)\n",
    "            std[std==0] = 1 # avoid dividing by zero\n",
    "            \n",
    "        elif mode == \"Global_mean_local_std\": # Subtract the global mean, but divide by local standard deviation\n",
    "            mean = np.mean(train_targets[:,i,...],axis=(0,1,2), keepdims=True)\n",
    "            std = np.std(train_targets[:,i,...],axis=(0), keepdims=True)\n",
    "            std[std==0] = 1 # avoid dividing by zero\n",
    "            \n",
    "        elif mode == \"Pixelwise_mean_global_std\": # Subtract the local mean, but divide by global standard deviation\n",
    "            mean = np.mean(train_targets[:,i,...],axis=(0), keepdims=True)\n",
    "            std = np.mean(torch.std(train_targets[:,i,...], axis=(0), keepdims=True), axis=(1,2), keepdims=True)\n",
    "            std[std==0] = 1 # avoid dividing by zero\n",
    "            \n",
    "        elif mode == \"Pixelwise\":  # Subtract pixelwise mean and ivide each pixel by its own standard deviation\n",
    "            mean = np.mean(train_targets[:,i,...], axis=(0), keepdims=True)\n",
    "            std = np.std(train_targets[:,i,...],axis=(0), keepdims=True)                            \n",
    "            std[std==0] = 1 # avoid dividing by zero   \n",
    "        \n",
    "        train_targets[:,i,...] = (train_targets[:,i,...] - mean)/std\n",
    "        test_targets[:,i,...] = (test_targets[:,i,...] - mean)/std \n",
    "\n",
    "    return train_predictors, train_targets, test_predictors, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_model(X_train, Y_train):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \"\"\"get the trained model\"\"\"\n",
    "    regressor = LinearRegression().fit(X_train, Y_train)\n",
    "    return regressor\n",
    "\n",
    "def train_lasso(X_train, Y_train):\n",
    "    \"\"\"get the trained LASSO model\"\"\"\n",
    "    from sklearn.linear_model import MultiTaskLassoCV\n",
    "    lasso = MultiTaskLassoCV().fit(X_train, Y_train)\n",
    "    return lasso\n",
    "\n",
    "def train_onedim_lasso(X_train, Y_train):\n",
    "    \"\"\"get trained LASSO with one-dimensional output\"\"\"    \n",
    "    from sklearn.linear_model import LassoCV\n",
    "    lasso = LassoCV().fit(X_train, Y_train)\n",
    "    return lasso    \n",
    "    \n",
    "def predict_with_model(model, X_test, Y_test):\n",
    "    return model.predict(X_test), Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a wrapper for all the rescaling pca etc\n",
    "\n",
    "def train_pca(X_tr, Y_tr, regtype=model_training_description[\"REGTYPE\"], n_pc_in=20, n_pc_target=20):\n",
    "    \"\"\" \n",
    "    Train PCA and regression model on the training data. In opposition to the version in the Jonathan_PCA_methods notebook, \n",
    "    we don't rescale here seperately, rescaling is already done in the dataloader.\n",
    "    Assume inputdata of shape (n_timesteps, n_variables, n_lat, n_lon).\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    X_train = X_tr.reshape(X_tr.shape[0],-1)\n",
    "    Y_train = Y_tr.reshape(Y_tr.shape[0],-1)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_pc_in)\n",
    "    principalComponents = pca.fit_transform(X_train)\n",
    "    \n",
    "    pca_targets = PCA(n_components=n_pc_target)\n",
    "    principalComponents_targets = pca_targets.fit_transform(Y_train)\n",
    "    \n",
    "    # Get the model \n",
    "    if regtype == 'lasso':\n",
    "        model = train_lasso(principalComponents, principalComponents_targets)\n",
    "    elif regtype == 'linreg':\n",
    "        model = train_global_model(principalComponents, principalComponents_targets)\n",
    "    else:\n",
    "        print(\"This regression model is currently not implemented.\")\n",
    "    return pca, pca_targets, model\n",
    "\n",
    "def test_pca_scaling(X_te, Y_te, pca, pca_targets, model):\n",
    "    \"\"\"\n",
    "    Assume inputdata of shape (n_timesteps, n_variables, n_lat, n_lon)\n",
    "    \"\"\"\n",
    "    X_test = X_te.reshape(X_te.shape[0],-1)\n",
    "    Y_test = Y_te.reshape(Y_te.shape[0],-1)\n",
    "\n",
    "    X_test_rescaled = pca.transform(X_test)\n",
    "    predict_test = model.predict(X_test_rescaled)\n",
    "    predict_test = pca_targets.inverse_transform(predict_test)\n",
    "    \n",
    "    return predict_test.reshape(*Y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_out = np.logspace(np.log10(3),np.log10(300),10).astype(\"int\")\n",
    "n_pc_in = (1.5*np.logspace(np.log10(3),np.log10(300),10)).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "1\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "2\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "3\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "4\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "5\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "6\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "7\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "8\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "9\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n",
      "writing model and training description\n",
      "writing model predictions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(n_pc_in)):\n",
    "    print(i)\n",
    "    model_training_description[\"n_pc_in\"] = n_pc_in[i]\n",
    "    model_training_description[\"n_pc_target\"] = n_pc_out[i]\n",
    "    for j in range(3):\n",
    "        model_training_description[\"RUN_NR\"] = j\n",
    "        \n",
    "        \n",
    "        train_predictors, train_targets, test_predictors, test_targets = load_data(DATASET, s_mode_predictors=model_training_description[\"S_MODE_PREDICTORS\"],\\\n",
    "                                                                                   s_mode_targets=model_training_description[\"S_MODE_TARGETS\"])\n",
    "        pca, pca_targets, pca_model = train_pca(train_predictors, \n",
    "                                                train_targets, \n",
    "                                                regtype=model_training_description[\"REGTYPE\"], \n",
    "                                                n_pc_in=model_training_description[\"n_pc_in\"], \n",
    "                                                n_pc_target=model_training_description[\"n_pc_target\"])\n",
    "        \n",
    "        predict_targets = test_pca_scaling(test_predictors, \n",
    "                                           test_targets, \n",
    "                                           pca, pca_targets, pca_model)\n",
    "\n",
    "        hash_value = hex(hash(frozenset(model_training_description.items())))\n",
    "        model_name = \"classical_pca_\"\n",
    "        \n",
    "        run_directory = os.path.join(DATASET_FOLDER, model_name + hash_value)\n",
    "        if os.path.exists(run_directory):\n",
    "            raise FileExistsError(\"Hash collision. Probably a model run with this configuration already exists.\")\n",
    "        else:\n",
    "            os.makedirs(run_directory)\n",
    "            print(\"writing model and training description\")\n",
    "            with gzip.open(os.path.join(run_directory, \"model_training_description.gz\"), 'wb') as f:\n",
    "                pickle.dump(model_training_description, f)\n",
    "            print(\"writing model predictions\")\n",
    "            with gzip.open(os.path.join(run_directory, \"predictions.gz\"), 'wb') as f:\n",
    "                pickle.dump({\"predictions\": predict_targets.reshape(test_targets.shape)}, f)        \n",
    "            print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
