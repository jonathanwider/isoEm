{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734354f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from predict import *\n",
    "from evaluate import *\n",
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed05f4",
   "metadata": {},
   "source": [
    "# Test monthly stuff from thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771afb0",
   "metadata": {},
   "source": [
    "## Create datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce1deb",
   "metadata": {},
   "source": [
    "All months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f5af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED\"] = np.sort([0,1,2,3,4,5,6,7,8,9,10,11]).tolist()\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698616dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "Specified dataset already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6012/1450278723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_monthly_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Uni\\IcoCNN_new\\datasets.py\u001b[0m in \u001b[0;36mcreate_monthly_dataset\u001b[1;34m(description, dataset_folder, output_folder)\u001b[0m\n\u001b[0;32m    847\u001b[0m     \u001b[0mfolder_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dset_{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_if_folder_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Specified dataset already exists.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: Specified dataset already exists."
     ]
    }
   ],
   "source": [
    "create_monthly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a472a41",
   "metadata": {},
   "source": [
    "All months, use lagged months in predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7443bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED\"] = np.sort([0,1,2,3,4,5,6,7,8,9,10,11]).tolist()\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0,-1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb355d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create_monthly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde7093",
   "metadata": {},
   "source": [
    "Individual dataset for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27c7491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n",
      "loading variables\n",
      "writing pickle\n",
      "done\n",
      "writing dataset description\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0]).tolist()\n",
    "\n",
    "for i in range(12):\n",
    "    description[\"MONTHS_USED\"] = np.sort([i]).tolist()\n",
    "    create_monthly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161e75b",
   "metadata": {},
   "source": [
    "# Run experiments on these datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575dddea",
   "metadata": {},
   "source": [
    "All months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d06c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED\"] = np.sort([0,1,2,3,4,5,6,7,8,9,10,11]).tolist()\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0]).tolist()\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f34180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [1201/1213] Loss: 0.3377\n",
      "Epoch [2], Iter [1201/1213] Loss: 0.3503\n",
      "Epoch [3], Iter [1201/1213] Loss: 0.3242\n",
      "Epoch [4], Iter [1201/1213] Loss: 0.2790\n",
      "Epoch [5], Iter [1201/1213] Loss: 0.2671\n",
      "Epoch [6], Iter [1201/1213] Loss: 0.2562\n",
      "Epoch [7], Iter [1201/1213] Loss: 0.3223\n",
      "Epoch [8], Iter [1201/1213] Loss: 0.3075\n",
      "Epoch [9], Iter [1201/1213] Loss: 0.2768\n",
      "Epoch [10], Iter [1201/1213] Loss: 0.2937\n",
      "Epoch [11], Iter [1201/1213] Loss: 0.2770\n",
      "Epoch [12], Iter [1201/1213] Loss: 0.2651\n",
      "Epoch [13], Iter [1201/1213] Loss: 0.2805\n",
      "Epoch [14], Iter [1201/1213] Loss: 0.2830\n",
      "Epoch [15], Iter [1201/1213] Loss: 0.2564\n",
      "Epoch [16], Iter [1201/1213] Loss: 0.2703\n",
      "Epoch [17], Iter [1201/1213] Loss: 0.2641\n",
      "Epoch [18], Iter [1201/1213] Loss: 0.2368\n",
      "Epoch [19], Iter [1201/1213] Loss: 0.2629\n",
      "Epoch [20], Iter [1201/1213] Loss: 0.2822\n",
      "Epoch [21], Iter [1201/1213] Loss: 0.2745\n",
      "Epoch [22], Iter [1201/1213] Loss: 0.2442\n",
      "Epoch [23], Iter [1201/1213] Loss: 0.2484\n",
      "Epoch [24], Iter [1201/1213] Loss: 0.2495\n",
      "Epoch [25], Iter [1201/1213] Loss: 0.2437\n",
      "Epoch [26], Iter [1201/1213] Loss: 0.2454\n",
      "Epoch [27], Iter [1201/1213] Loss: 0.2530\n",
      "Epoch [28], Iter [1201/1213] Loss: 0.2417\n",
      "Epoch [29], Iter [1201/1213] Loss: 0.2428\n",
      "Epoch [30], Iter [1201/1213] Loss: 0.2492\n",
      "Epoch [31], Iter [1201/1213] Loss: 0.2544\n",
      "Epoch [32], Iter [1201/1213] Loss: 0.2611\n",
      "Epoch [33], Iter [1201/1213] Loss: 0.2519\n",
      "Epoch [34], Iter [1201/1213] Loss: 0.2497\n",
      "Test MSE: 0.2871914207935333\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1213] Loss: 0.3053\n",
      "Epoch [2], Iter [1201/1213] Loss: 0.3144\n",
      "Epoch [3], Iter [1201/1213] Loss: 0.2924\n",
      "Epoch [4], Iter [1201/1213] Loss: 0.3135\n",
      "Epoch [5], Iter [1201/1213] Loss: 0.2716\n",
      "Epoch [6], Iter [1201/1213] Loss: 0.2923\n",
      "Epoch [7], Iter [1201/1213] Loss: 0.2795\n",
      "Epoch [8], Iter [1201/1213] Loss: 0.2650\n",
      "Epoch [9], Iter [1201/1213] Loss: 0.2695\n",
      "Epoch [10], Iter [1201/1213] Loss: 0.2838\n",
      "Epoch [11], Iter [1201/1213] Loss: 0.3215\n",
      "Epoch [12], Iter [1201/1213] Loss: 0.2947\n",
      "Epoch [13], Iter [1201/1213] Loss: 0.2801\n",
      "Epoch [14], Iter [1201/1213] Loss: 0.2551\n",
      "Epoch [15], Iter [1201/1213] Loss: 0.2595\n",
      "Epoch [16], Iter [1201/1213] Loss: 0.2733\n",
      "Epoch [17], Iter [1201/1213] Loss: 0.2713\n",
      "Epoch [18], Iter [1201/1213] Loss: 0.2701\n",
      "Epoch [19], Iter [1201/1213] Loss: 0.2436\n",
      "Epoch [20], Iter [1201/1213] Loss: 0.2572\n",
      "Epoch [21], Iter [1201/1213] Loss: 0.2393\n",
      "Epoch [22], Iter [1201/1213] Loss: 0.2381\n",
      "Epoch [23], Iter [1201/1213] Loss: 0.2441\n",
      "Epoch [24], Iter [1201/1213] Loss: 0.2415\n",
      "Epoch [25], Iter [1201/1213] Loss: 0.2892\n",
      "Epoch [26], Iter [1201/1213] Loss: 0.2371\n",
      "Epoch [27], Iter [1201/1213] Loss: 0.2643\n",
      "Epoch [28], Iter [1201/1213] Loss: 0.2374\n",
      "Epoch [29], Iter [1201/1213] Loss: 0.2648\n",
      "Epoch [30], Iter [1201/1213] Loss: 0.3006\n",
      "Epoch [31], Iter [1201/1213] Loss: 0.2288\n",
      "Epoch [32], Iter [1201/1213] Loss: 0.2699\n",
      "Epoch [33], Iter [1201/1213] Loss: 0.2429\n",
      "Epoch [34], Iter [1201/1213] Loss: 0.2341\n",
      "Epoch [35], Iter [1201/1213] Loss: 0.2643\n",
      "Epoch [36], Iter [1201/1213] Loss: 0.2241\n",
      "Epoch [37], Iter [1201/1213] Loss: 0.2847\n",
      "Test MSE: 0.28735437989234924\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1213] Loss: 0.3820\n",
      "Epoch [2], Iter [1201/1213] Loss: 0.3073\n",
      "Epoch [3], Iter [1201/1213] Loss: 0.3039\n",
      "Epoch [4], Iter [1201/1213] Loss: 0.2823\n",
      "Epoch [5], Iter [1201/1213] Loss: 0.3359\n",
      "Epoch [6], Iter [1201/1213] Loss: 0.2863\n",
      "Epoch [7], Iter [1201/1213] Loss: 0.2718\n",
      "Epoch [8], Iter [1201/1213] Loss: 0.2713\n",
      "Epoch [9], Iter [1201/1213] Loss: 0.2700\n",
      "Epoch [10], Iter [1201/1213] Loss: 0.2608\n",
      "Epoch [11], Iter [1201/1213] Loss: 0.2755\n",
      "Epoch [12], Iter [1201/1213] Loss: 0.2948\n",
      "Epoch [13], Iter [1201/1213] Loss: 0.2613\n",
      "Epoch [14], Iter [1201/1213] Loss: 0.2454\n",
      "Epoch [15], Iter [1201/1213] Loss: 0.2653\n",
      "Epoch [16], Iter [1201/1213] Loss: 0.2827\n",
      "Epoch [17], Iter [1201/1213] Loss: 0.2814\n",
      "Epoch [18], Iter [1201/1213] Loss: 0.2680\n",
      "Epoch [19], Iter [1201/1213] Loss: 0.2839\n",
      "Epoch [20], Iter [1201/1213] Loss: 0.2595\n",
      "Test MSE: 0.28685131669044495\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1213] Loss: 0.3117\n",
      "Epoch [2], Iter [1201/1213] Loss: 0.3252\n",
      "Epoch [3], Iter [1201/1213] Loss: 0.3003\n",
      "Epoch [4], Iter [1201/1213] Loss: 0.3150\n",
      "Epoch [5], Iter [1201/1213] Loss: 0.3193\n",
      "Epoch [6], Iter [1201/1213] Loss: 0.2606\n",
      "Epoch [7], Iter [1201/1213] Loss: 0.3315\n",
      "Epoch [8], Iter [1201/1213] Loss: 0.2921\n",
      "Epoch [9], Iter [1201/1213] Loss: 0.2837\n",
      "Epoch [10], Iter [1201/1213] Loss: 0.2843\n",
      "Epoch [11], Iter [1201/1213] Loss: 0.2849\n",
      "Epoch [12], Iter [1201/1213] Loss: 0.2640\n",
      "Epoch [13], Iter [1201/1213] Loss: 0.2449\n",
      "Epoch [14], Iter [1201/1213] Loss: 0.2842\n",
      "Epoch [15], Iter [1201/1213] Loss: 0.2711\n",
      "Epoch [16], Iter [1201/1213] Loss: 0.2587\n",
      "Epoch [17], Iter [1201/1213] Loss: 0.2513\n",
      "Epoch [18], Iter [1201/1213] Loss: 0.2712\n",
      "Epoch [19], Iter [1201/1213] Loss: 0.2593\n",
      "Epoch [20], Iter [1201/1213] Loss: 0.2515\n",
      "Epoch [21], Iter [1201/1213] Loss: 0.2500\n",
      "Epoch [22], Iter [1201/1213] Loss: 0.2893\n",
      "Epoch [23], Iter [1201/1213] Loss: 0.2502\n",
      "Epoch [24], Iter [1201/1213] Loss: 0.2735\n",
      "Epoch [25], Iter [1201/1213] Loss: 0.2401\n",
      "Epoch [26], Iter [1201/1213] Loss: 0.2259\n",
      "Epoch [27], Iter [1201/1213] Loss: 0.2414\n",
      "Epoch [28], Iter [1201/1213] Loss: 0.2692\n",
      "Epoch [29], Iter [1201/1213] Loss: 0.2782\n",
      "Epoch [30], Iter [1201/1213] Loss: 0.3015\n",
      "Epoch [31], Iter [1201/1213] Loss: 0.2381\n",
      "Epoch [32], Iter [1201/1213] Loss: 0.2481\n",
      "Epoch [33], Iter [1201/1213] Loss: 0.2417\n",
      "Epoch [34], Iter [1201/1213] Loss: 0.2575\n",
      "Epoch [35], Iter [1201/1213] Loss: 0.2464\n",
      "Epoch [36], Iter [1201/1213] Loss: 0.2430\n",
      "Epoch [37], Iter [1201/1213] Loss: 0.2481\n",
      "Test MSE: 0.2878226935863495\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1213] Loss: 0.3479\n",
      "Epoch [2], Iter [1201/1213] Loss: 0.2884\n",
      "Epoch [3], Iter [1201/1213] Loss: 0.3432\n",
      "Epoch [4], Iter [1201/1213] Loss: 0.2841\n",
      "Epoch [5], Iter [1201/1213] Loss: 0.2870\n",
      "Epoch [6], Iter [1201/1213] Loss: 0.3070\n",
      "Epoch [7], Iter [1201/1213] Loss: 0.2888\n",
      "Epoch [8], Iter [1201/1213] Loss: 0.2728\n",
      "Epoch [9], Iter [1201/1213] Loss: 0.2597\n",
      "Epoch [10], Iter [1201/1213] Loss: 0.2852\n",
      "Epoch [11], Iter [1201/1213] Loss: 0.2595\n",
      "Epoch [12], Iter [1201/1213] Loss: 0.2838\n",
      "Epoch [13], Iter [1201/1213] Loss: 0.3081\n",
      "Epoch [14], Iter [1201/1213] Loss: 0.2511\n",
      "Epoch [15], Iter [1201/1213] Loss: 0.2456\n",
      "Epoch [16], Iter [1201/1213] Loss: 0.2725\n",
      "Epoch [17], Iter [1201/1213] Loss: 0.2834\n",
      "Epoch [18], Iter [1201/1213] Loss: 0.2563\n",
      "Epoch [19], Iter [1201/1213] Loss: 0.2913\n",
      "Epoch [20], Iter [1201/1213] Loss: 0.2455\n",
      "Epoch [21], Iter [1201/1213] Loss: 0.2793\n",
      "Epoch [22], Iter [1201/1213] Loss: 0.2626\n",
      "Epoch [23], Iter [1201/1213] Loss: 0.2934\n",
      "Epoch [24], Iter [1201/1213] Loss: 0.2445\n",
      "Epoch [25], Iter [1201/1213] Loss: 0.2385\n",
      "Epoch [26], Iter [1201/1213] Loss: 0.2619\n",
      "Epoch [27], Iter [1201/1213] Loss: 0.2633\n",
      "Epoch [28], Iter [1201/1213] Loss: 0.2523\n",
      "Epoch [29], Iter [1201/1213] Loss: 0.2566\n",
      "Epoch [30], Iter [1201/1213] Loss: 0.2569\n",
      "Test MSE: 0.288938969373703\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 5\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd8528",
   "metadata": {},
   "source": [
    "All months, use previous timestep in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528da814",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED\"] = np.sort([0,1,2,3,4,5,6,7,8,9,10,11]).tolist()\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0,-1]).tolist()\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))*2\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b418b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [1201/1212] Loss: 0.3365\n",
      "Epoch [2], Iter [1201/1212] Loss: 0.3547\n",
      "Epoch [3], Iter [1201/1212] Loss: 0.3124\n",
      "Epoch [4], Iter [1201/1212] Loss: 0.2976\n",
      "Epoch [5], Iter [1201/1212] Loss: 0.2796\n",
      "Epoch [6], Iter [1201/1212] Loss: 0.2849\n",
      "Epoch [7], Iter [1201/1212] Loss: 0.2914\n",
      "Epoch [8], Iter [1201/1212] Loss: 0.2648\n",
      "Epoch [9], Iter [1201/1212] Loss: 0.2871\n",
      "Epoch [10], Iter [1201/1212] Loss: 0.2775\n",
      "Epoch [11], Iter [1201/1212] Loss: 0.2750\n",
      "Epoch [12], Iter [1201/1212] Loss: 0.2581\n",
      "Epoch [13], Iter [1201/1212] Loss: 0.2892\n",
      "Epoch [14], Iter [1201/1212] Loss: 0.2732\n",
      "Epoch [15], Iter [1201/1212] Loss: 0.2727\n",
      "Epoch [16], Iter [1201/1212] Loss: 0.2684\n",
      "Epoch [17], Iter [1201/1212] Loss: 0.3049\n",
      "Epoch [18], Iter [1201/1212] Loss: 0.2759\n",
      "Epoch [19], Iter [1201/1212] Loss: 0.2661\n",
      "Epoch [20], Iter [1201/1212] Loss: 0.2590\n",
      "Epoch [21], Iter [1201/1212] Loss: 0.3034\n",
      "Epoch [22], Iter [1201/1212] Loss: 0.2746\n",
      "Epoch [23], Iter [1201/1212] Loss: 0.2524\n",
      "Epoch [24], Iter [1201/1212] Loss: 0.2732\n",
      "Epoch [25], Iter [1201/1212] Loss: 0.2570\n",
      "Epoch [26], Iter [1201/1212] Loss: 0.2452\n",
      "Epoch [27], Iter [1201/1212] Loss: 0.2642\n",
      "Epoch [28], Iter [1201/1212] Loss: 0.2515\n",
      "Epoch [29], Iter [1201/1212] Loss: 0.2630\n",
      "Test MSE: 0.2854060232639313\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1212] Loss: 0.3256\n",
      "Epoch [2], Iter [1201/1212] Loss: 0.3237\n",
      "Epoch [3], Iter [1201/1212] Loss: 0.2856\n",
      "Epoch [4], Iter [1201/1212] Loss: 0.3242\n",
      "Epoch [5], Iter [1201/1212] Loss: 0.3200\n",
      "Epoch [6], Iter [1201/1212] Loss: 0.3125\n",
      "Epoch [7], Iter [1201/1212] Loss: 0.2905\n",
      "Epoch [8], Iter [1201/1212] Loss: 0.2815\n",
      "Epoch [9], Iter [1201/1212] Loss: 0.2557\n",
      "Epoch [10], Iter [1201/1212] Loss: 0.2862\n",
      "Epoch [11], Iter [1201/1212] Loss: 0.2689\n",
      "Epoch [12], Iter [1201/1212] Loss: 0.2604\n",
      "Epoch [13], Iter [1201/1212] Loss: 0.2629\n",
      "Epoch [14], Iter [1201/1212] Loss: 0.2946\n",
      "Epoch [15], Iter [1201/1212] Loss: 0.2589\n",
      "Epoch [16], Iter [1201/1212] Loss: 0.2572\n",
      "Epoch [17], Iter [1201/1212] Loss: 0.2705\n",
      "Epoch [18], Iter [1201/1212] Loss: 0.2586\n",
      "Epoch [19], Iter [1201/1212] Loss: 0.2742\n",
      "Epoch [20], Iter [1201/1212] Loss: 0.2575\n",
      "Epoch [21], Iter [1201/1212] Loss: 0.2397\n",
      "Epoch [22], Iter [1201/1212] Loss: 0.2684\n",
      "Epoch [23], Iter [1201/1212] Loss: 0.2639\n",
      "Epoch [24], Iter [1201/1212] Loss: 0.2649\n",
      "Epoch [25], Iter [1201/1212] Loss: 0.2486\n",
      "Epoch [26], Iter [1201/1212] Loss: 0.2788\n",
      "Epoch [27], Iter [1201/1212] Loss: 0.2729\n",
      "Test MSE: 0.28521934151649475\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1212] Loss: 0.3183\n",
      "Epoch [2], Iter [1201/1212] Loss: 0.3356\n",
      "Epoch [3], Iter [1201/1212] Loss: 0.3301\n",
      "Epoch [4], Iter [1201/1212] Loss: 0.3520\n",
      "Epoch [5], Iter [1201/1212] Loss: 0.2640\n",
      "Epoch [6], Iter [1201/1212] Loss: 0.3059\n",
      "Epoch [7], Iter [1201/1212] Loss: 0.2878\n",
      "Epoch [8], Iter [1201/1212] Loss: 0.2768\n",
      "Epoch [9], Iter [1201/1212] Loss: 0.2688\n",
      "Epoch [10], Iter [1201/1212] Loss: 0.2544\n",
      "Epoch [11], Iter [1201/1212] Loss: 0.2687\n",
      "Epoch [12], Iter [1201/1212] Loss: 0.2893\n",
      "Epoch [13], Iter [1201/1212] Loss: 0.2709\n",
      "Epoch [14], Iter [1201/1212] Loss: 0.2687\n",
      "Epoch [15], Iter [1201/1212] Loss: 0.2777\n",
      "Epoch [16], Iter [1201/1212] Loss: 0.2582\n",
      "Epoch [17], Iter [1201/1212] Loss: 0.2604\n",
      "Epoch [18], Iter [1201/1212] Loss: 0.2526\n",
      "Epoch [19], Iter [1201/1212] Loss: 0.2777\n",
      "Epoch [20], Iter [1201/1212] Loss: 0.2780\n",
      "Epoch [21], Iter [1201/1212] Loss: 0.2341\n",
      "Epoch [22], Iter [1201/1212] Loss: 0.2718\n",
      "Epoch [23], Iter [1201/1212] Loss: 0.2464\n",
      "Epoch [24], Iter [1201/1212] Loss: 0.2629\n",
      "Epoch [25], Iter [1201/1212] Loss: 0.2761\n",
      "Epoch [26], Iter [1201/1212] Loss: 0.2693\n",
      "Epoch [27], Iter [1201/1212] Loss: 0.2604\n",
      "Epoch [28], Iter [1201/1212] Loss: 0.2719\n",
      "Epoch [29], Iter [1201/1212] Loss: 0.2235\n",
      "Test MSE: 0.2862195074558258\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1212] Loss: 0.3478\n",
      "Epoch [2], Iter [1201/1212] Loss: 0.3087\n",
      "Epoch [3], Iter [1201/1212] Loss: 0.3262\n",
      "Epoch [4], Iter [1201/1212] Loss: 0.2750\n",
      "Epoch [5], Iter [1201/1212] Loss: 0.2935\n",
      "Epoch [6], Iter [1201/1212] Loss: 0.2734\n",
      "Epoch [7], Iter [1201/1212] Loss: 0.3182\n",
      "Epoch [8], Iter [1201/1212] Loss: 0.3201\n",
      "Epoch [9], Iter [1201/1212] Loss: 0.2799\n",
      "Epoch [10], Iter [1201/1212] Loss: 0.2757\n",
      "Epoch [11], Iter [1201/1212] Loss: 0.2659\n",
      "Epoch [12], Iter [1201/1212] Loss: 0.2774\n",
      "Epoch [13], Iter [1201/1212] Loss: 0.2516\n",
      "Epoch [14], Iter [1201/1212] Loss: 0.2424\n",
      "Epoch [15], Iter [1201/1212] Loss: 0.2569\n",
      "Epoch [16], Iter [1201/1212] Loss: 0.2671\n",
      "Epoch [17], Iter [1201/1212] Loss: 0.2740\n",
      "Epoch [18], Iter [1201/1212] Loss: 0.2493\n",
      "Epoch [19], Iter [1201/1212] Loss: 0.2579\n",
      "Epoch [20], Iter [1201/1212] Loss: 0.2802\n",
      "Epoch [21], Iter [1201/1212] Loss: 0.2436\n",
      "Epoch [22], Iter [1201/1212] Loss: 0.2641\n",
      "Epoch [23], Iter [1201/1212] Loss: 0.2760\n",
      "Epoch [24], Iter [1201/1212] Loss: 0.2202\n",
      "Epoch [25], Iter [1201/1212] Loss: 0.2752\n",
      "Epoch [26], Iter [1201/1212] Loss: 0.2769\n",
      "Epoch [27], Iter [1201/1212] Loss: 0.2586\n",
      "Epoch [28], Iter [1201/1212] Loss: 0.2582\n",
      "Epoch [29], Iter [1201/1212] Loss: 0.2477\n",
      "Test MSE: 0.2849627435207367\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1212] Loss: 0.3754\n",
      "Epoch [2], Iter [1201/1212] Loss: 0.3013\n",
      "Epoch [3], Iter [1201/1212] Loss: 0.3376\n",
      "Epoch [4], Iter [1201/1212] Loss: 0.2891\n",
      "Epoch [5], Iter [1201/1212] Loss: 0.2984\n",
      "Epoch [6], Iter [1201/1212] Loss: 0.2758\n",
      "Epoch [7], Iter [1201/1212] Loss: 0.2657\n",
      "Epoch [8], Iter [1201/1212] Loss: 0.2749\n",
      "Epoch [9], Iter [1201/1212] Loss: 0.2655\n",
      "Epoch [10], Iter [1201/1212] Loss: 0.3013\n",
      "Epoch [11], Iter [1201/1212] Loss: 0.3136\n",
      "Epoch [12], Iter [1201/1212] Loss: 0.2865\n",
      "Epoch [13], Iter [1201/1212] Loss: 0.2660\n",
      "Epoch [14], Iter [1201/1212] Loss: 0.2568\n",
      "Epoch [15], Iter [1201/1212] Loss: 0.2833\n",
      "Epoch [16], Iter [1201/1212] Loss: 0.2762\n",
      "Epoch [17], Iter [1201/1212] Loss: 0.2473\n",
      "Epoch [18], Iter [1201/1212] Loss: 0.2723\n",
      "Epoch [19], Iter [1201/1212] Loss: 0.2599\n",
      "Epoch [20], Iter [1201/1212] Loss: 0.2555\n",
      "Epoch [21], Iter [1201/1212] Loss: 0.2874\n",
      "Epoch [22], Iter [1201/1212] Loss: 0.2600\n",
      "Epoch [23], Iter [1201/1212] Loss: 0.3026\n",
      "Epoch [24], Iter [1201/1212] Loss: 0.2442\n",
      "Epoch [25], Iter [1201/1212] Loss: 0.2794\n",
      "Epoch [26], Iter [1201/1212] Loss: 0.2499\n",
      "Epoch [27], Iter [1201/1212] Loss: 0.2488\n",
      "Epoch [28], Iter [1201/1212] Loss: 0.2536\n",
      "Epoch [29], Iter [1201/1212] Loss: 0.2367\n",
      "Epoch [30], Iter [1201/1212] Loss: 0.2340\n",
      "Test MSE: 0.28556180000305176\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 5\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d605fc5",
   "metadata": {},
   "source": [
    "Individual models for different cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fab9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0]).tolist()\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644a7011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6740\n",
      "Epoch [2], Iter [91/101] Loss: 0.5675\n",
      "Epoch [3], Iter [91/101] Loss: 0.5574\n",
      "Epoch [4], Iter [91/101] Loss: 0.5664\n",
      "Epoch [5], Iter [91/101] Loss: 0.5560\n",
      "Epoch [6], Iter [91/101] Loss: 0.5400\n",
      "Epoch [7], Iter [91/101] Loss: 0.5408\n",
      "Epoch [8], Iter [91/101] Loss: 0.5206\n",
      "Epoch [9], Iter [91/101] Loss: 0.4829\n",
      "Epoch [10], Iter [91/101] Loss: 0.4576\n",
      "Epoch [11], Iter [91/101] Loss: 0.5028\n",
      "Epoch [12], Iter [91/101] Loss: 0.4924\n",
      "Epoch [13], Iter [91/101] Loss: 0.4882\n",
      "Epoch [14], Iter [91/101] Loss: 0.4959\n",
      "Epoch [15], Iter [91/101] Loss: 0.4706\n",
      "Epoch [16], Iter [91/101] Loss: 0.4493\n",
      "Epoch [17], Iter [91/101] Loss: 0.4709\n",
      "Epoch [18], Iter [91/101] Loss: 0.4470\n",
      "Epoch [19], Iter [91/101] Loss: 0.4376\n",
      "Epoch [20], Iter [91/101] Loss: 0.4094\n",
      "Epoch [21], Iter [91/101] Loss: 0.4399\n",
      "Epoch [22], Iter [91/101] Loss: 0.4361\n",
      "Epoch [23], Iter [91/101] Loss: 0.4306\n",
      "Epoch [24], Iter [91/101] Loss: 0.4207\n",
      "Test MSE: 0.5284131169319153\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6817\n",
      "Epoch [2], Iter [91/101] Loss: 0.5756\n",
      "Epoch [3], Iter [91/101] Loss: 0.5729\n",
      "Epoch [4], Iter [91/101] Loss: 0.5992\n",
      "Epoch [5], Iter [91/101] Loss: 0.5566\n",
      "Epoch [6], Iter [91/101] Loss: 0.4968\n",
      "Epoch [7], Iter [91/101] Loss: 0.4991\n",
      "Epoch [8], Iter [91/101] Loss: 0.4925\n",
      "Epoch [9], Iter [91/101] Loss: 0.5095\n",
      "Epoch [10], Iter [91/101] Loss: 0.4841\n",
      "Epoch [11], Iter [91/101] Loss: 0.5099\n",
      "Epoch [12], Iter [91/101] Loss: 0.4698\n",
      "Epoch [13], Iter [91/101] Loss: 0.4660\n",
      "Epoch [14], Iter [91/101] Loss: 0.4623\n",
      "Epoch [15], Iter [91/101] Loss: 0.4330\n",
      "Epoch [16], Iter [91/101] Loss: 0.4384\n",
      "Epoch [17], Iter [91/101] Loss: 0.4526\n",
      "Epoch [18], Iter [91/101] Loss: 0.4044\n",
      "Test MSE: 0.5345911383628845\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.5852\n",
      "Epoch [2], Iter [91/101] Loss: 0.6341\n",
      "Epoch [3], Iter [91/101] Loss: 0.5318\n",
      "Epoch [4], Iter [91/101] Loss: 0.5489\n",
      "Epoch [5], Iter [91/101] Loss: 0.5337\n",
      "Epoch [6], Iter [91/101] Loss: 0.4938\n",
      "Epoch [7], Iter [91/101] Loss: 0.5452\n",
      "Epoch [8], Iter [91/101] Loss: 0.4737\n",
      "Epoch [9], Iter [91/101] Loss: 0.4761\n",
      "Epoch [10], Iter [91/101] Loss: 0.4732\n",
      "Epoch [11], Iter [91/101] Loss: 0.4547\n",
      "Epoch [12], Iter [91/101] Loss: 0.4682\n",
      "Epoch [13], Iter [91/101] Loss: 0.4634\n",
      "Epoch [14], Iter [91/101] Loss: 0.4295\n",
      "Epoch [15], Iter [91/101] Loss: 0.4640\n",
      "Epoch [16], Iter [91/101] Loss: 0.4100\n",
      "Epoch [17], Iter [91/101] Loss: 0.4313\n",
      "Epoch [18], Iter [91/101] Loss: 0.4394\n",
      "Epoch [19], Iter [91/101] Loss: 0.4289\n",
      "Epoch [20], Iter [91/101] Loss: 0.4105\n",
      "Epoch [21], Iter [91/101] Loss: 0.4261\n",
      "Test MSE: 0.5174954533576965\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6289\n",
      "Epoch [2], Iter [91/101] Loss: 0.5637\n",
      "Epoch [3], Iter [91/101] Loss: 0.5273\n",
      "Epoch [4], Iter [91/101] Loss: 0.5398\n",
      "Epoch [5], Iter [91/101] Loss: 0.5113\n",
      "Epoch [6], Iter [91/101] Loss: 0.4940\n",
      "Epoch [7], Iter [91/101] Loss: 0.4967\n",
      "Epoch [8], Iter [91/101] Loss: 0.4851\n",
      "Epoch [9], Iter [91/101] Loss: 0.4718\n",
      "Epoch [10], Iter [91/101] Loss: 0.5159\n",
      "Epoch [11], Iter [91/101] Loss: 0.4724\n",
      "Epoch [12], Iter [91/101] Loss: 0.4520\n",
      "Epoch [13], Iter [91/101] Loss: 0.4510\n",
      "Epoch [14], Iter [91/101] Loss: 0.4501\n",
      "Epoch [15], Iter [91/101] Loss: 0.4420\n",
      "Epoch [16], Iter [91/101] Loss: 0.4249\n",
      "Epoch [17], Iter [91/101] Loss: 0.4125\n",
      "Epoch [18], Iter [91/101] Loss: 0.4216\n",
      "Epoch [19], Iter [91/101] Loss: 0.4459\n",
      "Epoch [20], Iter [91/101] Loss: 0.4061\n",
      "Epoch [21], Iter [91/101] Loss: 0.4108\n",
      "Test MSE: 0.5132009387016296\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6715\n",
      "Epoch [2], Iter [91/101] Loss: 0.5646\n",
      "Epoch [3], Iter [91/101] Loss: 0.6376\n",
      "Epoch [4], Iter [91/101] Loss: 0.5283\n",
      "Epoch [5], Iter [91/101] Loss: 0.5530\n",
      "Epoch [6], Iter [91/101] Loss: 0.4994\n",
      "Epoch [7], Iter [91/101] Loss: 0.5596\n",
      "Epoch [8], Iter [91/101] Loss: 0.5250\n",
      "Epoch [9], Iter [91/101] Loss: 0.5281\n",
      "Epoch [10], Iter [91/101] Loss: 0.4948\n",
      "Epoch [11], Iter [91/101] Loss: 1.7032\n",
      "Epoch [12], Iter [91/101] Loss: 0.4846\n",
      "Epoch [13], Iter [91/101] Loss: 0.4535\n",
      "Epoch [14], Iter [91/101] Loss: 0.4838\n",
      "Epoch [15], Iter [91/101] Loss: 0.4594\n",
      "Epoch [16], Iter [91/101] Loss: 0.4603\n",
      "Epoch [17], Iter [91/101] Loss: 0.4291\n",
      "Epoch [18], Iter [91/101] Loss: 0.4354\n",
      "Epoch [19], Iter [91/101] Loss: 0.4255\n",
      "Epoch [20], Iter [91/101] Loss: 0.4322\n",
      "Test MSE: 0.543761134147644\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6884\n",
      "Epoch [2], Iter [91/101] Loss: 0.6133\n",
      "Epoch [3], Iter [91/101] Loss: 0.6099\n",
      "Epoch [4], Iter [91/101] Loss: 0.5847\n",
      "Epoch [5], Iter [91/101] Loss: 0.5604\n",
      "Epoch [6], Iter [91/101] Loss: 0.5583\n",
      "Epoch [7], Iter [91/101] Loss: 0.5454\n",
      "Epoch [8], Iter [91/101] Loss: 0.5360\n",
      "Epoch [9], Iter [91/101] Loss: 0.5740\n",
      "Epoch [10], Iter [91/101] Loss: 0.5064\n",
      "Epoch [11], Iter [91/101] Loss: 0.4964\n",
      "Epoch [12], Iter [91/101] Loss: 0.4895\n",
      "Epoch [13], Iter [91/101] Loss: 0.5203\n",
      "Epoch [14], Iter [91/101] Loss: 0.4922\n",
      "Epoch [15], Iter [91/101] Loss: 0.4625\n",
      "Epoch [16], Iter [91/101] Loss: 0.4664\n",
      "Epoch [17], Iter [91/101] Loss: 0.4663\n",
      "Epoch [18], Iter [91/101] Loss: 0.4592\n",
      "Epoch [19], Iter [91/101] Loss: 0.4502\n",
      "Epoch [20], Iter [91/101] Loss: 0.4313\n",
      "Epoch [21], Iter [91/101] Loss: 0.4594\n",
      "Epoch [22], Iter [91/101] Loss: 0.4177\n",
      "Epoch [23], Iter [91/101] Loss: 0.4544\n",
      "Test MSE: 0.5603032112121582\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6703\n",
      "Epoch [2], Iter [91/101] Loss: 0.6239\n",
      "Epoch [3], Iter [91/101] Loss: 0.5912\n",
      "Epoch [4], Iter [91/101] Loss: 0.6008\n",
      "Epoch [5], Iter [91/101] Loss: 0.5591\n",
      "Epoch [6], Iter [91/101] Loss: 0.5377\n",
      "Epoch [7], Iter [91/101] Loss: 0.5298\n",
      "Epoch [8], Iter [91/101] Loss: 0.5754\n",
      "Epoch [9], Iter [91/101] Loss: 0.5374\n",
      "Epoch [10], Iter [91/101] Loss: 0.5070\n",
      "Epoch [11], Iter [91/101] Loss: 0.5164\n",
      "Epoch [12], Iter [91/101] Loss: 0.4913\n",
      "Epoch [13], Iter [91/101] Loss: 0.4617\n",
      "Epoch [14], Iter [91/101] Loss: 0.5092\n",
      "Epoch [15], Iter [91/101] Loss: 0.4470\n",
      "Epoch [16], Iter [91/101] Loss: 0.4864\n",
      "Epoch [17], Iter [91/101] Loss: 0.4687\n",
      "Epoch [18], Iter [91/101] Loss: 0.4396\n",
      "Epoch [19], Iter [91/101] Loss: 0.4470\n",
      "Epoch [20], Iter [91/101] Loss: 0.4620\n",
      "Epoch [21], Iter [91/101] Loss: 0.4284\n",
      "Epoch [22], Iter [91/101] Loss: 0.4263\n",
      "Epoch [23], Iter [91/101] Loss: 0.4150\n",
      "Test MSE: 0.5613496899604797\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6886\n",
      "Epoch [2], Iter [91/101] Loss: 0.6839\n",
      "Epoch [3], Iter [91/101] Loss: 0.5782\n",
      "Epoch [4], Iter [91/101] Loss: 0.5629\n",
      "Epoch [5], Iter [91/101] Loss: 0.6080\n",
      "Epoch [6], Iter [91/101] Loss: 0.5491\n",
      "Epoch [7], Iter [91/101] Loss: 0.5585\n",
      "Epoch [8], Iter [91/101] Loss: 0.5530\n",
      "Epoch [9], Iter [91/101] Loss: 0.5322\n",
      "Epoch [10], Iter [91/101] Loss: 0.5528\n",
      "Epoch [11], Iter [91/101] Loss: 0.5434\n",
      "Epoch [12], Iter [91/101] Loss: 0.5158\n",
      "Epoch [13], Iter [91/101] Loss: 0.4931\n",
      "Epoch [14], Iter [91/101] Loss: 0.5107\n",
      "Epoch [15], Iter [91/101] Loss: 0.5098\n",
      "Epoch [16], Iter [91/101] Loss: 0.5007\n",
      "Epoch [17], Iter [91/101] Loss: 0.4506\n",
      "Epoch [18], Iter [91/101] Loss: 0.4514\n",
      "Test MSE: 0.5870315432548523\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6674\n",
      "Epoch [2], Iter [91/101] Loss: 0.6515\n",
      "Epoch [3], Iter [91/101] Loss: 0.5704\n",
      "Epoch [4], Iter [91/101] Loss: 0.5547\n",
      "Epoch [5], Iter [91/101] Loss: 0.5485\n",
      "Epoch [6], Iter [91/101] Loss: 0.5835\n",
      "Epoch [7], Iter [91/101] Loss: 0.5465\n",
      "Epoch [8], Iter [91/101] Loss: 0.5900\n",
      "Epoch [9], Iter [91/101] Loss: 0.4967\n",
      "Epoch [10], Iter [91/101] Loss: 0.5105\n",
      "Epoch [11], Iter [91/101] Loss: 0.5466\n",
      "Epoch [12], Iter [91/101] Loss: 0.4951\n",
      "Epoch [13], Iter [91/101] Loss: 0.4586\n",
      "Epoch [14], Iter [91/101] Loss: 0.4928\n",
      "Epoch [15], Iter [91/101] Loss: 0.4689\n",
      "Epoch [16], Iter [91/101] Loss: 0.4645\n",
      "Epoch [17], Iter [91/101] Loss: 0.5084\n",
      "Epoch [18], Iter [91/101] Loss: 0.4389\n",
      "Epoch [19], Iter [91/101] Loss: 0.4520\n",
      "Epoch [20], Iter [91/101] Loss: 0.4683\n",
      "Epoch [21], Iter [91/101] Loss: 0.4333\n",
      "Test MSE: 0.5649827718734741\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Iter [91/101] Loss: 0.5877\n",
      "Epoch [3], Iter [91/101] Loss: 0.6459\n",
      "Epoch [4], Iter [91/101] Loss: 0.5500\n",
      "Epoch [5], Iter [91/101] Loss: 0.6005\n",
      "Epoch [6], Iter [91/101] Loss: 0.5573\n",
      "Epoch [7], Iter [91/101] Loss: 0.5670\n",
      "Epoch [8], Iter [91/101] Loss: 0.5714\n",
      "Epoch [9], Iter [91/101] Loss: 0.5446\n",
      "Epoch [10], Iter [91/101] Loss: 0.5150\n",
      "Epoch [11], Iter [91/101] Loss: 0.5462\n",
      "Epoch [12], Iter [91/101] Loss: 0.4828\n",
      "Epoch [13], Iter [91/101] Loss: 0.4996\n",
      "Epoch [14], Iter [91/101] Loss: 0.5065\n",
      "Epoch [15], Iter [91/101] Loss: 0.5058\n",
      "Epoch [16], Iter [91/101] Loss: 0.5007\n",
      "Epoch [17], Iter [91/101] Loss: 0.4582\n",
      "Epoch [18], Iter [91/101] Loss: 0.4525\n",
      "Epoch [19], Iter [91/101] Loss: 0.4971\n",
      "Epoch [20], Iter [91/101] Loss: 0.4593\n",
      "Epoch [21], Iter [91/101] Loss: 0.4750\n",
      "Epoch [22], Iter [91/101] Loss: 0.4424\n",
      "Test MSE: 0.6790149807929993\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6702\n",
      "Epoch [2], Iter [91/101] Loss: 0.5893\n",
      "Epoch [3], Iter [91/101] Loss: 0.6073\n",
      "Epoch [4], Iter [91/101] Loss: 0.6260\n",
      "Epoch [5], Iter [91/101] Loss: 0.5739\n",
      "Epoch [6], Iter [91/101] Loss: 0.5730\n",
      "Epoch [7], Iter [91/101] Loss: 0.5770\n",
      "Epoch [8], Iter [91/101] Loss: 0.5151\n",
      "Epoch [9], Iter [91/101] Loss: 0.5449\n",
      "Epoch [10], Iter [91/101] Loss: 0.5630\n",
      "Epoch [11], Iter [91/101] Loss: 0.5509\n",
      "Epoch [12], Iter [91/101] Loss: 0.5151\n",
      "Epoch [13], Iter [91/101] Loss: 0.5364\n",
      "Epoch [14], Iter [91/101] Loss: 0.5216\n",
      "Epoch [15], Iter [91/101] Loss: 0.4795\n",
      "Epoch [16], Iter [91/101] Loss: 0.4856\n",
      "Epoch [17], Iter [91/101] Loss: 0.4620\n",
      "Epoch [18], Iter [91/101] Loss: 0.4902\n",
      "Epoch [19], Iter [91/101] Loss: 0.4917\n",
      "Epoch [20], Iter [91/101] Loss: 0.4694\n",
      "Epoch [21], Iter [91/101] Loss: 0.4711\n",
      "Test MSE: 0.5584151148796082\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6681\n",
      "Epoch [2], Iter [91/101] Loss: 0.6214\n",
      "Epoch [3], Iter [91/101] Loss: 0.5802\n",
      "Epoch [4], Iter [91/101] Loss: 0.6026\n",
      "Epoch [5], Iter [91/101] Loss: 0.6010\n",
      "Epoch [6], Iter [91/101] Loss: 0.5689\n",
      "Epoch [7], Iter [91/101] Loss: 0.5771\n",
      "Epoch [8], Iter [91/101] Loss: 0.6009\n",
      "Epoch [9], Iter [91/101] Loss: 0.5452\n",
      "Epoch [10], Iter [91/101] Loss: 0.5244\n",
      "Epoch [11], Iter [91/101] Loss: 0.4910\n",
      "Epoch [12], Iter [91/101] Loss: 0.5045\n",
      "Epoch [13], Iter [91/101] Loss: 0.4679\n",
      "Epoch [14], Iter [91/101] Loss: 0.4790\n",
      "Epoch [15], Iter [91/101] Loss: 0.4605\n",
      "Epoch [16], Iter [91/101] Loss: 0.4552\n",
      "Epoch [17], Iter [91/101] Loss: 0.4578\n",
      "Epoch [18], Iter [91/101] Loss: 0.4425\n",
      "Epoch [19], Iter [91/101] Loss: 0.4977\n",
      "Epoch [20], Iter [91/101] Loss: 0.4584\n",
      "Test MSE: 0.5490121841430664\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6804\n",
      "Epoch [2], Iter [91/101] Loss: 0.6243\n",
      "Epoch [3], Iter [91/101] Loss: 0.5818\n",
      "Epoch [4], Iter [91/101] Loss: 0.5373\n",
      "Epoch [5], Iter [91/101] Loss: 0.5605\n",
      "Epoch [6], Iter [91/101] Loss: 0.5342\n",
      "Epoch [7], Iter [91/101] Loss: 0.5370\n",
      "Epoch [8], Iter [91/101] Loss: 0.5049\n",
      "Epoch [9], Iter [91/101] Loss: 0.4984\n",
      "Epoch [10], Iter [91/101] Loss: 0.4820\n",
      "Epoch [11], Iter [91/101] Loss: 0.5489\n",
      "Epoch [12], Iter [91/101] Loss: 0.4727\n",
      "Epoch [13], Iter [91/101] Loss: 0.4658\n",
      "Epoch [14], Iter [91/101] Loss: 0.4632\n",
      "Epoch [15], Iter [91/101] Loss: 0.4743\n",
      "Epoch [16], Iter [91/101] Loss: 0.4353\n",
      "Epoch [17], Iter [91/101] Loss: 0.4395\n",
      "Epoch [18], Iter [91/101] Loss: 0.4796\n",
      "Test MSE: 0.5348000526428223\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6978\n",
      "Epoch [2], Iter [91/101] Loss: 0.5646\n",
      "Epoch [3], Iter [91/101] Loss: 0.6597\n",
      "Epoch [4], Iter [91/101] Loss: 0.5619\n",
      "Epoch [5], Iter [91/101] Loss: 0.5259\n",
      "Epoch [6], Iter [91/101] Loss: 0.5550\n",
      "Epoch [7], Iter [91/101] Loss: 0.5073\n",
      "Epoch [8], Iter [91/101] Loss: 0.4834\n",
      "Epoch [9], Iter [91/101] Loss: 0.4948\n",
      "Epoch [10], Iter [91/101] Loss: 0.4871\n",
      "Epoch [11], Iter [91/101] Loss: 0.4672\n",
      "Epoch [12], Iter [91/101] Loss: 0.4827\n",
      "Epoch [13], Iter [91/101] Loss: 0.4449\n",
      "Epoch [14], Iter [91/101] Loss: 0.4759\n",
      "Epoch [15], Iter [91/101] Loss: 0.4408\n",
      "Epoch [16], Iter [91/101] Loss: 0.4464\n",
      "Epoch [17], Iter [91/101] Loss: 0.4510\n",
      "Epoch [18], Iter [91/101] Loss: 0.4271\n",
      "Epoch [19], Iter [91/101] Loss: 0.4396\n",
      "Epoch [20], Iter [91/101] Loss: 0.4416\n",
      "Test MSE: 0.5299809575080872\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6632\n",
      "Epoch [2], Iter [91/101] Loss: 0.5635\n",
      "Epoch [3], Iter [91/101] Loss: 0.5546\n",
      "Epoch [4], Iter [91/101] Loss: 0.5252\n",
      "Epoch [5], Iter [91/101] Loss: 0.5543\n",
      "Epoch [6], Iter [91/101] Loss: 0.5576\n",
      "Epoch [7], Iter [91/101] Loss: 0.5388\n",
      "Epoch [8], Iter [91/101] Loss: 0.4623\n",
      "Epoch [9], Iter [91/101] Loss: 0.5010\n",
      "Epoch [10], Iter [91/101] Loss: 0.5153\n",
      "Epoch [11], Iter [91/101] Loss: 0.4747\n",
      "Epoch [12], Iter [91/101] Loss: 0.4581\n",
      "Epoch [13], Iter [91/101] Loss: 0.4850\n",
      "Epoch [14], Iter [91/101] Loss: 0.4762\n",
      "Epoch [15], Iter [91/101] Loss: 0.4655\n",
      "Epoch [16], Iter [91/101] Loss: 0.4448\n",
      "Epoch [17], Iter [91/101] Loss: 0.4476\n",
      "Epoch [18], Iter [91/101] Loss: 0.4551\n",
      "Epoch [19], Iter [91/101] Loss: 0.4341\n",
      "Epoch [20], Iter [91/101] Loss: 0.3980\n",
      "Epoch [21], Iter [91/101] Loss: 0.4016\n",
      "Epoch [22], Iter [91/101] Loss: 0.4164\n",
      "Epoch [23], Iter [91/101] Loss: 0.4325\n",
      "Test MSE: 0.5219917893409729\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6497\n",
      "Epoch [2], Iter [91/101] Loss: 0.5966\n",
      "Epoch [3], Iter [91/101] Loss: 0.5753\n",
      "Epoch [4], Iter [91/101] Loss: 0.5573\n",
      "Epoch [5], Iter [91/101] Loss: 0.5187\n",
      "Epoch [6], Iter [91/101] Loss: 0.5514\n",
      "Epoch [7], Iter [91/101] Loss: 0.4944\n",
      "Epoch [8], Iter [91/101] Loss: 0.5066\n",
      "Epoch [9], Iter [91/101] Loss: 0.4930\n",
      "Epoch [10], Iter [91/101] Loss: 0.4639\n",
      "Epoch [11], Iter [91/101] Loss: 0.4511\n",
      "Epoch [12], Iter [91/101] Loss: 0.4387\n",
      "Epoch [13], Iter [91/101] Loss: 0.4705\n",
      "Epoch [14], Iter [91/101] Loss: 0.4396\n",
      "Epoch [15], Iter [91/101] Loss: 0.4201\n",
      "Epoch [16], Iter [91/101] Loss: 0.4462\n",
      "Epoch [17], Iter [91/101] Loss: 0.4591\n",
      "Epoch [18], Iter [91/101] Loss: 0.4259\n",
      "Test MSE: 0.5155255794525146\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6313\n",
      "Epoch [2], Iter [91/101] Loss: 0.5959\n",
      "Epoch [3], Iter [91/101] Loss: 0.6123\n",
      "Epoch [4], Iter [91/101] Loss: 0.5532\n",
      "Epoch [5], Iter [91/101] Loss: 0.5564\n",
      "Epoch [6], Iter [91/101] Loss: 0.5033\n",
      "Epoch [7], Iter [91/101] Loss: 0.5290\n",
      "Epoch [8], Iter [91/101] Loss: 0.5117\n",
      "Epoch [9], Iter [91/101] Loss: 0.4863\n",
      "Epoch [10], Iter [91/101] Loss: 0.5214\n",
      "Epoch [11], Iter [91/101] Loss: 0.5040\n",
      "Epoch [12], Iter [91/101] Loss: 0.4855\n",
      "Epoch [13], Iter [91/101] Loss: 0.4868\n",
      "Epoch [14], Iter [91/101] Loss: 0.5003\n",
      "Epoch [15], Iter [91/101] Loss: 0.4592\n",
      "Epoch [16], Iter [91/101] Loss: 0.4472\n",
      "Epoch [17], Iter [91/101] Loss: 0.4565\n",
      "Epoch [18], Iter [91/101] Loss: 0.4171\n",
      "Epoch [19], Iter [91/101] Loss: 0.4600\n",
      "Epoch [20], Iter [91/101] Loss: 0.4103\n",
      "Epoch [21], Iter [91/101] Loss: 0.4372\n",
      "Epoch [22], Iter [91/101] Loss: 0.4084\n",
      "Epoch [23], Iter [91/101] Loss: 0.4069\n",
      "Epoch [24], Iter [91/101] Loss: 0.3965\n",
      "Test MSE: 0.5458301901817322\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6731\n",
      "Epoch [2], Iter [91/101] Loss: 0.6339\n",
      "Epoch [3], Iter [91/101] Loss: 0.5887\n",
      "Epoch [4], Iter [91/101] Loss: 0.5528\n",
      "Epoch [5], Iter [91/101] Loss: 0.5700\n",
      "Epoch [6], Iter [91/101] Loss: 0.5640\n",
      "Epoch [7], Iter [91/101] Loss: 0.5855\n",
      "Epoch [8], Iter [91/101] Loss: 0.5350\n",
      "Epoch [9], Iter [91/101] Loss: 0.5595\n",
      "Epoch [10], Iter [91/101] Loss: 0.5329\n",
      "Epoch [11], Iter [91/101] Loss: 0.5320\n",
      "Epoch [12], Iter [91/101] Loss: 0.5303\n",
      "Epoch [13], Iter [91/101] Loss: 0.5182\n",
      "Epoch [14], Iter [91/101] Loss: 0.5502\n",
      "Epoch [15], Iter [91/101] Loss: 0.4732\n",
      "Epoch [16], Iter [91/101] Loss: 0.4871\n",
      "Epoch [17], Iter [91/101] Loss: 0.4753\n",
      "Epoch [18], Iter [91/101] Loss: 0.4747\n",
      "Epoch [19], Iter [91/101] Loss: 0.4273\n",
      "Epoch [20], Iter [91/101] Loss: 0.4643\n",
      "Epoch [21], Iter [91/101] Loss: 0.4525\n",
      "Epoch [22], Iter [91/101] Loss: 0.4472\n",
      "Test MSE: 0.5612105131149292\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6149\n",
      "Epoch [2], Iter [91/101] Loss: 0.5926\n",
      "Epoch [3], Iter [91/101] Loss: 0.5880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Iter [91/101] Loss: 0.6029\n",
      "Epoch [5], Iter [91/101] Loss: 0.5757\n",
      "Epoch [6], Iter [91/101] Loss: 0.5616\n",
      "Epoch [7], Iter [91/101] Loss: 0.5725\n",
      "Epoch [8], Iter [91/101] Loss: 0.5308\n",
      "Epoch [9], Iter [91/101] Loss: 0.5067\n",
      "Epoch [10], Iter [91/101] Loss: 0.5386\n",
      "Epoch [11], Iter [91/101] Loss: 0.5263\n",
      "Epoch [12], Iter [91/101] Loss: 0.4832\n",
      "Epoch [13], Iter [91/101] Loss: 0.4713\n",
      "Epoch [14], Iter [91/101] Loss: 0.4593\n",
      "Epoch [15], Iter [91/101] Loss: 0.4881\n",
      "Test MSE: 0.5687409043312073\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6741\n",
      "Epoch [2], Iter [91/101] Loss: 0.6151\n",
      "Epoch [3], Iter [91/101] Loss: 0.6518\n",
      "Epoch [4], Iter [91/101] Loss: 0.5292\n",
      "Epoch [5], Iter [91/101] Loss: 0.6265\n",
      "Epoch [6], Iter [91/101] Loss: 0.5665\n",
      "Epoch [7], Iter [91/101] Loss: 0.5361\n",
      "Epoch [8], Iter [91/101] Loss: 0.5434\n",
      "Epoch [9], Iter [91/101] Loss: 0.5207\n",
      "Epoch [10], Iter [91/101] Loss: 0.5443\n",
      "Epoch [11], Iter [91/101] Loss: 0.5093\n",
      "Epoch [12], Iter [91/101] Loss: 0.5185\n",
      "Epoch [13], Iter [91/101] Loss: 0.4950\n",
      "Epoch [14], Iter [91/101] Loss: 0.4616\n",
      "Epoch [15], Iter [91/101] Loss: 0.4471\n",
      "Epoch [16], Iter [91/101] Loss: 0.4612\n",
      "Epoch [17], Iter [91/101] Loss: 0.5030\n",
      "Epoch [18], Iter [91/101] Loss: 0.4384\n",
      "Epoch [19], Iter [91/101] Loss: 0.4532\n",
      "Test MSE: 0.5997627973556519\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.8665\n",
      "Epoch [2], Iter [91/101] Loss: 0.5858\n",
      "Epoch [3], Iter [91/101] Loss: 0.6454\n",
      "Epoch [4], Iter [91/101] Loss: 0.5963\n",
      "Epoch [5], Iter [91/101] Loss: 0.5930\n",
      "Epoch [6], Iter [91/101] Loss: 0.5650\n",
      "Epoch [7], Iter [91/101] Loss: 0.5816\n",
      "Epoch [8], Iter [91/101] Loss: 0.5268\n",
      "Epoch [9], Iter [91/101] Loss: 0.5441\n",
      "Epoch [10], Iter [91/101] Loss: 0.5255\n",
      "Epoch [11], Iter [91/101] Loss: 0.5054\n",
      "Epoch [12], Iter [91/101] Loss: 0.4730\n",
      "Epoch [13], Iter [91/101] Loss: 0.4816\n",
      "Epoch [14], Iter [91/101] Loss: 0.4760\n",
      "Epoch [15], Iter [91/101] Loss: 0.4758\n",
      "Epoch [16], Iter [91/101] Loss: 0.4716\n",
      "Epoch [17], Iter [91/101] Loss: 0.4909\n",
      "Epoch [18], Iter [91/101] Loss: 0.4372\n",
      "Epoch [19], Iter [91/101] Loss: 0.4700\n",
      "Test MSE: 0.5630113482475281\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6605\n",
      "Epoch [2], Iter [91/101] Loss: 0.7033\n",
      "Epoch [3], Iter [91/101] Loss: 0.6140\n",
      "Epoch [4], Iter [91/101] Loss: 0.5711\n",
      "Epoch [5], Iter [91/101] Loss: 0.5851\n",
      "Epoch [6], Iter [91/101] Loss: 0.5346\n",
      "Epoch [7], Iter [91/101] Loss: 0.5285\n",
      "Epoch [8], Iter [91/101] Loss: 0.5483\n",
      "Epoch [9], Iter [91/101] Loss: 0.5175\n",
      "Epoch [10], Iter [91/101] Loss: 0.5198\n",
      "Epoch [11], Iter [91/101] Loss: 0.5133\n",
      "Epoch [12], Iter [91/101] Loss: 0.5147\n",
      "Epoch [13], Iter [91/101] Loss: 0.4840\n",
      "Epoch [14], Iter [91/101] Loss: 0.4826\n",
      "Epoch [15], Iter [91/101] Loss: 0.4657\n",
      "Epoch [16], Iter [91/101] Loss: 0.4548\n",
      "Epoch [17], Iter [91/101] Loss: 0.4792\n",
      "Epoch [18], Iter [91/101] Loss: 0.4438\n",
      "Epoch [19], Iter [91/101] Loss: 0.4419\n",
      "Epoch [20], Iter [91/101] Loss: 0.4312\n",
      "Epoch [21], Iter [91/101] Loss: 0.4530\n",
      "Epoch [22], Iter [91/101] Loss: 0.4475\n",
      "Test MSE: 0.6696649193763733\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6292\n",
      "Epoch [2], Iter [91/101] Loss: 0.5947\n",
      "Epoch [3], Iter [91/101] Loss: 0.6262\n",
      "Epoch [4], Iter [91/101] Loss: 0.5569\n",
      "Epoch [5], Iter [91/101] Loss: 0.5964\n",
      "Epoch [6], Iter [91/101] Loss: 0.5664\n",
      "Epoch [7], Iter [91/101] Loss: 0.5305\n",
      "Epoch [8], Iter [91/101] Loss: 0.5903\n",
      "Epoch [9], Iter [91/101] Loss: 0.5437\n",
      "Epoch [10], Iter [91/101] Loss: 0.5758\n",
      "Epoch [11], Iter [91/101] Loss: 0.5107\n",
      "Epoch [12], Iter [91/101] Loss: 0.5070\n",
      "Epoch [13], Iter [91/101] Loss: 0.4672\n",
      "Epoch [14], Iter [91/101] Loss: 0.5122\n",
      "Test MSE: 0.5678426027297974\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6396\n",
      "Epoch [2], Iter [91/101] Loss: 0.6334\n",
      "Epoch [3], Iter [91/101] Loss: 0.6018\n",
      "Epoch [4], Iter [91/101] Loss: 0.5608\n",
      "Epoch [5], Iter [91/101] Loss: 0.5632\n",
      "Epoch [6], Iter [91/101] Loss: 0.5677\n",
      "Epoch [7], Iter [91/101] Loss: 0.5584\n",
      "Epoch [8], Iter [91/101] Loss: 0.5111\n",
      "Epoch [9], Iter [91/101] Loss: 0.5175\n",
      "Epoch [10], Iter [91/101] Loss: 0.5330\n",
      "Epoch [11], Iter [91/101] Loss: 0.5286\n",
      "Epoch [12], Iter [91/101] Loss: 0.4975\n",
      "Epoch [13], Iter [91/101] Loss: 0.5108\n",
      "Epoch [14], Iter [91/101] Loss: 0.4922\n",
      "Epoch [15], Iter [91/101] Loss: 0.4723\n",
      "Epoch [16], Iter [91/101] Loss: 0.4885\n",
      "Epoch [17], Iter [91/101] Loss: 0.5414\n",
      "Epoch [18], Iter [91/101] Loss: 0.4507\n",
      "Epoch [19], Iter [91/101] Loss: 0.4624\n",
      "Test MSE: 0.5493724346160889\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6390\n",
      "Epoch [2], Iter [91/101] Loss: 0.5912\n",
      "Epoch [3], Iter [91/101] Loss: 0.6085\n",
      "Epoch [4], Iter [91/101] Loss: 0.5779\n",
      "Epoch [5], Iter [91/101] Loss: 0.5231\n",
      "Epoch [6], Iter [91/101] Loss: 0.5224\n",
      "Epoch [7], Iter [91/101] Loss: 0.5304\n",
      "Epoch [8], Iter [91/101] Loss: 0.5004\n",
      "Epoch [9], Iter [91/101] Loss: 0.5097\n",
      "Epoch [10], Iter [91/101] Loss: 0.4760\n",
      "Epoch [11], Iter [91/101] Loss: 0.4602\n",
      "Epoch [12], Iter [91/101] Loss: 0.4482\n",
      "Epoch [13], Iter [91/101] Loss: 0.4789\n",
      "Epoch [14], Iter [91/101] Loss: 0.4469\n",
      "Epoch [15], Iter [91/101] Loss: 0.4847\n",
      "Epoch [16], Iter [91/101] Loss: 0.4594\n",
      "Epoch [17], Iter [91/101] Loss: 0.4290\n",
      "Epoch [18], Iter [91/101] Loss: 0.4544\n",
      "Test MSE: 0.5322161912918091\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6934\n",
      "Epoch [2], Iter [91/101] Loss: 0.6202\n",
      "Epoch [3], Iter [91/101] Loss: 0.5920\n",
      "Epoch [4], Iter [91/101] Loss: 0.5376\n",
      "Epoch [5], Iter [91/101] Loss: 0.5895\n",
      "Epoch [6], Iter [91/101] Loss: 0.5172\n",
      "Epoch [7], Iter [91/101] Loss: 0.5202\n",
      "Epoch [8], Iter [91/101] Loss: 0.5310\n",
      "Epoch [9], Iter [91/101] Loss: 0.5232\n",
      "Epoch [10], Iter [91/101] Loss: 0.5288\n",
      "Epoch [11], Iter [91/101] Loss: 0.4971\n",
      "Epoch [12], Iter [91/101] Loss: 0.4844\n",
      "Epoch [13], Iter [91/101] Loss: 0.4617\n",
      "Epoch [14], Iter [91/101] Loss: 0.5054\n",
      "Epoch [15], Iter [91/101] Loss: 0.4655\n",
      "Epoch [16], Iter [91/101] Loss: 0.4709\n",
      "Epoch [17], Iter [91/101] Loss: 0.4730\n",
      "Epoch [18], Iter [91/101] Loss: 0.4709\n",
      "Epoch [19], Iter [91/101] Loss: 0.4324\n",
      "Test MSE: 0.5323395133018494\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6367\n",
      "Epoch [2], Iter [91/101] Loss: 0.6913\n",
      "Epoch [3], Iter [91/101] Loss: 0.5633\n",
      "Epoch [4], Iter [91/101] Loss: 0.5450\n",
      "Epoch [5], Iter [91/101] Loss: 0.5251\n",
      "Epoch [6], Iter [91/101] Loss: 0.5912\n",
      "Epoch [7], Iter [91/101] Loss: 0.5208\n",
      "Epoch [8], Iter [91/101] Loss: 0.5071\n",
      "Epoch [9], Iter [91/101] Loss: 0.4798\n",
      "Epoch [10], Iter [91/101] Loss: 0.5172\n",
      "Epoch [11], Iter [91/101] Loss: 0.5185\n",
      "Epoch [12], Iter [91/101] Loss: 0.4583\n",
      "Epoch [13], Iter [91/101] Loss: 0.4819\n",
      "Epoch [14], Iter [91/101] Loss: 0.4525\n",
      "Epoch [15], Iter [91/101] Loss: 0.4533\n",
      "Epoch [16], Iter [91/101] Loss: 0.4275\n",
      "Epoch [17], Iter [91/101] Loss: 0.4554\n",
      "Epoch [18], Iter [91/101] Loss: 0.4169\n",
      "Epoch [19], Iter [91/101] Loss: 0.4569\n",
      "Epoch [20], Iter [91/101] Loss: 0.4722\n",
      "Epoch [21], Iter [91/101] Loss: 0.4029\n",
      "Test MSE: 0.5196843147277832\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6216\n",
      "Epoch [2], Iter [91/101] Loss: 0.5683\n",
      "Epoch [3], Iter [91/101] Loss: 0.6281\n",
      "Epoch [4], Iter [91/101] Loss: 0.5762\n",
      "Epoch [5], Iter [91/101] Loss: 0.5713\n",
      "Epoch [6], Iter [91/101] Loss: 0.5083\n",
      "Epoch [7], Iter [91/101] Loss: 0.6385\n",
      "Epoch [8], Iter [91/101] Loss: 0.5194\n",
      "Epoch [9], Iter [91/101] Loss: 0.4984\n",
      "Epoch [10], Iter [91/101] Loss: 0.4854\n",
      "Epoch [11], Iter [91/101] Loss: 0.4778\n",
      "Epoch [12], Iter [91/101] Loss: 0.4658\n",
      "Epoch [13], Iter [91/101] Loss: 0.4608\n",
      "Epoch [14], Iter [91/101] Loss: 0.4928\n",
      "Epoch [15], Iter [91/101] Loss: 0.4437\n",
      "Epoch [16], Iter [91/101] Loss: 0.4647\n",
      "Epoch [17], Iter [91/101] Loss: 0.4489\n",
      "Epoch [18], Iter [91/101] Loss: 0.4360\n",
      "Epoch [19], Iter [91/101] Loss: 0.4276\n",
      "Epoch [20], Iter [91/101] Loss: 0.4203\n",
      "Epoch [21], Iter [91/101] Loss: 0.4159\n",
      "Epoch [22], Iter [91/101] Loss: 0.3990\n",
      "Epoch [23], Iter [91/101] Loss: 0.3865\n",
      "Epoch [24], Iter [91/101] Loss: 0.3875\n",
      "Test MSE: 0.514696478843689\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Iter [91/101] Loss: 0.6137\n",
      "Epoch [3], Iter [91/101] Loss: 0.5739\n",
      "Epoch [4], Iter [91/101] Loss: 0.5732\n",
      "Epoch [5], Iter [91/101] Loss: 0.5248\n",
      "Epoch [6], Iter [91/101] Loss: 0.5245\n",
      "Epoch [7], Iter [91/101] Loss: 0.5317\n",
      "Epoch [8], Iter [91/101] Loss: 0.5821\n",
      "Epoch [9], Iter [91/101] Loss: 0.5029\n",
      "Epoch [10], Iter [91/101] Loss: 0.4929\n",
      "Epoch [11], Iter [91/101] Loss: 0.4981\n",
      "Epoch [12], Iter [91/101] Loss: 0.4768\n",
      "Epoch [13], Iter [91/101] Loss: 0.5054\n",
      "Epoch [14], Iter [91/101] Loss: 0.4737\n",
      "Epoch [15], Iter [91/101] Loss: 0.4618\n",
      "Epoch [16], Iter [91/101] Loss: 0.4415\n",
      "Epoch [17], Iter [91/101] Loss: 1.5540\n",
      "Test MSE: 0.5481976270675659\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6146\n",
      "Epoch [2], Iter [91/101] Loss: 0.6168\n",
      "Epoch [3], Iter [91/101] Loss: 0.5948\n",
      "Epoch [4], Iter [91/101] Loss: 0.5728\n",
      "Epoch [5], Iter [91/101] Loss: 0.5694\n",
      "Epoch [6], Iter [91/101] Loss: 0.5905\n",
      "Epoch [7], Iter [91/101] Loss: 0.5604\n",
      "Epoch [8], Iter [91/101] Loss: 0.5492\n",
      "Epoch [9], Iter [91/101] Loss: 0.5691\n",
      "Epoch [10], Iter [91/101] Loss: 0.5068\n",
      "Epoch [11], Iter [91/101] Loss: 0.4981\n",
      "Epoch [12], Iter [91/101] Loss: 0.5243\n",
      "Epoch [13], Iter [91/101] Loss: 0.4720\n",
      "Epoch [14], Iter [91/101] Loss: 0.4863\n",
      "Epoch [15], Iter [91/101] Loss: 0.4995\n",
      "Epoch [16], Iter [91/101] Loss: 0.4862\n",
      "Epoch [17], Iter [91/101] Loss: 0.4883\n",
      "Epoch [18], Iter [91/101] Loss: 0.4655\n",
      "Epoch [19], Iter [91/101] Loss: 0.4491\n",
      "Epoch [20], Iter [91/101] Loss: 0.4384\n",
      "Test MSE: 0.5650482177734375\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7006\n",
      "Epoch [2], Iter [91/101] Loss: 0.5908\n",
      "Epoch [3], Iter [91/101] Loss: 2.1659\n",
      "Epoch [4], Iter [91/101] Loss: 0.5625\n",
      "Epoch [5], Iter [91/101] Loss: 0.5330\n",
      "Epoch [6], Iter [91/101] Loss: 0.5790\n",
      "Epoch [7], Iter [91/101] Loss: 0.5232\n",
      "Epoch [8], Iter [91/101] Loss: 0.5102\n",
      "Epoch [9], Iter [91/101] Loss: 0.5562\n",
      "Epoch [10], Iter [91/101] Loss: 0.4849\n",
      "Epoch [11], Iter [91/101] Loss: 0.4890\n",
      "Epoch [12], Iter [91/101] Loss: 0.4666\n",
      "Epoch [13], Iter [91/101] Loss: 0.4877\n",
      "Epoch [14], Iter [91/101] Loss: 0.4666\n",
      "Epoch [15], Iter [91/101] Loss: 0.4948\n",
      "Epoch [16], Iter [91/101] Loss: 0.4675\n",
      "Epoch [17], Iter [91/101] Loss: 0.4800\n",
      "Epoch [18], Iter [91/101] Loss: 0.4843\n",
      "Epoch [19], Iter [91/101] Loss: 0.4608\n",
      "Epoch [20], Iter [91/101] Loss: 0.4412\n",
      "Test MSE: 0.5631230473518372\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6856\n",
      "Epoch [2], Iter [91/101] Loss: 0.6440\n",
      "Epoch [3], Iter [91/101] Loss: 0.6122\n",
      "Epoch [4], Iter [91/101] Loss: 0.6108\n",
      "Epoch [5], Iter [91/101] Loss: 0.5780\n",
      "Epoch [6], Iter [91/101] Loss: 0.5805\n",
      "Epoch [7], Iter [91/101] Loss: 0.5413\n",
      "Epoch [8], Iter [91/101] Loss: 0.5598\n",
      "Epoch [9], Iter [91/101] Loss: 0.5007\n",
      "Epoch [10], Iter [91/101] Loss: 0.4776\n",
      "Epoch [11], Iter [91/101] Loss: 0.5283\n",
      "Epoch [12], Iter [91/101] Loss: 0.5334\n",
      "Epoch [13], Iter [91/101] Loss: 0.4792\n",
      "Epoch [14], Iter [91/101] Loss: 0.4617\n",
      "Epoch [15], Iter [91/101] Loss: 0.4573\n",
      "Epoch [16], Iter [91/101] Loss: 0.4635\n",
      "Epoch [17], Iter [91/101] Loss: 0.4634\n",
      "Epoch [18], Iter [91/101] Loss: 0.4950\n",
      "Epoch [19], Iter [91/101] Loss: 0.4448\n",
      "Epoch [20], Iter [91/101] Loss: 0.4521\n",
      "Epoch [21], Iter [91/101] Loss: 0.4160\n",
      "Test MSE: 0.5955309271812439\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6841\n",
      "Epoch [2], Iter [91/101] Loss: 0.6099\n",
      "Epoch [3], Iter [91/101] Loss: 0.6079\n",
      "Epoch [4], Iter [91/101] Loss: 0.6002\n",
      "Epoch [5], Iter [91/101] Loss: 0.5511\n",
      "Epoch [6], Iter [91/101] Loss: 0.5467\n",
      "Epoch [7], Iter [91/101] Loss: 0.5919\n",
      "Epoch [8], Iter [91/101] Loss: 0.5588\n",
      "Epoch [9], Iter [91/101] Loss: 0.5324\n",
      "Epoch [10], Iter [91/101] Loss: 0.4985\n",
      "Epoch [11], Iter [91/101] Loss: 0.5103\n",
      "Epoch [12], Iter [91/101] Loss: 0.4676\n",
      "Epoch [13], Iter [91/101] Loss: 0.4806\n",
      "Epoch [14], Iter [91/101] Loss: 0.4970\n",
      "Epoch [15], Iter [91/101] Loss: 0.4858\n",
      "Epoch [16], Iter [91/101] Loss: 0.4349\n",
      "Epoch [17], Iter [91/101] Loss: 0.5096\n",
      "Epoch [18], Iter [91/101] Loss: 0.4917\n",
      "Test MSE: 0.5673145055770874\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6980\n",
      "Epoch [2], Iter [91/101] Loss: 0.6530\n",
      "Epoch [3], Iter [91/101] Loss: 0.6446\n",
      "Epoch [4], Iter [91/101] Loss: 0.5829\n",
      "Epoch [5], Iter [91/101] Loss: 0.5847\n",
      "Epoch [6], Iter [91/101] Loss: 0.5755\n",
      "Epoch [7], Iter [91/101] Loss: 0.5836\n",
      "Epoch [8], Iter [91/101] Loss: 0.5620\n",
      "Epoch [9], Iter [91/101] Loss: 0.5287\n",
      "Epoch [10], Iter [91/101] Loss: 0.5171\n",
      "Epoch [11], Iter [91/101] Loss: 0.5286\n",
      "Epoch [12], Iter [91/101] Loss: 0.5157\n",
      "Epoch [13], Iter [91/101] Loss: 0.5019\n",
      "Epoch [14], Iter [91/101] Loss: 0.5179\n",
      "Epoch [15], Iter [91/101] Loss: 0.5035\n",
      "Epoch [16], Iter [91/101] Loss: 0.4688\n",
      "Epoch [17], Iter [91/101] Loss: 0.4677\n",
      "Epoch [18], Iter [91/101] Loss: 0.4493\n",
      "Test MSE: 0.6757623553276062\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6679\n",
      "Epoch [2], Iter [91/101] Loss: 0.6715\n",
      "Epoch [3], Iter [91/101] Loss: 0.6115\n",
      "Epoch [4], Iter [91/101] Loss: 0.6201\n",
      "Epoch [5], Iter [91/101] Loss: 0.5631\n",
      "Epoch [6], Iter [91/101] Loss: 0.5833\n",
      "Epoch [7], Iter [91/101] Loss: 0.5296\n",
      "Epoch [8], Iter [91/101] Loss: 0.5015\n",
      "Epoch [9], Iter [91/101] Loss: 0.5407\n",
      "Epoch [10], Iter [91/101] Loss: 0.5679\n",
      "Epoch [11], Iter [91/101] Loss: 0.5273\n",
      "Epoch [12], Iter [91/101] Loss: 0.5080\n",
      "Epoch [13], Iter [91/101] Loss: 0.5045\n",
      "Epoch [14], Iter [91/101] Loss: 0.4827\n",
      "Epoch [15], Iter [91/101] Loss: 0.4797\n",
      "Epoch [16], Iter [91/101] Loss: 0.4766\n",
      "Epoch [17], Iter [91/101] Loss: 0.4892\n",
      "Epoch [18], Iter [91/101] Loss: 0.4561\n",
      "Epoch [19], Iter [91/101] Loss: 0.4586\n",
      "Test MSE: 0.5595974326133728\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6508\n",
      "Epoch [2], Iter [91/101] Loss: 0.5861\n",
      "Epoch [3], Iter [91/101] Loss: 0.6075\n",
      "Epoch [4], Iter [91/101] Loss: 0.5688\n",
      "Epoch [5], Iter [91/101] Loss: 0.5880\n",
      "Epoch [6], Iter [91/101] Loss: 0.5358\n",
      "Epoch [7], Iter [91/101] Loss: 0.5410\n",
      "Epoch [8], Iter [91/101] Loss: 0.5246\n",
      "Epoch [9], Iter [91/101] Loss: 0.5068\n",
      "Epoch [10], Iter [91/101] Loss: 0.5089\n",
      "Epoch [11], Iter [91/101] Loss: 0.5056\n",
      "Epoch [12], Iter [91/101] Loss: 0.4767\n",
      "Epoch [13], Iter [91/101] Loss: 0.4866\n",
      "Epoch [14], Iter [91/101] Loss: 0.4837\n",
      "Epoch [15], Iter [91/101] Loss: 0.4488\n",
      "Epoch [16], Iter [91/101] Loss: 0.5238\n",
      "Epoch [17], Iter [91/101] Loss: 0.4556\n",
      "Test MSE: 0.548316478729248\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6901\n",
      "Epoch [2], Iter [91/101] Loss: 0.6785\n",
      "Epoch [3], Iter [91/101] Loss: 0.6035\n",
      "Epoch [4], Iter [91/101] Loss: 0.5774\n",
      "Epoch [5], Iter [91/101] Loss: 0.6108\n",
      "Epoch [6], Iter [91/101] Loss: 0.5266\n",
      "Epoch [7], Iter [91/101] Loss: 0.4941\n",
      "Epoch [8], Iter [91/101] Loss: 0.4916\n",
      "Epoch [9], Iter [91/101] Loss: 0.5178\n",
      "Epoch [10], Iter [91/101] Loss: 0.4504\n",
      "Epoch [11], Iter [91/101] Loss: 0.4929\n",
      "Epoch [12], Iter [91/101] Loss: 0.4676\n",
      "Epoch [13], Iter [91/101] Loss: 0.4678\n",
      "Epoch [14], Iter [91/101] Loss: 0.4802\n",
      "Epoch [15], Iter [91/101] Loss: 0.4878\n",
      "Epoch [16], Iter [91/101] Loss: 0.4647\n",
      "Epoch [17], Iter [91/101] Loss: 0.4744\n",
      "Epoch [18], Iter [91/101] Loss: 0.4261\n",
      "Epoch [19], Iter [91/101] Loss: 0.4174\n",
      "Epoch [20], Iter [91/101] Loss: 0.4397\n",
      "Test MSE: 0.5279436707496643\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6385\n",
      "Epoch [2], Iter [91/101] Loss: 0.6006\n",
      "Epoch [3], Iter [91/101] Loss: 0.5895\n",
      "Epoch [4], Iter [91/101] Loss: 0.5101\n",
      "Epoch [5], Iter [91/101] Loss: 0.5228\n",
      "Epoch [6], Iter [91/101] Loss: 0.5104\n",
      "Epoch [7], Iter [91/101] Loss: 0.5535\n",
      "Epoch [8], Iter [91/101] Loss: 0.5275\n",
      "Epoch [9], Iter [91/101] Loss: 0.4853\n",
      "Epoch [10], Iter [91/101] Loss: 0.4534\n",
      "Epoch [11], Iter [91/101] Loss: 0.4639\n",
      "Epoch [12], Iter [91/101] Loss: 0.4804\n",
      "Epoch [13], Iter [91/101] Loss: 0.4858\n",
      "Epoch [14], Iter [91/101] Loss: 0.4407\n",
      "Epoch [15], Iter [91/101] Loss: 0.4516\n",
      "Epoch [16], Iter [91/101] Loss: 0.4499\n",
      "Epoch [17], Iter [91/101] Loss: 0.4495\n",
      "Epoch [18], Iter [91/101] Loss: 0.4510\n",
      "Epoch [19], Iter [91/101] Loss: 0.4248\n",
      "Test MSE: 0.5390440821647644\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Iter [91/101] Loss: 0.6599\n",
      "Epoch [2], Iter [91/101] Loss: 0.5963\n",
      "Epoch [3], Iter [91/101] Loss: 0.5607\n",
      "Epoch [4], Iter [91/101] Loss: 0.5499\n",
      "Epoch [5], Iter [91/101] Loss: 0.5505\n",
      "Epoch [6], Iter [91/101] Loss: 0.5310\n",
      "Epoch [7], Iter [91/101] Loss: 0.5104\n",
      "Epoch [8], Iter [91/101] Loss: 0.5005\n",
      "Epoch [9], Iter [91/101] Loss: 0.4630\n",
      "Epoch [10], Iter [91/101] Loss: 0.4634\n",
      "Epoch [11], Iter [91/101] Loss: 0.4713\n",
      "Epoch [12], Iter [91/101] Loss: 0.4796\n",
      "Epoch [13], Iter [91/101] Loss: 0.4894\n",
      "Epoch [14], Iter [91/101] Loss: 0.4373\n",
      "Epoch [15], Iter [91/101] Loss: 0.4507\n",
      "Epoch [16], Iter [91/101] Loss: 0.4425\n",
      "Epoch [17], Iter [91/101] Loss: 0.4386\n",
      "Epoch [18], Iter [91/101] Loss: 0.4172\n",
      "Epoch [19], Iter [91/101] Loss: 0.4283\n",
      "Epoch [20], Iter [91/101] Loss: 0.4105\n",
      "Epoch [21], Iter [91/101] Loss: 0.4402\n",
      "Epoch [22], Iter [91/101] Loss: 0.3964\n",
      "Epoch [23], Iter [91/101] Loss: 0.3993\n",
      "Epoch [24], Iter [91/101] Loss: 0.3703\n",
      "Test MSE: 0.5247753262519836\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6454\n",
      "Epoch [2], Iter [91/101] Loss: 0.6117\n",
      "Epoch [3], Iter [91/101] Loss: 0.5601\n",
      "Epoch [4], Iter [91/101] Loss: 0.5333\n",
      "Epoch [5], Iter [91/101] Loss: 0.5566\n",
      "Epoch [6], Iter [91/101] Loss: 0.5086\n",
      "Epoch [7], Iter [91/101] Loss: 0.5169\n",
      "Epoch [8], Iter [91/101] Loss: 0.4777\n",
      "Epoch [9], Iter [91/101] Loss: 0.4824\n",
      "Epoch [10], Iter [91/101] Loss: 0.4838\n",
      "Epoch [11], Iter [91/101] Loss: 0.4720\n",
      "Epoch [12], Iter [91/101] Loss: 0.5385\n",
      "Epoch [13], Iter [91/101] Loss: 0.4816\n",
      "Epoch [14], Iter [91/101] Loss: 0.4476\n",
      "Epoch [15], Iter [91/101] Loss: 0.4442\n",
      "Epoch [16], Iter [91/101] Loss: 0.4372\n",
      "Epoch [17], Iter [91/101] Loss: 0.4288\n",
      "Epoch [18], Iter [91/101] Loss: 0.4039\n",
      "Epoch [19], Iter [91/101] Loss: 0.4307\n",
      "Epoch [20], Iter [91/101] Loss: 0.4191\n",
      "Epoch [21], Iter [91/101] Loss: 0.3915\n",
      "Epoch [22], Iter [91/101] Loss: 0.4167\n",
      "Test MSE: 0.5141370892524719\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6288\n",
      "Epoch [2], Iter [91/101] Loss: 0.5865\n",
      "Epoch [3], Iter [91/101] Loss: 0.5830\n",
      "Epoch [4], Iter [91/101] Loss: 0.5399\n",
      "Epoch [5], Iter [91/101] Loss: 0.5171\n",
      "Epoch [6], Iter [91/101] Loss: 0.4928\n",
      "Epoch [7], Iter [91/101] Loss: 0.5285\n",
      "Epoch [8], Iter [91/101] Loss: 0.5147\n",
      "Epoch [9], Iter [91/101] Loss: 0.5171\n",
      "Epoch [10], Iter [91/101] Loss: 0.4758\n",
      "Epoch [11], Iter [91/101] Loss: 0.4788\n",
      "Epoch [12], Iter [91/101] Loss: 0.5015\n",
      "Epoch [13], Iter [91/101] Loss: 0.4913\n",
      "Epoch [14], Iter [91/101] Loss: 0.4530\n",
      "Epoch [15], Iter [91/101] Loss: 0.4739\n",
      "Epoch [16], Iter [91/101] Loss: 0.4196\n",
      "Epoch [17], Iter [91/101] Loss: 0.4973\n",
      "Epoch [18], Iter [91/101] Loss: 0.4283\n",
      "Epoch [19], Iter [91/101] Loss: 0.4467\n",
      "Epoch [20], Iter [91/101] Loss: 0.4102\n",
      "Epoch [21], Iter [91/101] Loss: 0.4143\n",
      "Test MSE: 0.5337130427360535\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7077\n",
      "Epoch [2], Iter [91/101] Loss: 0.7025\n",
      "Epoch [3], Iter [91/101] Loss: 0.6276\n",
      "Epoch [4], Iter [91/101] Loss: 0.5960\n",
      "Epoch [5], Iter [91/101] Loss: 0.5597\n",
      "Epoch [6], Iter [91/101] Loss: 0.5583\n",
      "Epoch [7], Iter [91/101] Loss: 0.5624\n",
      "Epoch [8], Iter [91/101] Loss: 0.5460\n",
      "Epoch [9], Iter [91/101] Loss: 0.5287\n",
      "Epoch [10], Iter [91/101] Loss: 0.5267\n",
      "Epoch [11], Iter [91/101] Loss: 0.5445\n",
      "Epoch [12], Iter [91/101] Loss: 0.5102\n",
      "Epoch [13], Iter [91/101] Loss: 0.5217\n",
      "Epoch [14], Iter [91/101] Loss: 0.4799\n",
      "Epoch [15], Iter [91/101] Loss: 0.4506\n",
      "Epoch [16], Iter [91/101] Loss: 0.5181\n",
      "Epoch [17], Iter [91/101] Loss: 0.5182\n",
      "Epoch [18], Iter [91/101] Loss: 0.4829\n",
      "Epoch [19], Iter [91/101] Loss: 0.4941\n",
      "Epoch [20], Iter [91/101] Loss: 0.4658\n",
      "Epoch [21], Iter [91/101] Loss: 0.4388\n",
      "Epoch [22], Iter [91/101] Loss: 0.4325\n",
      "Test MSE: 0.5589183568954468\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6562\n",
      "Epoch [2], Iter [91/101] Loss: 0.6258\n",
      "Epoch [3], Iter [91/101] Loss: 0.5982\n",
      "Epoch [4], Iter [91/101] Loss: 0.5989\n",
      "Epoch [5], Iter [91/101] Loss: 0.5981\n",
      "Epoch [6], Iter [91/101] Loss: 0.5834\n",
      "Epoch [7], Iter [91/101] Loss: 0.5204\n",
      "Epoch [8], Iter [91/101] Loss: 0.5115\n",
      "Epoch [9], Iter [91/101] Loss: 0.5373\n",
      "Epoch [10], Iter [91/101] Loss: 0.5164\n",
      "Epoch [11], Iter [91/101] Loss: 0.5248\n",
      "Epoch [12], Iter [91/101] Loss: 0.5262\n",
      "Epoch [13], Iter [91/101] Loss: 0.4965\n",
      "Epoch [14], Iter [91/101] Loss: 0.5422\n",
      "Epoch [15], Iter [91/101] Loss: 0.5077\n",
      "Epoch [16], Iter [91/101] Loss: 0.4699\n",
      "Epoch [17], Iter [91/101] Loss: 0.4739\n",
      "Epoch [18], Iter [91/101] Loss: 0.4400\n",
      "Epoch [19], Iter [91/101] Loss: 0.4526\n",
      "Epoch [20], Iter [91/101] Loss: 0.4411\n",
      "Test MSE: 0.5705416798591614\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7098\n",
      "Epoch [2], Iter [91/101] Loss: 0.6191\n",
      "Epoch [3], Iter [91/101] Loss: 0.5813\n",
      "Epoch [4], Iter [91/101] Loss: 0.5907\n",
      "Epoch [5], Iter [91/101] Loss: 0.5763\n",
      "Epoch [6], Iter [91/101] Loss: 0.5252\n",
      "Epoch [7], Iter [91/101] Loss: 0.5437\n",
      "Epoch [8], Iter [91/101] Loss: 0.5306\n",
      "Epoch [9], Iter [91/101] Loss: 0.4927\n",
      "Epoch [10], Iter [91/101] Loss: 0.5081\n",
      "Epoch [11], Iter [91/101] Loss: 0.5199\n",
      "Epoch [12], Iter [91/101] Loss: 0.4992\n",
      "Epoch [13], Iter [91/101] Loss: 0.5264\n",
      "Epoch [14], Iter [91/101] Loss: 0.4849\n",
      "Epoch [15], Iter [91/101] Loss: 0.4794\n",
      "Epoch [16], Iter [91/101] Loss: 0.4744\n",
      "Epoch [17], Iter [91/101] Loss: 0.4482\n",
      "Epoch [18], Iter [91/101] Loss: 0.4192\n",
      "Epoch [19], Iter [91/101] Loss: 0.4688\n",
      "Epoch [20], Iter [91/101] Loss: 0.4325\n",
      "Epoch [21], Iter [91/101] Loss: 0.4304\n",
      "Test MSE: 0.608518660068512\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6781\n",
      "Epoch [2], Iter [91/101] Loss: 0.6797\n",
      "Epoch [3], Iter [91/101] Loss: 0.6053\n",
      "Epoch [4], Iter [91/101] Loss: 0.6010\n",
      "Epoch [5], Iter [91/101] Loss: 0.5891\n",
      "Epoch [6], Iter [91/101] Loss: 0.5659\n",
      "Epoch [7], Iter [91/101] Loss: 0.5730\n",
      "Epoch [8], Iter [91/101] Loss: 0.5300\n",
      "Epoch [9], Iter [91/101] Loss: 0.5521\n",
      "Epoch [10], Iter [91/101] Loss: 0.5132\n",
      "Epoch [11], Iter [91/101] Loss: 0.4887\n",
      "Epoch [12], Iter [91/101] Loss: 0.5445\n",
      "Epoch [13], Iter [91/101] Loss: 0.5068\n",
      "Epoch [14], Iter [91/101] Loss: 0.4517\n",
      "Epoch [15], Iter [91/101] Loss: 0.4514\n",
      "Epoch [16], Iter [91/101] Loss: 0.4728\n",
      "Epoch [17], Iter [91/101] Loss: 0.4724\n",
      "Epoch [18], Iter [91/101] Loss: 0.4479\n",
      "Epoch [19], Iter [91/101] Loss: 0.4284\n",
      "Epoch [20], Iter [91/101] Loss: 0.4282\n",
      "Epoch [21], Iter [91/101] Loss: 0.4434\n",
      "Epoch [22], Iter [91/101] Loss: 0.4477\n",
      "Epoch [23], Iter [91/101] Loss: 0.4377\n",
      "Test MSE: 0.5652603507041931\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7973\n",
      "Epoch [2], Iter [91/101] Loss: 0.5768\n",
      "Epoch [3], Iter [91/101] Loss: 0.5871\n",
      "Epoch [4], Iter [91/101] Loss: 0.6139\n",
      "Epoch [5], Iter [91/101] Loss: 0.5979\n",
      "Epoch [6], Iter [91/101] Loss: 0.5440\n",
      "Epoch [7], Iter [91/101] Loss: 0.5492\n",
      "Epoch [8], Iter [91/101] Loss: 0.5879\n",
      "Epoch [9], Iter [91/101] Loss: 0.5551\n",
      "Epoch [10], Iter [91/101] Loss: 0.5128\n",
      "Epoch [11], Iter [91/101] Loss: 0.5398\n",
      "Epoch [12], Iter [91/101] Loss: 0.5337\n",
      "Epoch [13], Iter [91/101] Loss: 0.5063\n",
      "Epoch [14], Iter [91/101] Loss: 0.4818\n",
      "Epoch [15], Iter [91/101] Loss: 0.4826\n",
      "Epoch [16], Iter [91/101] Loss: 0.5114\n",
      "Epoch [17], Iter [91/101] Loss: 0.4982\n",
      "Epoch [18], Iter [91/101] Loss: 0.4435\n",
      "Epoch [19], Iter [91/101] Loss: 0.4545\n",
      "Epoch [20], Iter [91/101] Loss: 0.4969\n",
      "Epoch [21], Iter [91/101] Loss: 0.4418\n",
      "Test MSE: 0.6709521412849426\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6664\n",
      "Epoch [2], Iter [91/101] Loss: 0.6080\n",
      "Epoch [3], Iter [91/101] Loss: 0.6277\n",
      "Epoch [4], Iter [91/101] Loss: 0.6240\n",
      "Epoch [5], Iter [91/101] Loss: 0.5701\n",
      "Epoch [6], Iter [91/101] Loss: 0.5777\n",
      "Epoch [7], Iter [91/101] Loss: 0.5754\n",
      "Epoch [8], Iter [91/101] Loss: 0.5453\n",
      "Epoch [9], Iter [91/101] Loss: 0.5438\n",
      "Epoch [10], Iter [91/101] Loss: 0.5101\n",
      "Epoch [11], Iter [91/101] Loss: 0.5293\n",
      "Epoch [12], Iter [91/101] Loss: 0.5173\n",
      "Epoch [13], Iter [91/101] Loss: 0.5207\n",
      "Epoch [14], Iter [91/101] Loss: 0.5066\n",
      "Epoch [15], Iter [91/101] Loss: 0.5065\n",
      "Epoch [16], Iter [91/101] Loss: 0.5121\n",
      "Epoch [17], Iter [91/101] Loss: 0.4637\n",
      "Epoch [18], Iter [91/101] Loss: 0.5082\n",
      "Test MSE: 0.5610610842704773\n",
      "writing predictions\n",
      "writing descriptions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7108\n",
      "Epoch [2], Iter [91/101] Loss: 0.6069\n",
      "Epoch [3], Iter [91/101] Loss: 0.5899\n",
      "Epoch [4], Iter [91/101] Loss: 0.5915\n",
      "Epoch [5], Iter [91/101] Loss: 0.5282\n",
      "Epoch [6], Iter [91/101] Loss: 0.5610\n",
      "Epoch [7], Iter [91/101] Loss: 0.5493\n",
      "Epoch [8], Iter [91/101] Loss: 0.5589\n",
      "Epoch [9], Iter [91/101] Loss: 0.5046\n",
      "Epoch [10], Iter [91/101] Loss: 0.5091\n",
      "Epoch [11], Iter [91/101] Loss: 0.5170\n",
      "Epoch [12], Iter [91/101] Loss: 0.4845\n",
      "Epoch [13], Iter [91/101] Loss: 0.5104\n",
      "Epoch [14], Iter [91/101] Loss: 0.4842\n",
      "Epoch [15], Iter [91/101] Loss: 0.4971\n",
      "Epoch [16], Iter [91/101] Loss: 0.4685\n",
      "Epoch [17], Iter [91/101] Loss: 0.4819\n",
      "Epoch [18], Iter [91/101] Loss: 0.4405\n",
      "Epoch [19], Iter [91/101] Loss: 0.4573\n",
      "Epoch [20], Iter [91/101] Loss: 0.4599\n",
      "Epoch [21], Iter [91/101] Loss: 0.4177\n",
      "Epoch [22], Iter [91/101] Loss: 0.4334\n",
      "Test MSE: 0.5439441204071045\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.5866\n",
      "Epoch [2], Iter [91/101] Loss: 0.6269\n",
      "Epoch [3], Iter [91/101] Loss: 0.5742\n",
      "Epoch [4], Iter [91/101] Loss: 0.5687\n",
      "Epoch [5], Iter [91/101] Loss: 0.5383\n",
      "Epoch [6], Iter [91/101] Loss: 0.5764\n",
      "Epoch [7], Iter [91/101] Loss: 0.5609\n",
      "Epoch [8], Iter [91/101] Loss: 0.5195\n",
      "Epoch [9], Iter [91/101] Loss: 0.5286\n",
      "Epoch [10], Iter [91/101] Loss: 0.5286\n",
      "Epoch [11], Iter [91/101] Loss: 0.4821\n",
      "Epoch [12], Iter [91/101] Loss: 0.4828\n",
      "Epoch [13], Iter [91/101] Loss: 0.4735\n",
      "Epoch [14], Iter [91/101] Loss: 0.4758\n",
      "Epoch [15], Iter [91/101] Loss: 0.4872\n",
      "Epoch [16], Iter [91/101] Loss: 0.4516\n",
      "Epoch [17], Iter [91/101] Loss: 0.4330\n",
      "Epoch [18], Iter [91/101] Loss: 0.4255\n",
      "Epoch [19], Iter [91/101] Loss: 0.4451\n",
      "Epoch [20], Iter [91/101] Loss: 0.4336\n",
      "Epoch [21], Iter [91/101] Loss: 0.4254\n",
      "Epoch [22], Iter [91/101] Loss: 0.4373\n",
      "Epoch [23], Iter [91/101] Loss: 0.4286\n",
      "Epoch [24], Iter [91/101] Loss: 0.3976\n",
      "Epoch [25], Iter [91/101] Loss: 0.4060\n",
      "Test MSE: 0.535301923751831\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6729\n",
      "Epoch [2], Iter [91/101] Loss: 0.6013\n",
      "Epoch [3], Iter [91/101] Loss: 0.6215\n",
      "Epoch [4], Iter [91/101] Loss: 0.5953\n",
      "Epoch [5], Iter [91/101] Loss: 0.5431\n",
      "Epoch [6], Iter [91/101] Loss: 0.5343\n",
      "Epoch [7], Iter [91/101] Loss: 0.5108\n",
      "Epoch [8], Iter [91/101] Loss: 0.4910\n",
      "Epoch [9], Iter [91/101] Loss: 0.4812\n",
      "Epoch [10], Iter [91/101] Loss: 0.4755\n",
      "Epoch [11], Iter [91/101] Loss: 0.4652\n",
      "Epoch [12], Iter [91/101] Loss: 0.4731\n",
      "Epoch [13], Iter [91/101] Loss: 0.4561\n",
      "Epoch [14], Iter [91/101] Loss: 0.4688\n",
      "Epoch [15], Iter [91/101] Loss: 0.4535\n",
      "Epoch [16], Iter [91/101] Loss: 0.4927\n",
      "Epoch [17], Iter [91/101] Loss: 0.4415\n",
      "Epoch [18], Iter [91/101] Loss: 0.4398\n",
      "Epoch [19], Iter [91/101] Loss: 0.4565\n",
      "Epoch [20], Iter [91/101] Loss: 0.4267\n",
      "Epoch [21], Iter [91/101] Loss: 0.4282\n",
      "Epoch [22], Iter [91/101] Loss: 0.4159\n",
      "Epoch [23], Iter [91/101] Loss: 0.4125\n",
      "Epoch [24], Iter [91/101] Loss: 0.4056\n",
      "Epoch [25], Iter [91/101] Loss: 0.4098\n",
      "Test MSE: 0.5373468995094299\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.5774\n",
      "Epoch [2], Iter [91/101] Loss: 0.6571\n",
      "Epoch [3], Iter [91/101] Loss: 0.5383\n",
      "Epoch [4], Iter [91/101] Loss: 0.5537\n",
      "Epoch [5], Iter [91/101] Loss: 0.5988\n",
      "Epoch [6], Iter [91/101] Loss: 0.5217\n",
      "Epoch [7], Iter [91/101] Loss: 0.4941\n",
      "Epoch [8], Iter [91/101] Loss: 0.4879\n",
      "Epoch [9], Iter [91/101] Loss: 0.4808\n",
      "Epoch [10], Iter [91/101] Loss: 0.5023\n",
      "Epoch [11], Iter [91/101] Loss: 0.5259\n",
      "Epoch [12], Iter [91/101] Loss: 0.4841\n",
      "Epoch [13], Iter [91/101] Loss: 0.4911\n",
      "Epoch [14], Iter [91/101] Loss: 0.4460\n",
      "Epoch [15], Iter [91/101] Loss: 0.4483\n",
      "Epoch [16], Iter [91/101] Loss: 0.4130\n",
      "Epoch [17], Iter [91/101] Loss: 0.3992\n",
      "Epoch [18], Iter [91/101] Loss: 0.4258\n",
      "Epoch [19], Iter [91/101] Loss: 0.4250\n",
      "Epoch [20], Iter [91/101] Loss: 0.4311\n",
      "Epoch [21], Iter [91/101] Loss: 0.4365\n",
      "Test MSE: 0.528474748134613\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6554\n",
      "Epoch [2], Iter [91/101] Loss: 0.5858\n",
      "Epoch [3], Iter [91/101] Loss: 0.5459\n",
      "Epoch [4], Iter [91/101] Loss: 0.5462\n",
      "Epoch [5], Iter [91/101] Loss: 0.5014\n",
      "Epoch [6], Iter [91/101] Loss: 0.5592\n",
      "Epoch [7], Iter [91/101] Loss: 0.5126\n",
      "Epoch [8], Iter [91/101] Loss: 0.4682\n",
      "Epoch [9], Iter [91/101] Loss: 0.4890\n",
      "Epoch [10], Iter [91/101] Loss: 0.5407\n",
      "Epoch [11], Iter [91/101] Loss: 0.5137\n",
      "Epoch [12], Iter [91/101] Loss: 0.4474\n",
      "Epoch [13], Iter [91/101] Loss: 0.4675\n",
      "Epoch [14], Iter [91/101] Loss: 0.4668\n",
      "Epoch [15], Iter [91/101] Loss: 0.4268\n",
      "Epoch [16], Iter [91/101] Loss: 0.4650\n",
      "Epoch [17], Iter [91/101] Loss: 0.4419\n",
      "Epoch [18], Iter [91/101] Loss: 0.4369\n",
      "Test MSE: 0.5145437717437744\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6456\n",
      "Epoch [2], Iter [91/101] Loss: 0.5602\n",
      "Epoch [3], Iter [91/101] Loss: 0.5881\n",
      "Epoch [4], Iter [91/101] Loss: 0.5334\n",
      "Epoch [5], Iter [91/101] Loss: 0.5733\n",
      "Epoch [6], Iter [91/101] Loss: 0.5581\n",
      "Epoch [7], Iter [91/101] Loss: 0.4752\n",
      "Epoch [8], Iter [91/101] Loss: 0.5706\n",
      "Epoch [9], Iter [91/101] Loss: 0.5026\n",
      "Epoch [10], Iter [91/101] Loss: 0.4825\n",
      "Epoch [11], Iter [91/101] Loss: 0.5120\n",
      "Epoch [12], Iter [91/101] Loss: 0.4771\n",
      "Epoch [13], Iter [91/101] Loss: 0.4829\n",
      "Epoch [14], Iter [91/101] Loss: 0.4867\n",
      "Epoch [15], Iter [91/101] Loss: 0.4821\n",
      "Epoch [16], Iter [91/101] Loss: 0.4544\n",
      "Epoch [17], Iter [91/101] Loss: 0.4157\n",
      "Epoch [18], Iter [91/101] Loss: 0.4433\n",
      "Epoch [19], Iter [91/101] Loss: 0.4352\n",
      "Test MSE: 0.5416766405105591\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6872\n",
      "Epoch [2], Iter [91/101] Loss: 0.6337\n",
      "Epoch [3], Iter [91/101] Loss: 0.5871\n",
      "Epoch [4], Iter [91/101] Loss: 0.6184\n",
      "Epoch [5], Iter [91/101] Loss: 0.5780\n",
      "Epoch [6], Iter [91/101] Loss: 0.6086\n",
      "Epoch [7], Iter [91/101] Loss: 0.5414\n",
      "Epoch [8], Iter [91/101] Loss: 0.5843\n",
      "Epoch [9], Iter [91/101] Loss: 0.5034\n",
      "Epoch [10], Iter [91/101] Loss: 0.5071\n",
      "Epoch [11], Iter [91/101] Loss: 0.5248\n",
      "Epoch [12], Iter [91/101] Loss: 0.4716\n",
      "Epoch [13], Iter [91/101] Loss: 0.5003\n",
      "Epoch [14], Iter [91/101] Loss: 0.5050\n",
      "Epoch [15], Iter [91/101] Loss: 0.5235\n",
      "Epoch [16], Iter [91/101] Loss: 0.4896\n",
      "Epoch [17], Iter [91/101] Loss: 0.4773\n",
      "Epoch [18], Iter [91/101] Loss: 0.4591\n",
      "Epoch [19], Iter [91/101] Loss: 0.4509\n",
      "Epoch [20], Iter [91/101] Loss: 0.4772\n",
      "Test MSE: 0.5600147247314453\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7210\n",
      "Epoch [2], Iter [91/101] Loss: 0.6031\n",
      "Epoch [3], Iter [91/101] Loss: 0.5936\n",
      "Epoch [4], Iter [91/101] Loss: 0.5728\n",
      "Epoch [5], Iter [91/101] Loss: 0.5632\n",
      "Epoch [6], Iter [91/101] Loss: 0.5530\n",
      "Epoch [7], Iter [91/101] Loss: 0.5618\n",
      "Epoch [8], Iter [91/101] Loss: 0.5216\n",
      "Epoch [9], Iter [91/101] Loss: 0.5191\n",
      "Epoch [10], Iter [91/101] Loss: 0.5343\n",
      "Epoch [11], Iter [91/101] Loss: 0.5185\n",
      "Epoch [12], Iter [91/101] Loss: 0.5253\n",
      "Epoch [13], Iter [91/101] Loss: 0.4933\n",
      "Epoch [14], Iter [91/101] Loss: 0.4853\n",
      "Epoch [15], Iter [91/101] Loss: 0.4910\n",
      "Epoch [16], Iter [91/101] Loss: 0.4587\n",
      "Epoch [17], Iter [91/101] Loss: 0.4887\n",
      "Epoch [18], Iter [91/101] Loss: 0.4613\n",
      "Epoch [19], Iter [91/101] Loss: 0.4618\n",
      "Epoch [20], Iter [91/101] Loss: 0.4342\n",
      "Epoch [21], Iter [91/101] Loss: 0.4356\n",
      "Test MSE: 0.5578874349594116\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6397\n",
      "Epoch [2], Iter [91/101] Loss: 0.5816\n",
      "Epoch [3], Iter [91/101] Loss: 0.5954\n",
      "Epoch [4], Iter [91/101] Loss: 0.6078\n",
      "Epoch [5], Iter [91/101] Loss: 0.5477\n",
      "Epoch [6], Iter [91/101] Loss: 0.5218\n",
      "Epoch [7], Iter [91/101] Loss: 0.5578\n",
      "Epoch [8], Iter [91/101] Loss: 0.5390\n",
      "Epoch [9], Iter [91/101] Loss: 0.5403\n",
      "Epoch [10], Iter [91/101] Loss: 0.5204\n",
      "Epoch [11], Iter [91/101] Loss: 0.5126\n",
      "Epoch [12], Iter [91/101] Loss: 0.5203\n",
      "Epoch [13], Iter [91/101] Loss: 0.4938\n",
      "Epoch [14], Iter [91/101] Loss: 0.5019\n",
      "Epoch [15], Iter [91/101] Loss: 0.4670\n",
      "Epoch [16], Iter [91/101] Loss: 0.4331\n",
      "Epoch [17], Iter [91/101] Loss: 0.4518\n",
      "Epoch [18], Iter [91/101] Loss: 0.4439\n",
      "Epoch [19], Iter [91/101] Loss: 0.4460\n",
      "Test MSE: 0.6114137768745422\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Iter [91/101] Loss: 0.7134\n",
      "Epoch [2], Iter [91/101] Loss: 0.6508\n",
      "Epoch [3], Iter [91/101] Loss: 0.5724\n",
      "Epoch [4], Iter [91/101] Loss: 0.5545\n",
      "Epoch [5], Iter [91/101] Loss: 0.5979\n",
      "Epoch [6], Iter [91/101] Loss: 0.5674\n",
      "Epoch [7], Iter [91/101] Loss: 0.5178\n",
      "Epoch [8], Iter [91/101] Loss: 0.5358\n",
      "Epoch [9], Iter [91/101] Loss: 0.5194\n",
      "Epoch [10], Iter [91/101] Loss: 0.4833\n",
      "Epoch [11], Iter [91/101] Loss: 0.5427\n",
      "Epoch [12], Iter [91/101] Loss: 0.4940\n",
      "Epoch [13], Iter [91/101] Loss: 0.4733\n",
      "Epoch [14], Iter [91/101] Loss: 0.4454\n",
      "Epoch [15], Iter [91/101] Loss: 0.4298\n",
      "Epoch [16], Iter [91/101] Loss: 0.4813\n",
      "Epoch [17], Iter [91/101] Loss: 0.4502\n",
      "Epoch [18], Iter [91/101] Loss: 0.4390\n",
      "Epoch [19], Iter [91/101] Loss: 0.4450\n",
      "Epoch [20], Iter [91/101] Loss: 0.4307\n",
      "Test MSE: 0.5665153861045837\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6747\n",
      "Epoch [2], Iter [91/101] Loss: 0.6029\n",
      "Epoch [3], Iter [91/101] Loss: 0.6282\n",
      "Epoch [4], Iter [91/101] Loss: 0.5857\n",
      "Epoch [5], Iter [91/101] Loss: 0.5758\n",
      "Epoch [6], Iter [91/101] Loss: 0.5531\n",
      "Epoch [7], Iter [91/101] Loss: 0.6004\n",
      "Epoch [8], Iter [91/101] Loss: 0.5873\n",
      "Epoch [9], Iter [91/101] Loss: 0.5265\n",
      "Epoch [10], Iter [91/101] Loss: 0.4941\n",
      "Epoch [11], Iter [91/101] Loss: 0.5366\n",
      "Epoch [12], Iter [91/101] Loss: 0.4962\n",
      "Epoch [13], Iter [91/101] Loss: 0.4955\n",
      "Epoch [14], Iter [91/101] Loss: 0.4655\n",
      "Epoch [15], Iter [91/101] Loss: 0.4896\n",
      "Epoch [16], Iter [91/101] Loss: 0.4796\n",
      "Epoch [17], Iter [91/101] Loss: 0.4839\n",
      "Epoch [18], Iter [91/101] Loss: 0.4896\n",
      "Test MSE: 0.6705871820449829\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6352\n",
      "Epoch [2], Iter [91/101] Loss: 0.6547\n",
      "Epoch [3], Iter [91/101] Loss: 0.6238\n",
      "Epoch [4], Iter [91/101] Loss: 0.5839\n",
      "Epoch [5], Iter [91/101] Loss: 0.5535\n",
      "Epoch [6], Iter [91/101] Loss: 0.5529\n",
      "Epoch [7], Iter [91/101] Loss: 0.5187\n",
      "Epoch [8], Iter [91/101] Loss: 0.5575\n",
      "Epoch [9], Iter [91/101] Loss: 0.5384\n",
      "Epoch [10], Iter [91/101] Loss: 0.5129\n",
      "Epoch [11], Iter [91/101] Loss: 0.5338\n",
      "Epoch [12], Iter [91/101] Loss: 0.5135\n",
      "Epoch [13], Iter [91/101] Loss: 0.4720\n",
      "Epoch [14], Iter [91/101] Loss: 0.5162\n",
      "Epoch [15], Iter [91/101] Loss: 0.4593\n",
      "Epoch [16], Iter [91/101] Loss: 0.5300\n",
      "Epoch [17], Iter [91/101] Loss: 0.4375\n",
      "Epoch [18], Iter [91/101] Loss: 0.4590\n",
      "Epoch [19], Iter [91/101] Loss: 0.4841\n",
      "Epoch [20], Iter [91/101] Loss: 0.4247\n",
      "Epoch [21], Iter [91/101] Loss: 0.4278\n",
      "Epoch [22], Iter [91/101] Loss: 0.4048\n",
      "Epoch [23], Iter [91/101] Loss: 0.4403\n",
      "Test MSE: 0.5567143559455872\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6987\n",
      "Epoch [2], Iter [91/101] Loss: 0.5911\n",
      "Epoch [3], Iter [91/101] Loss: 0.5710\n",
      "Epoch [4], Iter [91/101] Loss: 0.5700\n",
      "Epoch [5], Iter [91/101] Loss: 0.5249\n",
      "Epoch [6], Iter [91/101] Loss: 0.5857\n",
      "Epoch [7], Iter [91/101] Loss: 0.4834\n",
      "Epoch [8], Iter [91/101] Loss: 0.5338\n",
      "Epoch [9], Iter [91/101] Loss: 0.5236\n",
      "Epoch [10], Iter [91/101] Loss: 0.5045\n",
      "Epoch [11], Iter [91/101] Loss: 0.4750\n",
      "Epoch [12], Iter [91/101] Loss: 0.5054\n",
      "Epoch [13], Iter [91/101] Loss: 0.4736\n",
      "Epoch [14], Iter [91/101] Loss: 0.4890\n",
      "Epoch [15], Iter [91/101] Loss: 0.4886\n",
      "Epoch [16], Iter [91/101] Loss: 0.4713\n",
      "Epoch [17], Iter [91/101] Loss: 0.4665\n",
      "Epoch [18], Iter [91/101] Loss: 0.4765\n",
      "Epoch [19], Iter [91/101] Loss: 0.4804\n",
      "Epoch [20], Iter [91/101] Loss: 0.4400\n",
      "Epoch [21], Iter [91/101] Loss: 0.4321\n",
      "Epoch [22], Iter [91/101] Loss: 0.4190\n",
      "Epoch [23], Iter [91/101] Loss: 0.4619\n",
      "Epoch [24], Iter [91/101] Loss: 0.4059\n",
      "Test MSE: 0.5433441996574402\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 5\n",
    "for j in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = j\n",
    "    for i in range(12):\n",
    "        description[\"MONTHS_USED\"] = np.sort([i]).tolist()\n",
    "        unet = train_unet(description, model_training_description, output_folder)\n",
    "        predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f2e6a",
   "metadata": {},
   "source": [
    "# Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323b9843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 matching runs found\n",
      "5 matching runs found\n"
     ]
    }
   ],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "c_all = {\n",
    "    \"DATASET_DESCRIPTION\": {'MONTHS_USED': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "                            'MONTHS_USED_IN_PREDICTION': [0]},\n",
    "    \"MODEL_TRAINING_DESCRIPTION\": {}\n",
    "}\n",
    "\n",
    "descriptions, predictions, gt, masks = load_data_for_comparison(output_folder, c_all, do_split=True)\n",
    "\n",
    "c_all_use_p = {\n",
    "    \"DATASET_DESCRIPTION\": {'MONTHS_USED': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "                            'MONTHS_USED_IN_PREDICTION': [-1, 0]},\n",
    "    \"MODEL_TRAINING_DESCRIPTION\": {}\n",
    "}\n",
    "\n",
    "descriptions_p, predictions_p, gt_p, masks_p = load_data_for_comparison(output_folder, c_all_use_p, do_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08c76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n",
      "5 matching runs found\n"
     ]
    }
   ],
   "source": [
    "r2_single = []\n",
    "\n",
    "for i in range(12):\n",
    "    r2_single.append([])\n",
    "    c_single = {\n",
    "        \"DATASET_DESCRIPTION\": {'MONTHS_USED': [i],\n",
    "                                'MONTHS_USED_IN_PREDICTION': [0]},\n",
    "        \"MODEL_TRAINING_DESCRIPTION\": {}\n",
    "    }\n",
    "    descriptions_s, predictions_s, gt_s, masks_s = load_data_for_comparison(output_folder, c_single, do_split=True)\n",
    "    for j in range(3):\n",
    "        r2 = get_r2(predictions_s[j][i], gt_s[j][i], masks_s[j][i])\n",
    "        r2_single[-1].append(get_weighted_average(r2, descriptions[j][\"DATASET_DESCRIPTION\"])[0])\n",
    "        \n",
    "r2_single = np.mean(np.array(r2_single),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be339ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_all = []\n",
    "r2_all_use_p = []\n",
    "\n",
    "for j in range(3):\n",
    "    r2_all.append([])\n",
    "    r2_all_use_p.append([])\n",
    "    for i in range(12):\n",
    "        r2 = get_r2(predictions[j][i], gt[j][i], masks[j][i])\n",
    "        r2_p = get_r2(predictions_p[j][i], gt_p[j][i], masks_p[j][i])\n",
    "        r2_all[-1].append(get_weighted_average(r2, descriptions[j][\"DATASET_DESCRIPTION\"])[0])\n",
    "        r2_all_use_p[-1].append(get_weighted_average(r2_p, descriptions_p[j][\"DATASET_DESCRIPTION\"])[0])\n",
    "        \n",
    "r2_all = np.mean(np.array(r2_all),axis=0)\n",
    "r2_all_use_p = np.mean(np.array(r2_all_use_p),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1264aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18fa36f5fd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(r2_all, label=\"UNet, all\")\n",
    "plt.plot(r2_all_use_p, label=\"UNet, all, use previous month\")\n",
    "plt.plot(r2_single, label=\"UNet, single\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('GrouPyTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "90b409070de10f2c3c5a8c504ecede612c695480a2027d21529fbe1a33a534d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
