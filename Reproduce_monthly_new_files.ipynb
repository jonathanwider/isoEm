{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734354f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from predict import *\n",
    "from evaluate import *\n",
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed05f4",
   "metadata": {},
   "source": [
    "# Test monthly stuff from thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771afb0",
   "metadata": {},
   "source": [
    "## Create datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce1deb",
   "metadata": {},
   "source": [
    "All months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f5af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED\"] = np.sort([0,1,2,3,4,5,6,7,8,9,10,11]).tolist()\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698616dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading variables\n",
      "writing pickle\n"
     ]
    }
   ],
   "source": [
    "create_monthly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a472a41",
   "metadata": {},
   "source": [
    "All months, use lagged months in predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED\"] = np.sort([0,1,2,3,4,5,6,7,8,9,10,11]).tolist()\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0,-1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb355d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_monthly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde7093",
   "metadata": {},
   "source": [
    "Individual dataset for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0]).tolist()\n",
    "\n",
    "for i in range(12):\n",
    "    description[\"MONTHS_USED\"] = np.sort([i]).tolist()\n",
    "    create_monthly_dataset(description, base_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161e75b",
   "metadata": {},
   "source": [
    "# Run experiments on these datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575dddea",
   "metadata": {},
   "source": [
    "All months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d06c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\", \n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED\"] = np.sort([0,1,2,3,4,5,6,7,8,9,10,11]).tolist()\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0]).tolist()\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f34180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [1201/1211] Loss: 0.3242\n",
      "Epoch [2], Iter [1201/1211] Loss: 0.3027\n",
      "Epoch [3], Iter [1201/1211] Loss: 0.3022\n",
      "Epoch [4], Iter [1201/1211] Loss: 0.2847\n",
      "Epoch [5], Iter [1201/1211] Loss: 0.2718\n",
      "Epoch [6], Iter [1201/1211] Loss: 0.3039\n",
      "Epoch [7], Iter [1201/1211] Loss: 0.2464\n",
      "Epoch [8], Iter [1201/1211] Loss: 0.2620\n",
      "Epoch [9], Iter [1201/1211] Loss: 0.2634\n",
      "Epoch [10], Iter [1201/1211] Loss: 0.2760\n",
      "Epoch [11], Iter [1201/1211] Loss: 0.2538\n",
      "Epoch [12], Iter [1201/1211] Loss: 0.2457\n",
      "Epoch [13], Iter [1201/1211] Loss: 0.2620\n",
      "Epoch [14], Iter [1201/1211] Loss: 0.2446\n",
      "Epoch [15], Iter [1201/1211] Loss: 0.2445\n",
      "Epoch [16], Iter [1201/1211] Loss: 0.2255\n",
      "Epoch [17], Iter [1201/1211] Loss: 0.2650\n",
      "Epoch [18], Iter [1201/1211] Loss: 0.2469\n",
      "Epoch [19], Iter [1201/1211] Loss: 0.2530\n",
      "Epoch [20], Iter [1201/1211] Loss: 0.2693\n",
      "Epoch [21], Iter [1201/1211] Loss: 0.2357\n",
      "Epoch [22], Iter [1201/1211] Loss: 0.2458\n",
      "Epoch [23], Iter [1201/1211] Loss: 0.2771\n",
      "Epoch [24], Iter [1201/1211] Loss: 0.2413\n",
      "Epoch [25], Iter [1201/1211] Loss: 0.2414\n",
      "Epoch [26], Iter [1201/1211] Loss: 0.2285\n",
      "Epoch [27], Iter [1201/1211] Loss: 0.2464\n",
      "Test MSE: 0.27035588026046753\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1211] Loss: 0.3354\n",
      "Epoch [2], Iter [1201/1211] Loss: 0.2887\n",
      "Epoch [3], Iter [1201/1211] Loss: 0.2783\n",
      "Epoch [4], Iter [1201/1211] Loss: 0.2659\n",
      "Epoch [5], Iter [1201/1211] Loss: 0.3008\n",
      "Epoch [6], Iter [1201/1211] Loss: 0.2880\n",
      "Epoch [7], Iter [1201/1211] Loss: 0.2488\n",
      "Epoch [8], Iter [1201/1211] Loss: 0.2719\n",
      "Epoch [9], Iter [1201/1211] Loss: 0.2448\n",
      "Epoch [10], Iter [1201/1211] Loss: 0.2627\n",
      "Epoch [11], Iter [1201/1211] Loss: 0.2694\n",
      "Epoch [12], Iter [1201/1211] Loss: 0.2702\n",
      "Epoch [13], Iter [1201/1211] Loss: 0.2433\n",
      "Epoch [14], Iter [1201/1211] Loss: 0.2537\n",
      "Epoch [15], Iter [1201/1211] Loss: 0.2437\n",
      "Epoch [16], Iter [1201/1211] Loss: 0.2520\n",
      "Epoch [17], Iter [1201/1211] Loss: 0.2428\n",
      "Epoch [18], Iter [1201/1211] Loss: 0.2761\n",
      "Epoch [19], Iter [1201/1211] Loss: 0.2297\n",
      "Epoch [20], Iter [1201/1211] Loss: 0.2296\n",
      "Epoch [21], Iter [1201/1211] Loss: 0.2369\n",
      "Epoch [22], Iter [1201/1211] Loss: 0.2338\n",
      "Epoch [23], Iter [1201/1211] Loss: 0.2271\n",
      "Epoch [24], Iter [1201/1211] Loss: 0.2437\n",
      "Epoch [25], Iter [1201/1211] Loss: 0.2519\n",
      "Epoch [26], Iter [1201/1211] Loss: 0.2145\n",
      "Epoch [27], Iter [1201/1211] Loss: 0.2309\n",
      "Epoch [28], Iter [1201/1211] Loss: 0.2358\n",
      "Test MSE: 0.2698502242565155\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1211] Loss: 0.3579\n",
      "Epoch [2], Iter [1201/1211] Loss: 0.3116\n",
      "Epoch [3], Iter [1201/1211] Loss: 0.3051\n",
      "Epoch [4], Iter [1201/1211] Loss: 0.2538\n",
      "Epoch [5], Iter [1201/1211] Loss: 0.3028\n",
      "Epoch [6], Iter [1201/1211] Loss: 0.2627\n",
      "Epoch [7], Iter [1201/1211] Loss: 0.2769\n",
      "Epoch [8], Iter [1201/1211] Loss: 0.2772\n",
      "Epoch [9], Iter [1201/1211] Loss: 0.2586\n",
      "Epoch [10], Iter [1201/1211] Loss: 0.2613\n",
      "Epoch [11], Iter [1201/1211] Loss: 0.2574\n",
      "Epoch [12], Iter [1201/1211] Loss: 0.2700\n",
      "Epoch [13], Iter [1201/1211] Loss: 0.2690\n",
      "Epoch [14], Iter [1201/1211] Loss: 0.2491\n",
      "Epoch [15], Iter [1201/1211] Loss: 0.2506\n",
      "Epoch [16], Iter [1201/1211] Loss: 0.2975\n",
      "Epoch [17], Iter [1201/1211] Loss: 0.2508\n",
      "Epoch [18], Iter [1201/1211] Loss: 0.2202\n",
      "Epoch [19], Iter [1201/1211] Loss: 0.2225\n",
      "Epoch [20], Iter [1201/1211] Loss: 0.2509\n",
      "Epoch [21], Iter [1201/1211] Loss: 0.2570\n",
      "Epoch [22], Iter [1201/1211] Loss: 0.2342\n",
      "Test MSE: 0.27136683464050293\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd8528",
   "metadata": {},
   "source": [
    "All months, use previous timestep in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528da814",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED\"] = np.sort([0,1,2,3,4,5,6,7,8,9,10,11]).tolist()\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0,-1]).tolist()\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\",\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))*2\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b418b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [1201/1210] Loss: 0.3701\n",
      "Epoch [2], Iter [1201/1210] Loss: 0.3048\n",
      "Epoch [3], Iter [1201/1210] Loss: 0.2898\n",
      "Epoch [4], Iter [1201/1210] Loss: 0.3137\n",
      "Epoch [5], Iter [1201/1210] Loss: 0.2678\n",
      "Epoch [6], Iter [1201/1210] Loss: 0.2941\n",
      "Epoch [7], Iter [1201/1210] Loss: 0.2587\n",
      "Epoch [8], Iter [1201/1210] Loss: 0.2814\n",
      "Epoch [9], Iter [1201/1210] Loss: 0.2382\n",
      "Epoch [10], Iter [1201/1210] Loss: 0.2431\n",
      "Epoch [11], Iter [1201/1210] Loss: 0.2614\n",
      "Epoch [12], Iter [1201/1210] Loss: 0.2487\n",
      "Epoch [13], Iter [1201/1210] Loss: 0.2420\n",
      "Epoch [14], Iter [1201/1210] Loss: 0.2562\n",
      "Epoch [15], Iter [1201/1210] Loss: 0.2419\n",
      "Epoch [16], Iter [1201/1210] Loss: 0.2492\n",
      "Epoch [17], Iter [1201/1210] Loss: 0.2652\n",
      "Epoch [18], Iter [1201/1210] Loss: 0.2396\n",
      "Epoch [19], Iter [1201/1210] Loss: 0.2464\n",
      "Epoch [20], Iter [1201/1210] Loss: 0.2658\n",
      "Epoch [21], Iter [1201/1210] Loss: 0.2617\n",
      "Epoch [22], Iter [1201/1210] Loss: 0.2403\n",
      "Epoch [23], Iter [1201/1210] Loss: 0.2486\n",
      "Epoch [24], Iter [1201/1210] Loss: 0.2208\n",
      "Epoch [25], Iter [1201/1210] Loss: 0.2291\n",
      "Epoch [26], Iter [1201/1210] Loss: 0.2381\n",
      "Epoch [27], Iter [1201/1210] Loss: 0.2380\n",
      "Test MSE: 0.2675383388996124\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1210] Loss: 0.3502\n",
      "Epoch [2], Iter [1201/1210] Loss: 0.2930\n",
      "Epoch [3], Iter [1201/1210] Loss: 0.2564\n",
      "Epoch [4], Iter [1201/1210] Loss: 0.2705\n",
      "Epoch [5], Iter [1201/1210] Loss: 0.2972\n",
      "Epoch [6], Iter [1201/1210] Loss: 0.2615\n",
      "Epoch [7], Iter [1201/1210] Loss: 0.2572\n",
      "Epoch [8], Iter [1201/1210] Loss: 0.2934\n",
      "Epoch [9], Iter [1201/1210] Loss: 0.2471\n",
      "Epoch [10], Iter [1201/1210] Loss: 0.2616\n",
      "Epoch [11], Iter [1201/1210] Loss: 0.2427\n",
      "Epoch [12], Iter [1201/1210] Loss: 0.2817\n",
      "Epoch [13], Iter [1201/1210] Loss: 0.2657\n",
      "Epoch [14], Iter [1201/1210] Loss: 0.2599\n",
      "Epoch [15], Iter [1201/1210] Loss: 0.2347\n",
      "Epoch [16], Iter [1201/1210] Loss: 0.2340\n",
      "Epoch [17], Iter [1201/1210] Loss: 0.2647\n",
      "Epoch [18], Iter [1201/1210] Loss: 0.2427\n",
      "Epoch [19], Iter [1201/1210] Loss: 0.2506\n",
      "Epoch [20], Iter [1201/1210] Loss: 0.2356\n",
      "Epoch [21], Iter [1201/1210] Loss: 0.2452\n",
      "Epoch [22], Iter [1201/1210] Loss: 0.2310\n",
      "Epoch [23], Iter [1201/1210] Loss: 0.2379\n",
      "Epoch [24], Iter [1201/1210] Loss: 0.2285\n",
      "Epoch [25], Iter [1201/1210] Loss: 0.2554\n",
      "Epoch [26], Iter [1201/1210] Loss: 0.2498\n",
      "Epoch [27], Iter [1201/1210] Loss: 0.2540\n",
      "Epoch [28], Iter [1201/1210] Loss: 0.2373\n",
      "Test MSE: 0.26859986782073975\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [1201/1210] Loss: 0.3454\n",
      "Epoch [2], Iter [1201/1210] Loss: 0.3254\n",
      "Epoch [3], Iter [1201/1210] Loss: 0.2911\n",
      "Epoch [4], Iter [1201/1210] Loss: 0.2977\n",
      "Epoch [5], Iter [1201/1210] Loss: 0.2988\n",
      "Epoch [6], Iter [1201/1210] Loss: 0.2837\n",
      "Epoch [7], Iter [1201/1210] Loss: 0.2543\n",
      "Epoch [8], Iter [1201/1210] Loss: 0.2248\n",
      "Epoch [9], Iter [1201/1210] Loss: 0.2368\n",
      "Epoch [10], Iter [1201/1210] Loss: 0.2695\n",
      "Epoch [11], Iter [1201/1210] Loss: 0.2551\n",
      "Epoch [12], Iter [1201/1210] Loss: 0.2903\n",
      "Epoch [13], Iter [1201/1210] Loss: 0.2747\n",
      "Epoch [14], Iter [1201/1210] Loss: 0.2893\n",
      "Epoch [15], Iter [1201/1210] Loss: 0.2456\n",
      "Epoch [16], Iter [1201/1210] Loss: 0.2441\n",
      "Epoch [17], Iter [1201/1210] Loss: 0.2522\n",
      "Epoch [18], Iter [1201/1210] Loss: 0.2415\n",
      "Epoch [19], Iter [1201/1210] Loss: 0.2417\n",
      "Epoch [20], Iter [1201/1210] Loss: 0.2514\n",
      "Epoch [21], Iter [1201/1210] Loss: 0.2410\n",
      "Epoch [22], Iter [1201/1210] Loss: 0.2258\n",
      "Epoch [23], Iter [1201/1210] Loss: 0.2310\n",
      "Epoch [24], Iter [1201/1210] Loss: 0.2423\n",
      "Epoch [25], Iter [1201/1210] Loss: 0.2467\n",
      "Epoch [26], Iter [1201/1210] Loss: 0.2376\n",
      "Epoch [27], Iter [1201/1210] Loss: 0.2304\n",
      "Epoch [28], Iter [1201/1210] Loss: 0.2592\n",
      "Test MSE: 0.26764973998069763\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "for i in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = i\n",
    "    unet = train_unet(description, model_training_description, output_folder)\n",
    "    predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d605fc5",
   "metadata": {},
   "source": [
    "Individual models for different cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fab9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Datasets\"\n",
    "output_folder = \"Output/Reproduce_monthly\"\n",
    "\n",
    "description = {}\n",
    "\n",
    "description[\"DATASETS_USED\"] = [\"isotopes\", \n",
    "                                \"tsurf\",\n",
    "                                \"prec\"]\n",
    "\n",
    "description[\"PREDICTOR_VARIABLES\"] = {\"tsurf\": [\"tsurf\"],\n",
    "                                      \"prec\": [\"prec\"]}\n",
    "\n",
    "description[\"TARGET_VARIABLES\"] = {\"isotopes\": [\"d18O\"]}\n",
    "\n",
    "description[\"DATASETS_NO_GAPS\"] = [\"isotopes\", \n",
    "                                   \"tsurf\", \n",
    "                                   \"prec\", \n",
    "                                   \"slp\"]\n",
    "\n",
    "description[\"CLIMATE_MODEL\"] = \"iHadCM3\"\n",
    "description[\"GRID_TYPE\"] = \"Flat\"\n",
    "\n",
    "description[\"START_YEAR\"] = 850\n",
    "description[\"END_YEAR\"] = 1850\n",
    "description[\"LATITUDES_SLICE\"] = [1,-1]\n",
    "\n",
    "description[\"SPLIT_YEAR\"] = 1750\n",
    "description[\"DO_SHUFFLE\"] = False\n",
    "description[\"PRECIP_WEIGHTING\"] = False\n",
    "\n",
    "\n",
    "description[\"TIMESCALE\"] = \"MONTHLY\"\n",
    "description[\"MONTHS_USED_IN_PREDICTION\"] = np.sort([0]).tolist()\n",
    "\n",
    "### MODEL_TRAINING ###############################################\n",
    "\n",
    "model_training_description = {}\n",
    "model_training_description[\"S_MODE_PREDICTORS\"] = [\"Pixelwise\",\"Pixelwise\"] # how to standardize the given variables\n",
    "model_training_description[\"S_MODE_TARGETS\"] = [\"Pixelwise\"]\n",
    "\n",
    "model_training_description[\"DATASET_FOLDER\"] = output_folder\n",
    "\n",
    "model_training_description[\"MODEL_TYPE\"] = \"UNet_Flat\"\n",
    "model_training_description[\"CREATE_VALIDATIONSET\"] = True\n",
    "model_training_description[\"SHUFFLE_VALIDATIONSET\"] = True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"NUM_EPOCHS\"] = \"early_stopping\"  # 20\n",
    "model_training_description[\"PATIENCE\"] = 5\n",
    "model_training_description[\"BATCH_SIZE\"] = 8\n",
    "model_training_description[\"LEARNING_RATE\"] = 5e-3  # 0.002637 # 5e-3  # use either this or default ADAM learning rate\n",
    "\n",
    "# model parameters\n",
    "model_training_description[\"DEPTH\"] = 3\n",
    "model_training_description[\"IN_CHANNELS\"] = len(util.flatten(description[\"PREDICTOR_VARIABLES\"].values()))\n",
    "model_training_description[\"CHANNELS_FIRST_CONV\"] = 32\n",
    "model_training_description[\"OUT_CHANNELS\"] = len(util.flatten(description[\"TARGET_VARIABLES\"].values()))\n",
    "model_training_description[\"FMAPS\"] = (32,32,64,64)\n",
    "\n",
    "\n",
    "model_training_description[\"ACTIVATION\"] = torch.nn.ReLU\n",
    "model_training_description[\"NORMALIZATION\"] = torch.nn.BatchNorm2d  # IcoBatchNorm2d \n",
    "\n",
    "\n",
    "model_training_description[\"OPTIMIZER\"] = \"Adam\"\n",
    "\n",
    "model_training_description[\"DEVICE\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_training_description[\"USE_CYLINDRICAL_PADDING\"] = True\n",
    "model_training_description[\"USE_COORD_CONV\"] = True\n",
    "model_training_description[\"LOSS\"] = \"Masked_AreaWeightedMSELoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644a7011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6997\n",
      "Epoch [2], Iter [91/101] Loss: 0.5886\n",
      "Epoch [3], Iter [91/101] Loss: 0.6248\n",
      "Epoch [4], Iter [91/101] Loss: 0.5791\n",
      "Epoch [5], Iter [91/101] Loss: 0.5282\n",
      "Epoch [6], Iter [91/101] Loss: 0.5808\n",
      "Epoch [7], Iter [91/101] Loss: 0.5097\n",
      "Epoch [8], Iter [91/101] Loss: 0.4942\n",
      "Epoch [9], Iter [91/101] Loss: 0.5140\n",
      "Epoch [10], Iter [91/101] Loss: 0.4823\n",
      "Epoch [11], Iter [91/101] Loss: 0.4941\n",
      "Epoch [12], Iter [91/101] Loss: 0.4951\n",
      "Epoch [13], Iter [91/101] Loss: 0.4570\n",
      "Epoch [14], Iter [91/101] Loss: 0.4812\n",
      "Epoch [15], Iter [91/101] Loss: 0.4548\n",
      "Epoch [16], Iter [91/101] Loss: 0.4274\n",
      "Epoch [17], Iter [91/101] Loss: 0.4572\n",
      "Epoch [18], Iter [91/101] Loss: 0.4336\n",
      "Epoch [19], Iter [91/101] Loss: 0.4649\n",
      "Epoch [20], Iter [91/101] Loss: 0.4346\n",
      "Epoch [21], Iter [91/101] Loss: 0.4086\n",
      "Epoch [22], Iter [91/101] Loss: 0.4095\n",
      "Epoch [23], Iter [91/101] Loss: 0.3947\n",
      "Epoch [24], Iter [91/101] Loss: 0.3943\n",
      "Test MSE: 0.5084053874015808\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6735\n",
      "Epoch [2], Iter [91/101] Loss: 0.5583\n",
      "Epoch [3], Iter [91/101] Loss: 0.5787\n",
      "Epoch [4], Iter [91/101] Loss: 0.5939\n",
      "Epoch [5], Iter [91/101] Loss: 0.5737\n",
      "Epoch [6], Iter [91/101] Loss: 0.5173\n",
      "Epoch [7], Iter [91/101] Loss: 0.5160\n",
      "Epoch [8], Iter [91/101] Loss: 0.4804\n",
      "Epoch [9], Iter [91/101] Loss: 0.4781\n",
      "Epoch [10], Iter [91/101] Loss: 0.4776\n",
      "Epoch [11], Iter [91/101] Loss: 0.4922\n",
      "Epoch [12], Iter [91/101] Loss: 0.4448\n",
      "Epoch [13], Iter [91/101] Loss: 0.4373\n",
      "Epoch [14], Iter [91/101] Loss: 0.4771\n",
      "Epoch [15], Iter [91/101] Loss: 0.4455\n",
      "Epoch [16], Iter [91/101] Loss: 0.4007\n",
      "Epoch [17], Iter [91/101] Loss: 0.4063\n",
      "Epoch [18], Iter [91/101] Loss: 0.4548\n",
      "Epoch [19], Iter [91/101] Loss: 0.4281\n",
      "Epoch [20], Iter [91/101] Loss: 0.3923\n",
      "Epoch [21], Iter [91/101] Loss: 0.4046\n",
      "Test MSE: 0.5145221948623657\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6320\n",
      "Epoch [2], Iter [91/100] Loss: 0.6071\n",
      "Epoch [3], Iter [91/100] Loss: 0.5359\n",
      "Epoch [4], Iter [91/100] Loss: 0.5237\n",
      "Epoch [5], Iter [91/100] Loss: 0.5185\n",
      "Epoch [6], Iter [91/100] Loss: 0.5132\n",
      "Epoch [7], Iter [91/100] Loss: 0.4742\n",
      "Epoch [8], Iter [91/100] Loss: 0.4900\n",
      "Epoch [9], Iter [91/100] Loss: 0.4924\n",
      "Epoch [10], Iter [91/100] Loss: 0.4572\n",
      "Epoch [11], Iter [91/100] Loss: 0.4898\n",
      "Epoch [12], Iter [91/100] Loss: 0.4431\n",
      "Epoch [13], Iter [91/100] Loss: 0.4321\n",
      "Epoch [14], Iter [91/100] Loss: 0.4625\n",
      "Epoch [15], Iter [91/100] Loss: 0.4426\n",
      "Epoch [16], Iter [91/100] Loss: 0.4203\n",
      "Test MSE: 0.5016807317733765\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6800\n",
      "Epoch [2], Iter [91/101] Loss: 0.5703\n",
      "Epoch [3], Iter [91/101] Loss: 0.5734\n",
      "Epoch [4], Iter [91/101] Loss: 0.5520\n",
      "Epoch [5], Iter [91/101] Loss: 0.5118\n",
      "Epoch [6], Iter [91/101] Loss: 0.5141\n",
      "Epoch [7], Iter [91/101] Loss: 0.5126\n",
      "Epoch [8], Iter [91/101] Loss: 0.4835\n",
      "Epoch [9], Iter [91/101] Loss: 0.4747\n",
      "Epoch [10], Iter [91/101] Loss: 0.4989\n",
      "Epoch [11], Iter [91/101] Loss: 0.4947\n",
      "Epoch [12], Iter [91/101] Loss: 0.4832\n",
      "Epoch [13], Iter [91/101] Loss: 0.4710\n",
      "Epoch [14], Iter [91/101] Loss: 0.4548\n",
      "Epoch [15], Iter [91/101] Loss: 0.4382\n",
      "Epoch [16], Iter [91/101] Loss: 0.4128\n",
      "Epoch [17], Iter [91/101] Loss: 0.4271\n",
      "Epoch [18], Iter [91/101] Loss: 0.4333\n",
      "Epoch [19], Iter [91/101] Loss: 0.4146\n",
      "Epoch [20], Iter [91/101] Loss: 0.4121\n",
      "Epoch [21], Iter [91/101] Loss: 0.3999\n",
      "Epoch [22], Iter [91/101] Loss: 0.3902\n",
      "Test MSE: 0.5280759930610657\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6482\n",
      "Epoch [2], Iter [91/101] Loss: 0.6583\n",
      "Epoch [3], Iter [91/101] Loss: 0.5593\n",
      "Epoch [4], Iter [91/101] Loss: 0.5779\n",
      "Epoch [5], Iter [91/101] Loss: 0.5502\n",
      "Epoch [6], Iter [91/101] Loss: 0.5824\n",
      "Epoch [7], Iter [91/101] Loss: 0.5760\n",
      "Epoch [8], Iter [91/101] Loss: 0.5286\n",
      "Epoch [9], Iter [91/101] Loss: 0.5238\n",
      "Epoch [10], Iter [91/101] Loss: 0.5292\n",
      "Epoch [11], Iter [91/101] Loss: 0.4693\n",
      "Epoch [12], Iter [91/101] Loss: 0.4846\n",
      "Epoch [13], Iter [91/101] Loss: 0.4930\n",
      "Epoch [14], Iter [91/101] Loss: 0.4672\n",
      "Epoch [15], Iter [91/101] Loss: 0.4695\n",
      "Epoch [16], Iter [91/101] Loss: 0.4655\n",
      "Epoch [17], Iter [91/101] Loss: 0.4248\n",
      "Epoch [18], Iter [91/101] Loss: 0.4297\n",
      "Test MSE: 0.551160991191864\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6389\n",
      "Epoch [2], Iter [91/101] Loss: 0.6290\n",
      "Epoch [3], Iter [91/101] Loss: 0.6011\n",
      "Epoch [4], Iter [91/101] Loss: 0.5996\n",
      "Epoch [5], Iter [91/101] Loss: 0.5473\n",
      "Epoch [6], Iter [91/101] Loss: 0.5621\n",
      "Epoch [7], Iter [91/101] Loss: 0.5652\n",
      "Epoch [8], Iter [91/101] Loss: 0.5370\n",
      "Epoch [9], Iter [91/101] Loss: 0.4997\n",
      "Epoch [10], Iter [91/101] Loss: 0.5501\n",
      "Epoch [11], Iter [91/101] Loss: 0.5012\n",
      "Epoch [12], Iter [91/101] Loss: 0.5185\n",
      "Epoch [13], Iter [91/101] Loss: 0.4537\n",
      "Epoch [14], Iter [91/101] Loss: 0.4691\n",
      "Epoch [15], Iter [91/101] Loss: 0.4587\n",
      "Epoch [16], Iter [91/101] Loss: 0.4539\n",
      "Epoch [17], Iter [91/101] Loss: 0.4433\n",
      "Epoch [18], Iter [91/101] Loss: 0.4295\n",
      "Epoch [19], Iter [91/101] Loss: 0.4507\n",
      "Epoch [20], Iter [91/101] Loss: 0.4190\n",
      "Epoch [21], Iter [91/101] Loss: 0.4273\n",
      "Epoch [22], Iter [91/101] Loss: 0.4090\n",
      "Epoch [23], Iter [91/101] Loss: 0.4430\n",
      "Epoch [24], Iter [91/101] Loss: 0.3993\n",
      "Test MSE: 0.5537398457527161\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6498\n",
      "Epoch [2], Iter [91/100] Loss: 0.6509\n",
      "Epoch [3], Iter [91/100] Loss: 0.5517\n",
      "Epoch [4], Iter [91/100] Loss: 0.5972\n",
      "Epoch [5], Iter [91/100] Loss: 0.5576\n",
      "Epoch [6], Iter [91/100] Loss: 0.5571\n",
      "Epoch [7], Iter [91/100] Loss: 0.5563\n",
      "Epoch [8], Iter [91/100] Loss: 0.5375\n",
      "Epoch [9], Iter [91/100] Loss: 0.5260\n",
      "Epoch [10], Iter [91/100] Loss: 0.4927\n",
      "Epoch [11], Iter [91/100] Loss: 0.4996\n",
      "Epoch [12], Iter [91/100] Loss: 0.5265\n",
      "Epoch [13], Iter [91/100] Loss: 0.4444\n",
      "Epoch [14], Iter [91/100] Loss: 0.4866\n",
      "Epoch [15], Iter [91/100] Loss: 0.4619\n",
      "Epoch [16], Iter [91/100] Loss: 0.4463\n",
      "Epoch [17], Iter [91/100] Loss: 0.4589\n",
      "Test MSE: 0.6180946230888367\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6668\n",
      "Epoch [2], Iter [91/100] Loss: 0.5846\n",
      "Epoch [3], Iter [91/100] Loss: 0.6060\n",
      "Epoch [4], Iter [91/100] Loss: 0.5504\n",
      "Epoch [5], Iter [91/100] Loss: 0.5766\n",
      "Epoch [6], Iter [91/100] Loss: 0.5606\n",
      "Epoch [7], Iter [91/100] Loss: 0.5654\n",
      "Epoch [8], Iter [91/100] Loss: 0.5173\n",
      "Epoch [9], Iter [91/100] Loss: 0.4945\n",
      "Epoch [10], Iter [91/100] Loss: 0.5111\n",
      "Epoch [11], Iter [91/100] Loss: 0.4707\n",
      "Epoch [12], Iter [91/100] Loss: 0.4691\n",
      "Epoch [13], Iter [91/100] Loss: 0.4533\n",
      "Epoch [14], Iter [91/100] Loss: 0.4540\n",
      "Epoch [15], Iter [91/100] Loss: 0.4840\n",
      "Epoch [16], Iter [91/100] Loss: 0.4698\n",
      "Epoch [17], Iter [91/100] Loss: 0.4431\n",
      "Epoch [18], Iter [91/100] Loss: 0.5192\n",
      "Epoch [19], Iter [91/100] Loss: 0.4650\n",
      "Epoch [20], Iter [91/100] Loss: 0.4263\n",
      "Epoch [21], Iter [91/100] Loss: 0.4261\n",
      "Epoch [22], Iter [91/100] Loss: 0.3998\n",
      "Epoch [23], Iter [91/100] Loss: 0.3989\n",
      "Epoch [24], Iter [91/100] Loss: 0.3779\n",
      "Epoch [25], Iter [91/100] Loss: 0.4178\n",
      "Epoch [26], Iter [91/100] Loss: 0.3877\n",
      "Test MSE: 0.5516474843025208\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6664\n",
      "Epoch [2], Iter [91/101] Loss: 0.6651\n",
      "Epoch [3], Iter [91/101] Loss: 0.5548\n",
      "Epoch [4], Iter [91/101] Loss: 0.5945\n",
      "Epoch [5], Iter [91/101] Loss: 0.5733\n",
      "Epoch [6], Iter [91/101] Loss: 0.5302\n",
      "Epoch [7], Iter [91/101] Loss: 0.5291\n",
      "Epoch [8], Iter [91/101] Loss: 0.5128\n",
      "Epoch [9], Iter [91/101] Loss: 0.5247\n",
      "Epoch [10], Iter [91/101] Loss: 0.4975\n",
      "Epoch [11], Iter [91/101] Loss: 0.5096\n",
      "Epoch [12], Iter [91/101] Loss: 0.4842\n",
      "Epoch [13], Iter [91/101] Loss: 0.5222\n",
      "Epoch [14], Iter [91/101] Loss: 0.4845\n",
      "Epoch [15], Iter [91/101] Loss: 0.4396\n",
      "Epoch [16], Iter [91/101] Loss: 0.5013\n",
      "Epoch [17], Iter [91/101] Loss: 0.4586\n",
      "Epoch [18], Iter [91/101] Loss: 0.4824\n",
      "Test MSE: 0.6282163858413696\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6435\n",
      "Epoch [2], Iter [91/100] Loss: 0.5833\n",
      "Epoch [3], Iter [91/100] Loss: 0.5966\n",
      "Epoch [4], Iter [91/100] Loss: 0.5829\n",
      "Epoch [5], Iter [91/100] Loss: 0.5524\n",
      "Epoch [6], Iter [91/100] Loss: 0.5403\n",
      "Epoch [7], Iter [91/100] Loss: 0.5248\n",
      "Epoch [8], Iter [91/100] Loss: 0.5486\n",
      "Epoch [9], Iter [91/100] Loss: 0.5378\n",
      "Epoch [10], Iter [91/100] Loss: 0.5212\n",
      "Epoch [11], Iter [91/100] Loss: 0.5127\n",
      "Epoch [12], Iter [91/100] Loss: 0.5154\n",
      "Epoch [13], Iter [91/100] Loss: 0.5022\n",
      "Epoch [14], Iter [91/100] Loss: 0.5502\n",
      "Epoch [15], Iter [91/100] Loss: 0.4663\n",
      "Epoch [16], Iter [91/100] Loss: 0.4574\n",
      "Epoch [17], Iter [91/100] Loss: 0.4631\n",
      "Epoch [18], Iter [91/100] Loss: 0.4433\n",
      "Epoch [19], Iter [91/100] Loss: 0.4415\n",
      "Epoch [20], Iter [91/100] Loss: 0.4330\n",
      "Epoch [21], Iter [91/100] Loss: 0.4319\n",
      "Test MSE: 0.545437753200531\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6838\n",
      "Epoch [2], Iter [91/100] Loss: 0.5707\n",
      "Epoch [3], Iter [91/100] Loss: 0.5667\n",
      "Epoch [4], Iter [91/100] Loss: 0.5347\n",
      "Epoch [5], Iter [91/100] Loss: 0.5864\n",
      "Epoch [6], Iter [91/100] Loss: 0.5611\n",
      "Epoch [7], Iter [91/100] Loss: 0.5280\n",
      "Epoch [8], Iter [91/100] Loss: 0.5085\n",
      "Epoch [9], Iter [91/100] Loss: 0.5019\n",
      "Epoch [10], Iter [91/100] Loss: 0.5400\n",
      "Epoch [11], Iter [91/100] Loss: 0.5153\n",
      "Epoch [12], Iter [91/100] Loss: 0.5084\n",
      "Epoch [13], Iter [91/100] Loss: 0.5181\n",
      "Epoch [14], Iter [91/100] Loss: 0.4411\n",
      "Epoch [15], Iter [91/100] Loss: 0.4365\n",
      "Epoch [16], Iter [91/100] Loss: 0.4522\n",
      "Epoch [17], Iter [91/100] Loss: 0.4566\n",
      "Epoch [18], Iter [91/100] Loss: 0.4437\n",
      "Test MSE: 0.5243545770645142\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7103\n",
      "Epoch [2], Iter [91/101] Loss: 0.6085\n",
      "Epoch [3], Iter [91/101] Loss: 0.5322\n",
      "Epoch [4], Iter [91/101] Loss: 0.5238\n",
      "Epoch [5], Iter [91/101] Loss: 0.5139\n",
      "Epoch [6], Iter [91/101] Loss: 0.4947\n",
      "Epoch [7], Iter [91/101] Loss: 0.4738\n",
      "Epoch [8], Iter [91/101] Loss: 0.4892\n",
      "Epoch [9], Iter [91/101] Loss: 0.4964\n",
      "Epoch [10], Iter [91/101] Loss: 0.5097\n",
      "Epoch [11], Iter [91/101] Loss: 0.4719\n",
      "Epoch [12], Iter [91/101] Loss: 0.4470\n",
      "Epoch [13], Iter [91/101] Loss: 0.4405\n",
      "Epoch [14], Iter [91/101] Loss: 0.4557\n",
      "Epoch [15], Iter [91/101] Loss: 1.4351\n",
      "Epoch [16], Iter [91/101] Loss: 0.4598\n",
      "Epoch [17], Iter [91/101] Loss: 0.4075\n",
      "Epoch [18], Iter [91/101] Loss: 0.4259\n",
      "Epoch [19], Iter [91/101] Loss: 0.3972\n",
      "Epoch [20], Iter [91/101] Loss: 0.3879\n",
      "Epoch [21], Iter [91/101] Loss: 0.4152\n",
      "Epoch [22], Iter [91/101] Loss: 0.3868\n",
      "Test MSE: 0.5155821442604065\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6249\n",
      "Epoch [2], Iter [91/101] Loss: 0.5766\n",
      "Epoch [3], Iter [91/101] Loss: 0.5676\n",
      "Epoch [4], Iter [91/101] Loss: 0.5276\n",
      "Epoch [5], Iter [91/101] Loss: 0.5365\n",
      "Epoch [6], Iter [91/101] Loss: 0.5061\n",
      "Epoch [7], Iter [91/101] Loss: 0.5185\n",
      "Epoch [8], Iter [91/101] Loss: 0.4750\n",
      "Epoch [9], Iter [91/101] Loss: 0.4862\n",
      "Epoch [10], Iter [91/101] Loss: 0.4871\n",
      "Epoch [11], Iter [91/101] Loss: 0.4469\n",
      "Epoch [12], Iter [91/101] Loss: 0.4629\n",
      "Epoch [13], Iter [91/101] Loss: 0.4358\n",
      "Epoch [14], Iter [91/101] Loss: 0.4503\n",
      "Epoch [15], Iter [91/101] Loss: 0.4667\n",
      "Epoch [16], Iter [91/101] Loss: 0.4333\n",
      "Epoch [17], Iter [91/101] Loss: 0.4042\n",
      "Epoch [18], Iter [91/101] Loss: 0.4250\n",
      "Epoch [19], Iter [91/101] Loss: 0.3997\n",
      "Epoch [20], Iter [91/101] Loss: 0.4313\n",
      "Epoch [21], Iter [91/101] Loss: 0.4029\n",
      "Epoch [22], Iter [91/101] Loss: 0.3842\n",
      "Epoch [23], Iter [91/101] Loss: 0.3904\n",
      "Test MSE: 0.5118170380592346\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6108\n",
      "Epoch [2], Iter [91/101] Loss: 0.5821\n",
      "Epoch [3], Iter [91/101] Loss: 0.5491\n",
      "Epoch [4], Iter [91/101] Loss: 0.5365\n",
      "Epoch [5], Iter [91/101] Loss: 0.5226\n",
      "Epoch [6], Iter [91/101] Loss: 0.4966\n",
      "Epoch [7], Iter [91/101] Loss: 0.4920\n",
      "Epoch [8], Iter [91/101] Loss: 0.4816\n",
      "Epoch [9], Iter [91/101] Loss: 0.4783\n",
      "Epoch [10], Iter [91/101] Loss: 0.4495\n",
      "Epoch [11], Iter [91/101] Loss: 0.4389\n",
      "Epoch [12], Iter [91/101] Loss: 0.4565\n",
      "Epoch [13], Iter [91/101] Loss: 0.4645\n",
      "Epoch [14], Iter [91/101] Loss: 0.4462\n",
      "Epoch [15], Iter [91/101] Loss: 0.4214\n",
      "Epoch [16], Iter [91/101] Loss: 0.4114\n",
      "Epoch [17], Iter [91/101] Loss: 0.4175\n",
      "Epoch [18], Iter [91/101] Loss: 0.4066\n",
      "Epoch [19], Iter [91/101] Loss: 0.4119\n",
      "Epoch [20], Iter [91/101] Loss: 0.4045\n",
      "Epoch [21], Iter [91/101] Loss: 0.3771\n",
      "Epoch [22], Iter [91/101] Loss: 0.3746\n",
      "Epoch [23], Iter [91/101] Loss: 0.3779\n",
      "Epoch [24], Iter [91/101] Loss: 0.3823\n",
      "Test MSE: 0.5084636807441711\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6058\n",
      "Epoch [2], Iter [91/100] Loss: 0.6505\n",
      "Epoch [3], Iter [91/100] Loss: 0.5661\n",
      "Epoch [4], Iter [91/100] Loss: 0.5079\n",
      "Epoch [5], Iter [91/100] Loss: 0.5227\n",
      "Epoch [6], Iter [91/100] Loss: 0.5081\n",
      "Epoch [7], Iter [91/100] Loss: 0.4992\n",
      "Epoch [8], Iter [91/100] Loss: 0.4713\n",
      "Epoch [9], Iter [91/100] Loss: 0.4805\n",
      "Epoch [10], Iter [91/100] Loss: 0.4566\n",
      "Epoch [11], Iter [91/100] Loss: 0.4841\n",
      "Epoch [12], Iter [91/100] Loss: 0.4521\n",
      "Epoch [13], Iter [91/100] Loss: 0.4365\n",
      "Epoch [14], Iter [91/100] Loss: 0.4308\n",
      "Epoch [15], Iter [91/100] Loss: 0.4035\n",
      "Epoch [16], Iter [91/100] Loss: 0.4271\n",
      "Epoch [17], Iter [91/100] Loss: 0.4304\n",
      "Epoch [18], Iter [91/100] Loss: 0.3997\n",
      "Epoch [19], Iter [91/100] Loss: 0.4407\n",
      "Epoch [20], Iter [91/100] Loss: 0.3908\n",
      "Epoch [21], Iter [91/100] Loss: 0.3628\n",
      "Test MSE: 0.5032047033309937\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6671\n",
      "Epoch [2], Iter [91/101] Loss: 0.5887\n",
      "Epoch [3], Iter [91/101] Loss: 0.5640\n",
      "Epoch [4], Iter [91/101] Loss: 0.5364\n",
      "Epoch [5], Iter [91/101] Loss: 0.5311\n",
      "Epoch [6], Iter [91/101] Loss: 0.5402\n",
      "Epoch [7], Iter [91/101] Loss: 0.5137\n",
      "Epoch [8], Iter [91/101] Loss: 0.5238\n",
      "Epoch [9], Iter [91/101] Loss: 0.4757\n",
      "Epoch [10], Iter [91/101] Loss: 0.5051\n",
      "Epoch [11], Iter [91/101] Loss: 0.4596\n",
      "Epoch [12], Iter [91/101] Loss: 0.5357\n",
      "Epoch [13], Iter [91/101] Loss: 0.4593\n",
      "Epoch [14], Iter [91/101] Loss: 0.4613\n",
      "Epoch [15], Iter [91/101] Loss: 0.4243\n",
      "Epoch [16], Iter [91/101] Loss: 0.4377\n",
      "Epoch [17], Iter [91/101] Loss: 0.4244\n",
      "Epoch [18], Iter [91/101] Loss: 0.4365\n",
      "Epoch [19], Iter [91/101] Loss: 0.3831\n",
      "Test MSE: 0.5292960405349731\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6680\n",
      "Epoch [2], Iter [91/101] Loss: 0.6939\n",
      "Epoch [3], Iter [91/101] Loss: 0.6221\n",
      "Epoch [4], Iter [91/101] Loss: 0.5812\n",
      "Epoch [5], Iter [91/101] Loss: 0.5444\n",
      "Epoch [6], Iter [91/101] Loss: 0.5525\n",
      "Epoch [7], Iter [91/101] Loss: 0.5134\n",
      "Epoch [8], Iter [91/101] Loss: 0.5406\n",
      "Epoch [9], Iter [91/101] Loss: 0.5121\n",
      "Epoch [10], Iter [91/101] Loss: 0.5191\n",
      "Epoch [11], Iter [91/101] Loss: 0.5015\n",
      "Epoch [12], Iter [91/101] Loss: 0.5114\n",
      "Epoch [13], Iter [91/101] Loss: 0.4811\n",
      "Epoch [14], Iter [91/101] Loss: 0.4680\n",
      "Epoch [15], Iter [91/101] Loss: 0.4336\n",
      "Epoch [16], Iter [91/101] Loss: 0.4463\n",
      "Epoch [17], Iter [91/101] Loss: 0.4509\n",
      "Test MSE: 0.5534605979919434\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6763\n",
      "Epoch [2], Iter [91/101] Loss: 0.5952\n",
      "Epoch [3], Iter [91/101] Loss: 0.5954\n",
      "Epoch [4], Iter [91/101] Loss: 0.5696\n",
      "Epoch [5], Iter [91/101] Loss: 0.5501\n",
      "Epoch [6], Iter [91/101] Loss: 0.5317\n",
      "Epoch [7], Iter [91/101] Loss: 0.5321\n",
      "Epoch [8], Iter [91/101] Loss: 0.4992\n",
      "Epoch [9], Iter [91/101] Loss: 0.5123\n",
      "Epoch [10], Iter [91/101] Loss: 0.4948\n",
      "Epoch [11], Iter [91/101] Loss: 0.4817\n",
      "Epoch [12], Iter [91/101] Loss: 0.4972\n",
      "Epoch [13], Iter [91/101] Loss: 0.5128\n",
      "Epoch [14], Iter [91/101] Loss: 0.4628\n",
      "Epoch [15], Iter [91/101] Loss: 0.4930\n",
      "Epoch [16], Iter [91/101] Loss: 0.4576\n",
      "Epoch [17], Iter [91/101] Loss: 0.4405\n",
      "Epoch [18], Iter [91/101] Loss: 0.4468\n",
      "Epoch [19], Iter [91/101] Loss: 0.4586\n",
      "Epoch [20], Iter [91/101] Loss: 0.4395\n",
      "Test MSE: 0.5544581413269043\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7231\n",
      "Epoch [2], Iter [91/100] Loss: 0.6506\n",
      "Epoch [3], Iter [91/100] Loss: 0.5646\n",
      "Epoch [4], Iter [91/100] Loss: 0.6098\n",
      "Epoch [5], Iter [91/100] Loss: 0.5740\n",
      "Epoch [6], Iter [91/100] Loss: 0.5390\n",
      "Epoch [7], Iter [91/100] Loss: 0.5359\n",
      "Epoch [8], Iter [91/100] Loss: 0.5140\n",
      "Epoch [9], Iter [91/100] Loss: 0.5136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Iter [91/100] Loss: 0.5090\n",
      "Epoch [11], Iter [91/100] Loss: 0.4735\n",
      "Epoch [12], Iter [91/100] Loss: 0.5196\n",
      "Epoch [13], Iter [91/100] Loss: 0.4999\n",
      "Epoch [14], Iter [91/100] Loss: 0.4843\n",
      "Epoch [15], Iter [91/100] Loss: 0.5597\n",
      "Epoch [16], Iter [91/100] Loss: 0.4795\n",
      "Epoch [17], Iter [91/100] Loss: 0.4406\n",
      "Epoch [18], Iter [91/100] Loss: 0.4414\n",
      "Epoch [19], Iter [91/100] Loss: 0.4305\n",
      "Epoch [20], Iter [91/100] Loss: 0.4266\n",
      "Epoch [21], Iter [91/100] Loss: 0.4237\n",
      "Epoch [22], Iter [91/100] Loss: 0.4118\n",
      "Test MSE: 0.6449451446533203\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7141\n",
      "Epoch [2], Iter [91/100] Loss: 0.6192\n",
      "Epoch [3], Iter [91/100] Loss: 0.6048\n",
      "Epoch [4], Iter [91/100] Loss: 0.6183\n",
      "Epoch [5], Iter [91/100] Loss: 0.5507\n",
      "Epoch [6], Iter [91/100] Loss: 0.5391\n",
      "Epoch [7], Iter [91/100] Loss: 0.5473\n",
      "Epoch [8], Iter [91/100] Loss: 0.5369\n",
      "Epoch [9], Iter [91/100] Loss: 0.5401\n",
      "Epoch [10], Iter [91/100] Loss: 0.4608\n",
      "Epoch [11], Iter [91/100] Loss: 0.4947\n",
      "Epoch [12], Iter [91/100] Loss: 0.5051\n",
      "Epoch [13], Iter [91/100] Loss: 0.4910\n",
      "Epoch [14], Iter [91/100] Loss: 0.4856\n",
      "Epoch [15], Iter [91/100] Loss: 0.4520\n",
      "Epoch [16], Iter [91/100] Loss: 0.4667\n",
      "Epoch [17], Iter [91/100] Loss: 0.4614\n",
      "Epoch [18], Iter [91/100] Loss: 0.4303\n",
      "Epoch [19], Iter [91/100] Loss: 0.4087\n",
      "Epoch [20], Iter [91/100] Loss: 0.4247\n",
      "Epoch [21], Iter [91/100] Loss: 0.4299\n",
      "Epoch [22], Iter [91/100] Loss: 0.4324\n",
      "Epoch [23], Iter [91/100] Loss: 0.4024\n",
      "Epoch [24], Iter [91/100] Loss: 0.3865\n",
      "Epoch [25], Iter [91/100] Loss: 0.4021\n",
      "Epoch [26], Iter [91/100] Loss: 0.4062\n",
      "Test MSE: 0.5470300316810608\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6714\n",
      "Epoch [2], Iter [91/101] Loss: 0.6900\n",
      "Epoch [3], Iter [91/101] Loss: 0.6024\n",
      "Epoch [4], Iter [91/101] Loss: 0.5956\n",
      "Epoch [5], Iter [91/101] Loss: 0.6219\n",
      "Epoch [6], Iter [91/101] Loss: 0.5474\n",
      "Epoch [7], Iter [91/101] Loss: 0.5529\n",
      "Epoch [8], Iter [91/101] Loss: 0.4862\n",
      "Epoch [9], Iter [91/101] Loss: 0.5413\n",
      "Epoch [10], Iter [91/101] Loss: 0.4942\n",
      "Epoch [11], Iter [91/101] Loss: 0.5106\n",
      "Epoch [12], Iter [91/101] Loss: 0.4659\n",
      "Epoch [13], Iter [91/101] Loss: 0.5195\n",
      "Epoch [14], Iter [91/101] Loss: 0.4768\n",
      "Epoch [15], Iter [91/101] Loss: 0.4568\n",
      "Epoch [16], Iter [91/101] Loss: 0.4573\n",
      "Epoch [17], Iter [91/101] Loss: 0.4268\n",
      "Epoch [18], Iter [91/101] Loss: 0.4607\n",
      "Epoch [19], Iter [91/101] Loss: 0.4198\n",
      "Epoch [20], Iter [91/101] Loss: 0.4125\n",
      "Test MSE: 0.6292594075202942\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6685\n",
      "Epoch [2], Iter [91/100] Loss: 0.6517\n",
      "Epoch [3], Iter [91/100] Loss: 0.5852\n",
      "Epoch [4], Iter [91/100] Loss: 0.5680\n",
      "Epoch [5], Iter [91/100] Loss: 0.5378\n",
      "Epoch [6], Iter [91/100] Loss: 0.5517\n",
      "Epoch [7], Iter [91/100] Loss: 0.5715\n",
      "Epoch [8], Iter [91/100] Loss: 0.5462\n",
      "Epoch [9], Iter [91/100] Loss: 0.5174\n",
      "Epoch [10], Iter [91/100] Loss: 0.5289\n",
      "Epoch [11], Iter [91/100] Loss: 0.5368\n",
      "Epoch [12], Iter [91/100] Loss: 0.5146\n",
      "Epoch [13], Iter [91/100] Loss: 0.4912\n",
      "Epoch [14], Iter [91/100] Loss: 0.4872\n",
      "Epoch [15], Iter [91/100] Loss: 0.5053\n",
      "Epoch [16], Iter [91/100] Loss: 0.4803\n",
      "Epoch [17], Iter [91/100] Loss: 0.4533\n",
      "Epoch [18], Iter [91/100] Loss: 0.4753\n",
      "Epoch [19], Iter [91/100] Loss: 0.4451\n",
      "Epoch [20], Iter [91/100] Loss: 0.4435\n",
      "Epoch [21], Iter [91/100] Loss: 0.4067\n",
      "Epoch [22], Iter [91/100] Loss: 0.4482\n",
      "Epoch [23], Iter [91/100] Loss: 0.4165\n",
      "Epoch [24], Iter [91/100] Loss: 0.4056\n",
      "Epoch [25], Iter [91/100] Loss: 0.4051\n",
      "Test MSE: 0.5461668968200684\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.7012\n",
      "Epoch [2], Iter [91/100] Loss: 0.6399\n",
      "Epoch [3], Iter [91/100] Loss: 0.5663\n",
      "Epoch [4], Iter [91/100] Loss: 0.5776\n",
      "Epoch [5], Iter [91/100] Loss: 0.5764\n",
      "Epoch [6], Iter [91/100] Loss: 0.5358\n",
      "Epoch [7], Iter [91/100] Loss: 0.5396\n",
      "Epoch [8], Iter [91/100] Loss: 0.5043\n",
      "Epoch [9], Iter [91/100] Loss: 0.5201\n",
      "Epoch [10], Iter [91/100] Loss: 0.4921\n",
      "Epoch [11], Iter [91/100] Loss: 0.5170\n",
      "Epoch [12], Iter [91/100] Loss: 0.4920\n",
      "Epoch [13], Iter [91/100] Loss: 0.5237\n",
      "Epoch [14], Iter [91/100] Loss: 0.4717\n",
      "Epoch [15], Iter [91/100] Loss: 0.4407\n",
      "Epoch [16], Iter [91/100] Loss: 0.4554\n",
      "Epoch [17], Iter [91/100] Loss: 0.4822\n",
      "Test MSE: 0.5261594653129578\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6561\n",
      "Epoch [2], Iter [91/101] Loss: 0.6188\n",
      "Epoch [3], Iter [91/101] Loss: 0.6218\n",
      "Epoch [4], Iter [91/101] Loss: 0.5800\n",
      "Epoch [5], Iter [91/101] Loss: 0.5373\n",
      "Epoch [6], Iter [91/101] Loss: 0.4978\n",
      "Epoch [7], Iter [91/101] Loss: 0.5283\n",
      "Epoch [8], Iter [91/101] Loss: 0.4721\n",
      "Epoch [9], Iter [91/101] Loss: 0.5163\n",
      "Epoch [10], Iter [91/101] Loss: 0.4607\n",
      "Epoch [11], Iter [91/101] Loss: 0.4788\n",
      "Epoch [12], Iter [91/101] Loss: 0.4529\n",
      "Epoch [13], Iter [91/101] Loss: 0.4769\n",
      "Epoch [14], Iter [91/101] Loss: 0.4737\n",
      "Epoch [15], Iter [91/101] Loss: 0.4684\n",
      "Epoch [16], Iter [91/101] Loss: 0.4681\n",
      "Epoch [17], Iter [91/101] Loss: 0.4516\n",
      "Epoch [18], Iter [91/101] Loss: 0.4200\n",
      "Epoch [19], Iter [91/101] Loss: 0.4000\n",
      "Epoch [20], Iter [91/101] Loss: 0.4042\n",
      "Epoch [21], Iter [91/101] Loss: 0.4077\n",
      "Epoch [22], Iter [91/101] Loss: 0.4030\n",
      "Test MSE: 0.5219075679779053\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6737\n",
      "Epoch [2], Iter [91/101] Loss: 0.6237\n",
      "Epoch [3], Iter [91/101] Loss: 0.5787\n",
      "Epoch [4], Iter [91/101] Loss: 0.5410\n",
      "Epoch [5], Iter [91/101] Loss: 0.5661\n",
      "Epoch [6], Iter [91/101] Loss: 0.5000\n",
      "Epoch [7], Iter [91/101] Loss: 0.5050\n",
      "Epoch [8], Iter [91/101] Loss: 0.5376\n",
      "Epoch [9], Iter [91/101] Loss: 0.4678\n",
      "Epoch [10], Iter [91/101] Loss: 0.4712\n",
      "Epoch [11], Iter [91/101] Loss: 0.4657\n",
      "Epoch [12], Iter [91/101] Loss: 0.4434\n",
      "Epoch [13], Iter [91/101] Loss: 0.4276\n",
      "Epoch [14], Iter [91/101] Loss: 0.4542\n",
      "Epoch [15], Iter [91/101] Loss: 0.4589\n",
      "Epoch [16], Iter [91/101] Loss: 0.4389\n",
      "Epoch [17], Iter [91/101] Loss: 0.4336\n",
      "Epoch [18], Iter [91/101] Loss: 0.4261\n",
      "Epoch [19], Iter [91/101] Loss: 0.4102\n",
      "Epoch [20], Iter [91/101] Loss: 0.4268\n",
      "Test MSE: 0.5185378193855286\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6169\n",
      "Epoch [2], Iter [91/101] Loss: 0.5768\n",
      "Epoch [3], Iter [91/101] Loss: 0.5855\n",
      "Epoch [4], Iter [91/101] Loss: 0.5071\n",
      "Epoch [5], Iter [91/101] Loss: 0.4907\n",
      "Epoch [6], Iter [91/101] Loss: 0.4843\n",
      "Epoch [7], Iter [91/101] Loss: 0.5292\n",
      "Epoch [8], Iter [91/101] Loss: 0.4976\n",
      "Epoch [9], Iter [91/101] Loss: 0.4768\n",
      "Epoch [10], Iter [91/101] Loss: 0.4524\n",
      "Epoch [11], Iter [91/101] Loss: 0.4422\n",
      "Epoch [12], Iter [91/101] Loss: 0.4350\n",
      "Epoch [13], Iter [91/101] Loss: 0.4685\n",
      "Epoch [14], Iter [91/101] Loss: 0.4679\n",
      "Epoch [15], Iter [91/101] Loss: 0.4526\n",
      "Epoch [16], Iter [91/101] Loss: 0.4450\n",
      "Epoch [17], Iter [91/101] Loss: 0.4236\n",
      "Epoch [18], Iter [91/101] Loss: 0.4293\n",
      "Epoch [19], Iter [91/101] Loss: 0.3938\n",
      "Epoch [20], Iter [91/101] Loss: 0.3857\n",
      "Epoch [21], Iter [91/101] Loss: 0.3967\n",
      "Epoch [22], Iter [91/101] Loss: 0.3857\n",
      "Test MSE: 0.5127843618392944\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6061\n",
      "Epoch [2], Iter [91/100] Loss: 0.5801\n",
      "Epoch [3], Iter [91/100] Loss: 0.5622\n",
      "Epoch [4], Iter [91/100] Loss: 0.5086\n",
      "Epoch [5], Iter [91/100] Loss: 0.5130\n",
      "Epoch [6], Iter [91/100] Loss: 0.5091\n",
      "Epoch [7], Iter [91/100] Loss: 0.5242\n",
      "Epoch [8], Iter [91/100] Loss: 0.5014\n",
      "Epoch [9], Iter [91/100] Loss: 0.4460\n",
      "Epoch [10], Iter [91/100] Loss: 0.4484\n",
      "Epoch [11], Iter [91/100] Loss: 0.4378\n",
      "Epoch [12], Iter [91/100] Loss: 0.4497\n",
      "Epoch [13], Iter [91/100] Loss: 0.4469\n",
      "Epoch [14], Iter [91/100] Loss: 0.4264\n",
      "Epoch [15], Iter [91/100] Loss: 0.4311\n",
      "Epoch [16], Iter [91/100] Loss: 0.4382\n",
      "Epoch [17], Iter [91/100] Loss: 0.3984\n",
      "Epoch [18], Iter [91/100] Loss: 0.3927\n",
      "Epoch [19], Iter [91/100] Loss: 0.3906\n",
      "Test MSE: 0.5170398950576782\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6262\n",
      "Epoch [2], Iter [91/101] Loss: 0.6046\n",
      "Epoch [3], Iter [91/101] Loss: 0.6264\n",
      "Epoch [4], Iter [91/101] Loss: 0.5763\n",
      "Epoch [5], Iter [91/101] Loss: 0.5215\n",
      "Epoch [6], Iter [91/101] Loss: 0.5215\n",
      "Epoch [7], Iter [91/101] Loss: 0.5146\n",
      "Epoch [8], Iter [91/101] Loss: 0.5023\n",
      "Epoch [9], Iter [91/101] Loss: 0.4942\n",
      "Epoch [10], Iter [91/101] Loss: 0.4953\n",
      "Epoch [11], Iter [91/101] Loss: 0.4874\n",
      "Epoch [12], Iter [91/101] Loss: 0.4526\n",
      "Epoch [13], Iter [91/101] Loss: 0.4418\n",
      "Epoch [14], Iter [91/101] Loss: 0.4514\n",
      "Epoch [15], Iter [91/101] Loss: 0.4414\n",
      "Epoch [16], Iter [91/101] Loss: 0.4353\n",
      "Epoch [17], Iter [91/101] Loss: 0.4585\n",
      "Epoch [18], Iter [91/101] Loss: 0.4021\n",
      "Test MSE: 0.5274527668952942\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.7504\n",
      "Epoch [2], Iter [91/101] Loss: 0.6532\n",
      "Epoch [3], Iter [91/101] Loss: 0.6072\n",
      "Epoch [4], Iter [91/101] Loss: 0.5861\n",
      "Epoch [5], Iter [91/101] Loss: 0.6039\n",
      "Epoch [6], Iter [91/101] Loss: 0.5840\n",
      "Epoch [7], Iter [91/101] Loss: 0.5363\n",
      "Epoch [8], Iter [91/101] Loss: 0.5486\n",
      "Epoch [9], Iter [91/101] Loss: 0.5330\n",
      "Epoch [10], Iter [91/101] Loss: 0.4925\n",
      "Epoch [11], Iter [91/101] Loss: 0.5463\n",
      "Epoch [12], Iter [91/101] Loss: 0.5187\n",
      "Epoch [13], Iter [91/101] Loss: 0.4800\n",
      "Epoch [14], Iter [91/101] Loss: 0.4862\n",
      "Epoch [15], Iter [91/101] Loss: 0.4747\n",
      "Epoch [16], Iter [91/101] Loss: 0.5125\n",
      "Epoch [17], Iter [91/101] Loss: 0.4568\n",
      "Epoch [18], Iter [91/101] Loss: 0.4835\n",
      "Epoch [19], Iter [91/101] Loss: 0.4766\n",
      "Epoch [20], Iter [91/101] Loss: 0.4536\n",
      "Epoch [21], Iter [91/101] Loss: 0.4354\n",
      "Epoch [22], Iter [91/101] Loss: 0.4468\n",
      "Epoch [23], Iter [91/101] Loss: 0.4200\n",
      "Epoch [24], Iter [91/101] Loss: 0.4367\n",
      "Test MSE: 0.553655743598938\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6516\n",
      "Epoch [2], Iter [91/101] Loss: 0.6279\n",
      "Epoch [3], Iter [91/101] Loss: 0.5980\n",
      "Epoch [4], Iter [91/101] Loss: 0.6018\n",
      "Epoch [5], Iter [91/101] Loss: 0.5873\n",
      "Epoch [6], Iter [91/101] Loss: 0.5339\n",
      "Epoch [7], Iter [91/101] Loss: 0.5408\n",
      "Epoch [8], Iter [91/101] Loss: 0.5481\n",
      "Epoch [9], Iter [91/101] Loss: 0.5452\n",
      "Epoch [10], Iter [91/101] Loss: 0.5024\n",
      "Epoch [11], Iter [91/101] Loss: 0.4936\n",
      "Epoch [12], Iter [91/101] Loss: 0.5014\n",
      "Epoch [13], Iter [91/101] Loss: 0.4890\n",
      "Epoch [14], Iter [91/101] Loss: 0.4583\n",
      "Epoch [15], Iter [91/101] Loss: 0.4598\n",
      "Epoch [16], Iter [91/101] Loss: 0.4546\n",
      "Epoch [17], Iter [91/101] Loss: 0.4533\n",
      "Epoch [18], Iter [91/101] Loss: 0.4468\n",
      "Epoch [19], Iter [91/101] Loss: 0.4460\n",
      "Epoch [20], Iter [91/101] Loss: 0.4551\n",
      "Epoch [21], Iter [91/101] Loss: 0.4406\n",
      "Test MSE: 0.5547301173210144\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6739\n",
      "Epoch [2], Iter [91/100] Loss: 0.6937\n",
      "Epoch [3], Iter [91/100] Loss: 0.6190\n",
      "Epoch [4], Iter [91/100] Loss: 0.5476\n",
      "Epoch [5], Iter [91/100] Loss: 0.5715\n",
      "Epoch [6], Iter [91/100] Loss: 0.5247\n",
      "Epoch [7], Iter [91/100] Loss: 0.5621\n",
      "Epoch [8], Iter [91/100] Loss: 0.5341\n",
      "Epoch [9], Iter [91/100] Loss: 0.4892\n",
      "Epoch [10], Iter [91/100] Loss: 0.5306\n",
      "Epoch [11], Iter [91/100] Loss: 0.4684\n",
      "Epoch [12], Iter [91/100] Loss: 0.5357\n",
      "Epoch [13], Iter [91/100] Loss: 0.4591\n",
      "Epoch [14], Iter [91/100] Loss: 0.4717\n",
      "Epoch [15], Iter [91/100] Loss: 0.4432\n",
      "Epoch [16], Iter [91/100] Loss: 0.4515\n",
      "Epoch [17], Iter [91/100] Loss: 0.4258\n",
      "Epoch [18], Iter [91/100] Loss: 0.4617\n",
      "Test MSE: 0.6033324599266052\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6555\n",
      "Epoch [2], Iter [91/100] Loss: 0.6073\n",
      "Epoch [3], Iter [91/100] Loss: 0.5798\n",
      "Epoch [4], Iter [91/100] Loss: 0.5603\n",
      "Epoch [5], Iter [91/100] Loss: 0.5817\n",
      "Epoch [6], Iter [91/100] Loss: 0.5486\n",
      "Epoch [7], Iter [91/100] Loss: 0.4951\n",
      "Epoch [8], Iter [91/100] Loss: 0.5175\n",
      "Epoch [9], Iter [91/100] Loss: 0.5201\n",
      "Epoch [10], Iter [91/100] Loss: 0.5204\n",
      "Epoch [11], Iter [91/100] Loss: 0.5118\n",
      "Epoch [12], Iter [91/100] Loss: 0.5009\n",
      "Epoch [13], Iter [91/100] Loss: 0.4892\n",
      "Epoch [14], Iter [91/100] Loss: 0.4923\n",
      "Epoch [15], Iter [91/100] Loss: 0.5041\n",
      "Epoch [16], Iter [91/100] Loss: 0.4522\n",
      "Epoch [17], Iter [91/100] Loss: 0.4294\n",
      "Epoch [18], Iter [91/100] Loss: 0.4204\n",
      "Epoch [19], Iter [91/100] Loss: 0.4582\n",
      "Epoch [20], Iter [91/100] Loss: 0.4159\n",
      "Epoch [21], Iter [91/100] Loss: 0.4222\n",
      "Test MSE: 0.550116240978241\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6561\n",
      "Epoch [2], Iter [91/101] Loss: 0.6092\n",
      "Epoch [3], Iter [91/101] Loss: 0.5877\n",
      "Epoch [4], Iter [91/101] Loss: 0.5521\n",
      "Epoch [5], Iter [91/101] Loss: 0.6019\n",
      "Epoch [6], Iter [91/101] Loss: 0.5698\n",
      "Epoch [7], Iter [91/101] Loss: 0.5493\n",
      "Epoch [8], Iter [91/101] Loss: 0.5281\n",
      "Epoch [9], Iter [91/101] Loss: 0.5456\n",
      "Epoch [10], Iter [91/101] Loss: 0.5104\n",
      "Epoch [11], Iter [91/101] Loss: 0.5035\n",
      "Epoch [12], Iter [91/101] Loss: 0.5171\n",
      "Epoch [13], Iter [91/101] Loss: 0.5038\n",
      "Epoch [14], Iter [91/101] Loss: 0.4815\n",
      "Epoch [15], Iter [91/101] Loss: 0.4842\n",
      "Epoch [16], Iter [91/101] Loss: 0.4674\n",
      "Epoch [17], Iter [91/101] Loss: 0.4772\n",
      "Epoch [18], Iter [91/101] Loss: 0.4369\n",
      "Epoch [19], Iter [91/101] Loss: 0.4647\n",
      "Test MSE: 0.6302177906036377\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6930\n",
      "Epoch [2], Iter [91/100] Loss: 0.6267\n",
      "Epoch [3], Iter [91/100] Loss: 0.6001\n",
      "Epoch [4], Iter [91/100] Loss: 0.5580\n",
      "Epoch [5], Iter [91/100] Loss: 0.5834\n",
      "Epoch [6], Iter [91/100] Loss: 0.5655\n",
      "Epoch [7], Iter [91/100] Loss: 0.5402\n",
      "Epoch [8], Iter [91/100] Loss: 0.5554\n",
      "Epoch [9], Iter [91/100] Loss: 0.5129\n",
      "Epoch [10], Iter [91/100] Loss: 0.5289\n",
      "Epoch [11], Iter [91/100] Loss: 0.5395\n",
      "Epoch [12], Iter [91/100] Loss: 0.5092\n",
      "Epoch [13], Iter [91/100] Loss: 0.4670\n",
      "Epoch [14], Iter [91/100] Loss: 0.4820\n",
      "Epoch [15], Iter [91/100] Loss: 0.4949\n",
      "Epoch [16], Iter [91/100] Loss: 0.4870\n",
      "Epoch [17], Iter [91/100] Loss: 0.4801\n",
      "Epoch [18], Iter [91/100] Loss: 0.4584\n",
      "Epoch [19], Iter [91/100] Loss: 0.4433\n",
      "Epoch [20], Iter [91/100] Loss: 0.4209\n",
      "Epoch [21], Iter [91/100] Loss: 0.4101\n",
      "Test MSE: 0.5446599721908569\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/100] Loss: 0.6362\n",
      "Epoch [2], Iter [91/100] Loss: 0.5819\n",
      "Epoch [3], Iter [91/100] Loss: 0.6033\n",
      "Epoch [4], Iter [91/100] Loss: 0.5646\n",
      "Epoch [5], Iter [91/100] Loss: 0.5768\n",
      "Epoch [6], Iter [91/100] Loss: 0.5598\n",
      "Epoch [7], Iter [91/100] Loss: 0.5324\n",
      "Epoch [8], Iter [91/100] Loss: 0.5379\n",
      "Epoch [9], Iter [91/100] Loss: 0.5591\n",
      "Epoch [10], Iter [91/100] Loss: 0.5202\n",
      "Epoch [11], Iter [91/100] Loss: 0.4694\n",
      "Epoch [12], Iter [91/100] Loss: 0.5070\n",
      "Epoch [13], Iter [91/100] Loss: 0.5127\n",
      "Epoch [14], Iter [91/100] Loss: 0.5025\n",
      "Epoch [15], Iter [91/100] Loss: 0.4572\n",
      "Epoch [16], Iter [91/100] Loss: 0.4236\n",
      "Epoch [17], Iter [91/100] Loss: 0.4598\n",
      "Epoch [18], Iter [91/100] Loss: 0.4372\n",
      "Epoch [19], Iter [91/100] Loss: 0.4531\n",
      "Epoch [20], Iter [91/100] Loss: 0.4388\n",
      "Epoch [21], Iter [91/100] Loss: 0.4454\n",
      "Epoch [22], Iter [91/100] Loss: 0.4306\n",
      "Epoch [23], Iter [91/100] Loss: 0.4001\n",
      "Test MSE: 0.5255058407783508\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n",
      "Starting training\n",
      "Epoch [1], Iter [91/101] Loss: 0.6630\n",
      "Epoch [2], Iter [91/101] Loss: 0.6114\n",
      "Epoch [3], Iter [91/101] Loss: 0.5733\n",
      "Epoch [4], Iter [91/101] Loss: 0.5612\n",
      "Epoch [5], Iter [91/101] Loss: 0.5488\n",
      "Epoch [6], Iter [91/101] Loss: 0.5464\n",
      "Epoch [7], Iter [91/101] Loss: 0.5227\n",
      "Epoch [8], Iter [91/101] Loss: 0.4968\n",
      "Epoch [9], Iter [91/101] Loss: 0.4794\n",
      "Epoch [10], Iter [91/101] Loss: 0.4670\n",
      "Epoch [11], Iter [91/101] Loss: 0.4748\n",
      "Epoch [12], Iter [91/101] Loss: 0.5021\n",
      "Epoch [13], Iter [91/101] Loss: 0.4483\n",
      "Epoch [14], Iter [91/101] Loss: 0.4509\n",
      "Epoch [15], Iter [91/101] Loss: 0.4348\n",
      "Epoch [16], Iter [91/101] Loss: 0.4412\n",
      "Epoch [17], Iter [91/101] Loss: 0.4397\n",
      "Epoch [18], Iter [91/101] Loss: 0.4259\n",
      "Epoch [19], Iter [91/101] Loss: 0.4285\n",
      "Epoch [20], Iter [91/101] Loss: 0.4240\n",
      "Epoch [21], Iter [91/101] Loss: 0.4283\n",
      "Epoch [22], Iter [91/101] Loss: 0.4258\n",
      "Test MSE: 0.5195324420928955\n",
      "writing predictions\n",
      "writing descriptions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "for j in range(n_runs):\n",
    "    model_training_description[\"RUN_NR\"] = j\n",
    "    for i in range(12):\n",
    "        description[\"MONTHS_USED\"] = np.sort([i]).tolist()\n",
    "        unet = train_unet(description, model_training_description, output_folder)\n",
    "        predict_save_unet(description, model_training_description, output_folder, unet, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f2e6a",
   "metadata": {},
   "source": [
    "# Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323b9843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 matching runs found\n",
      "3 matching runs found\n"
     ]
    }
   ],
   "source": [
    "c_all = {\n",
    "    \"DATASET_DESCRIPTION\": {'MONTHS_USED': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "                            'MONTHS_USED_IN_PREDICTION': [0]},\n",
    "    \"MODEL_TRAINING_DESCRIPTION\": {}\n",
    "}\n",
    "\n",
    "descriptions, predictions, gt, masks = load_data_for_comparison(output_folder, c_all)\n",
    "\n",
    "c_all_use_p = {\n",
    "    \"DATASET_DESCRIPTION\": {'MONTHS_USED': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "                            'MONTHS_USED_IN_PREDICTION': [-1, 0]},\n",
    "    \"MODEL_TRAINING_DESCRIPTION\": {}\n",
    "}\n",
    "\n",
    "descriptions_p, predictions_p, gt_p, masks_p = load_data_for_comparison(output_folder, c_all_use_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a08c76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n",
      "3 matching runs found\n"
     ]
    }
   ],
   "source": [
    "r2_single = []\n",
    "\n",
    "for i in range(12):\n",
    "    r2_single.append([])\n",
    "    c_single = {\n",
    "        \"DATASET_DESCRIPTION\": {'MONTHS_USED': [i],\n",
    "                                'MONTHS_USED_IN_PREDICTION': [0]},\n",
    "        \"MODEL_TRAINING_DESCRIPTION\": {}\n",
    "    }\n",
    "    descriptions_s, predictions_s, gt_s, masks_s = load_data_for_comparison(output_folder, c_single)\n",
    "    for j in range(3):\n",
    "        r2 = get_r2(predictions_s[j][i], gt_s[j][i], masks_s[j][i])\n",
    "        r2_single[-1].append(get_weighted_average(r2, descriptions[j][\"DATASET_DESCRIPTION\"])[0])\n",
    "        \n",
    "r2_single = np.mean(np.array(r2_single),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be339ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_all = []\n",
    "r2_all_use_p = []\n",
    "\n",
    "for j in range(3):\n",
    "    r2_all.append([])\n",
    "    r2_all_use_p.append([])\n",
    "    for i in range(12):\n",
    "        r2 = get_r2(predictions[j][i], gt[j][i], masks[j][i])\n",
    "        r2_p = get_r2(predictions_p[j][i], gt_p[j][i], masks_p[j][i])\n",
    "        r2_all[-1].append(get_weighted_average(r2, descriptions[j][\"DATASET_DESCRIPTION\"])[0])\n",
    "        r2_all_use_p[-1].append(get_weighted_average(r2_p, descriptions_p[j][\"DATASET_DESCRIPTION\"])[0])\n",
    "        \n",
    "r2_all = np.mean(np.array(r2_all),axis=0)\n",
    "r2_all_use_p = np.mean(np.array(r2_all_use_p),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1264aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28b80cf7220>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABTqUlEQVR4nO3deVxUVf/A8c9h30EBlUVlUdwQF1BLTSxzN5dyyVbbsyetrEcty7IyfdKesrJfmZk9pqmZe+bWpuaGKy64AiqLCigoyDpzfn9cRFTUEQcuM5z368VL5s69d74T9OXM9577PUJKiaIoimK9bPQOQFEURalYKtEriqJYOZXoFUVRrJxK9IqiKFZOJXpFURQrZ6d3AGXx8fGRQUFBeoehKIpiMXbu3JkupfQt67kqmeiDgoLYsWOH3mEoiqJYDCHEiRs9p0o3iqIoVk4lekVRFCunEr2iKIqVq5I1esV0hYWFJCUlkZeXp3coimJWTk5OBAYGYm9vr3coFk8leguXlJSEu7s7QUFBCCH0DkdRzEJKSUZGBklJSQQHB+sdjsVTpRsLl5eXh7e3t0ryilURQuDt7a0+qZqJSvRWQCV5xRqp32vzUaWbKqbQWEhWfhZZ+Vlk5meSmZep/ZufSahXKJ3rdtY7REVRLIxK9BVESkluUe6VhH3N11Xb865syy7Mvul5P+r4EQ+EPlBJ7+LWEhMT6dOnD/v37y/Z9t577+Hm5sYbb7zBsGHDWLduHfHx8Tg6OpKenk5UVBSJiYk3PGdmZibz5s3jpZdeKndcnTt3ZurUqURFRZXcgOfj41Pu8ymKJTMp0QshegDTAFtgppRy8jXPPwqMKX6YDQyXUu4tfs4LmAmEAxJ4Wkq5xSzR6+jkhZNsTN5IRm7GVcn7fP55svK0JF5gLLjh8W72bng6euLl6IWXkxdBnkF4OXpd2XbNl6uDK6P+HMX4f8bj7exNe//2lfhu74ytrS2zZs1i+PDhJu2fmZnJV199dUeJXlGUK26Z6IUQtsB0oCuQBMQIIZZLKQ+W2i0BiJZSnhdC9ARmAO2Kn5sGrJZSDhRCOAAuZn0Hleh83nnWJK5hRfwKYtNiAbAVtng6epYk6EC3QMK9w69P2k5eJds8HT2xt7n9KWOf3vspw1YPY9Rfo5jdYzaNazY291usEK+++iqffvopzz333HXPTZkyhYULF5Kfn8+AAQOYMGECY8eO5fjx47Rs2ZKuXbsyZcqUG557+PDhxMTEkJuby8CBA5kwYUJFvhVFsUimjOjbAseklPEAQoj5QD+gJNFLKTeX2n8rEFi8rwfQCRhWvF8BcONhbhWUb8jnr1N/sTJ+JZuSNlEki2hYoyGjIkfRPag7dVzrYCMq55q2u4M7X3X5isd+e4zh64czt9fcq56fsOIAB1MumPU1m/p78O4Dze7oHPXq1aNjx47MmTOHBx64UnZau3YtR48eZfv27Ugp6du3Lxs2bGDy5Mns37+fPXv23PLcEydOpGbNmhgMBrp06UJsbCwRERF3FK+iWBtTEn0AcKrU4ySujNbL8gzwW/H3IUAa8L0QogWwE3hFSplz7UFCiOeB50FLDHoySiM7z+xkZfxK1iWu42LhRWo51+Kxpo/RJ6QPjWo20i222q61+b8u/8cTq5/gxfUvMilskm6xwI1nRly7/a233qJv37707t27ZNvatWtZu3YtrVq1AiA7O5ujR4/e1s9/4cKFzJgxg6KiIlJTUzl48KBK9IpyDVMSfVn/J5e5orgQ4l60RN+x1PlbAyOklNuEENOAscA7151QyhloJR+ioqJ0WbE8PjOeFfEr+DX+V1JzUnG2c6Zr/a70CelD2zptsbWx1SOs6zSo0YBp907jhXUvcC7vHEZpxEbY3PHIuzy8vb05f/78VdvOnTt33U0uDRo0oGXLlixcuLBkm5SSN998kxdeeOGqfW92oba0hIQEpk6dSkxMDDVq1GDYsGFq3rWilMGUmkMSULfU40Ag5dqdhBARaBdd+0kpM0odmySl3Fb8eBFa4q8y0nPTmXNwDoNXDKbfsn7M2j+LEK8QJt8zmb8G/8XEjhO52//uKpPkL2tTpw0f3fMRBYYCkrOTkVKXv424ubnh5+fH77//DmhJfvXq1XTs2PG6fceNG8fUqVNLHnfv3p1Zs2aRna3NNEpOTubs2bO4u7tz8eLFq45t3Pj66xEXLlzA1dUVT09Pzpw5w2+//XbdPoqimDaijwEaCiGCgWTgYeCR0jsIIeoBi4HHpZRHLm+XUp4WQpwSQjSSUh4GulCqtq+XS4WX+PPUn6yIX8HWlK0YpIGm3k0Z3WY0PYN74uNsGdPwegT1IOZCDBfyL3Da5jR1XOrocpPJ//73P/71r3/x+uuvA/Duu+8SGhp63X7NmjWjdevW7Nq1C4Bu3boRFxfH3XffDWh/NH788UdCQ0Pp0KED4eHh9OzZkzFjxpT5h6xFixa0atWKZs2aERISQocOHSrwXSqK5RKmjASFEL2Az9CmV86SUk4UQrwIIKX8WggxE3gIuNz4vkhKGVV8bEu0kb4DEA88JaW8+rP+NaKioqS5Fx4xGA1sP72dlfErWX9iPZeKLuHn6kfvkN70CelDqNf1ickSxMXF4VXPi3O556jtWtti/kjdjpUrVxIfH8/IkSP1DkWpZHFxcTRp0kTvMCyCEGLn5bx7LZPm0UspVwGrrtn2danvnwWevcGxe4AyX7wyHD53mJXxK1kVv4qzuWdxt3enZ3BPeof0JrJ2ZKXNmKlIdVzqUGQs4kzOGext7PF09NQ7JLPq06eP3iEoikWzyjtjT+ecZlXCKlbGr+To+aPYCTs6BnZkTMgYoutG42jrqHeIZiWEIMAtgCJjEcnZydgJO1wdXPUOS1GUKsJqEn2BoaAkuW9P3Y5EEuEbwbh24+ge1J0aTjX0DrFC2Qgb6rrXJTErkZMXTxLsGYyTnZPeYSmKUgVYTaIXCD7Z8QnuDu682OJF+oT0oZ6HvvPxK5udjR31POqRkJXAiQsnCPEMwd5WLdqgKNWd1SR6e1t75veZj7+rf7Vub+pg60B9j/pasr94gmCP4Co3NVRRlMpl+VciSwlwC6jWSf4yJzsn6rrXpcBQwKmLpzBKo94hKYqiI6tK9MoVbg5u+Lv5k1OYQ0p2SoXdUJWYmEh4ePhV2957772SG6OGDRtGQEAA+fn5AKSnpxMUFHTTc17uXnknOnfuzOUpukFBQaSnp9/R+aqSZ599loMHdb8d5bYkJiYyb968ksezZ8/m5Zdf1jGi6kUleivm5ehFLZdaZOVncebSGd3iuNym2FTmSPSWoqio6LaPmTlzJk2bNq2AaCrOtYleqVwq0Vs5H2cfajrVJCM3g4zcjFsfUAEutykuK6lNmTKFNm3aEBERwbvvvgtwVZvif//73zc99/Dhw4mKiqJZs2Ylx5dH6VH/jh076Ny5MwB///03LVu2pGXLlrRq1aqkNUNZcV/Lzc2N119/ndatW9OlSxfS0tIA7dPGW2+9RXR0NNOmTWPnzp1ER0cTGRlJ9+7dSU1NJS4ujrZt25acKzExsaRZW+lPKz/99BPNmzcnPDycMWPGXPXaly1atIhhw4YB8PPPPxMeHk6LFi3o1KnTdTH/9ddfREdHM3jwYMLCwhg7dixz586lbdu2NG/enOPHjwNw4sQJunTpQkREBF26dOHkyZOA9glu5MiRtG/fnpCQEBYtWgRoP9ONGzfSsmVLPv30UwBSUlLo0aMHDRs2ZPTo0ab+qJRysJqLsQrw21g4ve+qTQKog6SGIR+D0UCRnSN24jZ+7HWaQ8/Jt97vJiy5TfHUqVOZPn06HTp0IDs7GycnpxvGfW3izMnJoXXr1nzyySe8//77TJgwgS+//BLQPrX8/fffFBYWEh0dzbJly/D19WXBggWMGzeOWbNmUVBQQHx8PCEhISxYsIDBgwdfdf6UlBTGjBnDzp07qVGjBt26dWPp0qX079//hu/n/fffZ82aNQQEBJCZmVnmPnv37iUuLo6aNWsSEhLCs88+y/bt25k2bRpffPEFn332GS+//DJPPPEETz75JLNmzWLkyJEsXboUgNTUVDZt2sShQ4fo27cvAwcOZPLkyUydOpWVK1cCWulmz5497N69G0dHRxo1asSIESOoW7dumTEpd0aN6KsBgcDR1hEbGxvyDfkYpMF8576NNsVTpkzBaLxyYbh0m+LWrVtz6NAhjh49eluvv3DhQlq3bk2rVq04cOCA2WvXHTp0YNSoUXz++edkZmZiZ2dnctw2NjYMGTIEgMcee4xNmzaVPHd5++HDh9m/fz9du3alZcuWfPjhhyQlJQEwePDgkm6fCxYsKDnmspiYGDp37oyvry92dnY8+uijbNiw4ZbvZ9iwYXz77bcYDGX/HrRp0wY/Pz8cHR0JDQ2lW7duADRv3ryks+iWLVt45BGt5dXjjz9+1Xvr378/NjY2NG3alDNnblwy7NKlC56enjg5OdG0aVNOnDhxw32VO6NG9NbkJiNvAdgbi0jISsBgNBDsGYyj3Z3fIWwtbYrt7OxK/giVPsfYsWPp3bs3q1at4q677mL9+vU3jPtWSv/xc3XV7lyWUtKsWTO2bLl+dc0hQ4YwaNAgHnzwQYQQNGzY8Krnb3aBvfRrlX4/X3/9Ndu2bePXX3+lZcuW7NmzB29v76uOdXS88nthY2NT8tjGxuaG1xRKv17p428WY+n9bG1ty3W9QjGNGtFXI3Y2dtT3qI8QghMXTlBoKLzjc1pam+IuXbqQnJx83fagoCB27twJwC+//FKy/fjx4zRv3pwxY8YQFRXFoUOHbhj3tYxGY0mNet68eWX+N2nUqBFpaWklib6wsJADBw4AEBoaiq2tLR988MF1o3mAdu3a8ffff5Oeno7BYOCnn34iOjoagNq1axMXF4fRaGTJkiVXvZ927drx/vvv4+Pjw6lTp647rynat2/P/PnzAZg7d26Z7620sn6mSuVRI/pqxsHWgXoe9UpaJQR5BN3xDVWW0qbYaDRy7Ngxatased1z7777Ls888wwfffQR7dpdWUDts88+488//8TW1pamTZvSs2dPHB0dy4y7Vq1aV53T1dWVAwcOEBkZiaenJwsWLLjudR0cHFi0aBEjR44kKyuLoqIiXn31VZo10xaRGTJkCP/+979JSEi47lg/Pz8mTZrEvffei5SSXr160a9fPwAmT55Mnz59qFu3LuHh4SV/lP79739z9OhRpJR06dKFFi1a3PS/2Y18/vnnPP3000yZMgVfX1++//77m+4fERGBnZ0dLVq0YNiwYdSoYd0tSaoak9oUV7aKaFNsrcrbxvViwUVOXjiJq70r9TzqVekunuZqU7x//35mzZrFf//7XzNFdnNubm4lCVYpH9Wm2HR33KZYsT7uDu74u/mTkp1CSnZKlb6r2FxtisPDwystyStKVaISfTVWw6kGhcZC0i6lYW9jT23X2nqHZFXUaF6pKlSir+Z8nX0pMhaRnpuOvY09NZ2vr18rimLZVKKv5oQQ+Ln6UWQsIjUnFTsbOzwcPfQOS1EUM6q6V+CUSnN5hSpnO2eSspO4VHhJ75AUpVqqqE6zakSvAGBrY1uyaMnJiycJ9jDPDVWKolzvQsEFErMSSchKuPJ1QZtCu7z/crO/nkr0SomrVqi6eIJgz2DsbW6+QlViYiJ9+vRh//79Jdvee+893NzceOONNxg2bBjr1q0jPj4eR0dH0tPTiYqKuundr5mZmcybN4+XXnqpXO9j/PjxdOrUifvvv79cx6tpkYo5GKWR1JzUq5J54gUtuafnXmmbbSfsqOtRl2CPYEK9QpFSmn0GnEr0ylUcbR2p516PExdOcDzzOH6ufng4eNzRL97lNsXDhw83af/LbYrLm+jff//9ch2nKOVxqfASJy6cKEnil79OXDhBnuFK+wkPBw+CPYPpGNCRYM9ggj2CCfYMJsA94JYDqjulEr1yHRd7F4I9g0nOTibpYhLuDu74ufqVe/3Zy22Kn3vuueuemzJlCgsXLiQ/P58BAwYwYcKEq9oUd+3alSlTppR5XoPBwDPPPMOOHTsQQvD000/z2muvMWzYMPr06cPAgQMJCgriySefZMWKFRQWFvLzzz/TuHFj0tLSeOSRR8jIyKBNmzasXr2anTt34uPjc8v4lOpHSkl6bvpVZZbL36fmpJbsZyNs8Hf1J9gzmHZ+7bSEXvxVw7GGbveqqERvRf6z/T8cOnfIrOes71GfwY0GcyzzGHVc6+Dl6HXbv6wV1aZ4z549JCcnl5SNbtR218fHh127dvHVV18xdepUZs6cyYQJE7jvvvt48803Wb16NTNmzLjuOFPbESvW53zeeTYlb2Jr6lbiM+NJuJBATmFOyfMudi4EeQbRunbrkpF5sGcw9Tzq4Whb9a5tqUSv3JSznTOhXqEld9Bm5Wfh7+aPg60DcHttivv27Uvv3r1LtpVu9wvaDUZHjx6lXr16JsUWEhJCfHw8I0aMoHfv3iXtdK/14IMPAhAZGcnixYsB2LRpU0mzrx49epTZe+VG8alEb32klBzLPMbfSX+zIWkDe9P2YpRGajrVJKxGGH1D+14ZnXsEU8ulVpW9k7wsKtFbkTFtx9x6p3IK8gjifP55zuSc4VjmMWq51MLbyVvXNsU1atRg7969rFmzhunTp7Nw4cIylyy83A63dCtcU3o8lbcdsWIZ8g357Di9oyS5J2drXU2b1GzCCxEvEB0YTRPvJlW6D5SpVKJXTCKEoKZTTdzt3UnNSeVMzhku5F/A382/pE1xly5dStoUv/LKK9edY9y4cVeN6Lt3784777zDo48+ipubG8nJydjb29+wTfGhQ1eXpdLT03FwcOChhx4iNDS0ZLk8U3Ts2JGFCxcyZswY1q5de90fq5vFd22XSsVypOemsyFpA3+f+pstqVvILcrFydaJu/zv4tnmz3JPwD1W2QpEJXrlttjb2lPXvS4XCi6QmpNKfFY8n37zKePfGF/pbYqTk5N56qmnShYMmTRpksnv491332Xo0KEsWLCA6Oho/Pz8cHd3v2qfG8WnEr3lkFISdy5OG7Wf2sD+DO16Th3XOvQN7UunwE60rdMWJzsnnSOtWKpNcXkV5kHiJqjdDDz8dAtDzzauRcYiTuecJis/C0c7R/xd/XGxdzH765irTXFp+fn52NraYmdnx5YtWxg+fLhJa9Qqlas8v9+5RblsS91WktzP5p5FIGju25zowGiiA6MJqxFmUTV2U9xxm2IhRA9gGmALzJRSTr7m+UeBywXibGC4lHJvqedtgR1AspTSPD1n9ZRxHH5+8spC3H4tIKwHNOwO/q3AxvJreqaws7Ej0D0QT0dPUrJTSMhKwNvZG19n3ztezKQ0c7UpLu3kyZMMHjwYo9GIg4MD3377rdlfQ6k8p3NOayWZpL/ZlrqNfEM+rvautPdvT3RgNB0DOuLt7H3rE1mpWyb64iQ9HegKJAExQojlUsrSqzAnANFSyvNCiJ7ADKBdqedfAeIAy++WtX8xLB8JNrbQ7yvIPg1H1sKGKfD3f8DVFxp2g7DuEHIvOFn+W74Vdwd3Gng14MylM2TkZnCh4AL+rv64ObjpHdoNNWzYkN27d+sdhlJORmlkf/p+/jr1FxuSNnD4/GEAAt0CGRQ2iE6BnYiqHVXuez+sjSkj+rbAMSllPIAQYj7QDyhJ9FLKzaX23woEXn4ghAgEegMTgVFmiFkfhXmwdhzEzITANjDwe/Cqqz13z+tw6RwcWw9HVsOhlbBnLtjYQ/32WtIP6wHe19etzaEibpm+XbY2tvi7+ZeM7k9cOEENpxrUdqlt1tG9Un2UVVbedWYXi48uZmPyRs7lncNW2NKyVktGRY4ium40wR7Buv+/UBWZkugDgNIrCCdx9Wj9Ws8ApVdp/gwYDbiXuXcxIcTzwPOAyfOoK03Gcfh5GJyOhfYjoMu7cO1IwaUmRAzWvgxFcGqblvSProU1b2lf3g208k5Yd6h3N9g53HFoTk5OZGRk4O3tXSV+wV3tXQn1CiXtUhrpuelcLLiotVFQrY+V2yClJCMjAyenKxdJ0y6l8ezaZ3Gyc6JjQEc6B3amQ0AHPB09dYzUMpiS6MvKHmVewRVC3IuW6DsWP+4DnJVS7hRCdL7Zi0gpZ6CVfIiKiqo6V4hLl2qGzodGPW99jK0dBHXQvrp9AOcTtfLOkdUQ8y1snQ4O7tDgPi3xN+wGbr7lCi8wMJCkpCTS0tLKdXxFMhgMZORnkGRMwtnOGQ9HD2yFGt0rpnFyciIwsKQ4wLxD8zBIAwt6L6CuR10dI7M8piT6JKD0f9VAIOXanYQQEcBMoKeUMqN4cwegrxCiF+AEeAghfpRSPnZnYVeC60o1s8CrnJ80agRBu+e1r/xsSPgbjqzRvg4uAwQEtNbKO2HdoU4EmDg6t7e3v+7mpKqk0FjI9/u/5+u9X+Ns58yYtmN4IOSBKvHpQ7EcOYU5LDi8gC71uqgkXw63nF4phLADjgBdgGQgBnhESnmg1D71gD+AJ66p15c+T2fgDVNm3eg+vdKUUo05SAmpe7XyzpHVkLwLkODuV+qCbmdwcDX/a1ey+Mx43t38LnvS9tAhoAPj7xqPv5u/3mEpFmLOwTl8HPMxc3vNJcI3Qu9wqqSbTa80aR598Yj8M7TplbOklBOFEC8CSCm/FkLMBB4CThQfUnTtC1pMoj+wBJaN0Eo1A742rVRjLtln4eg6OLoGjv0BBRfB1hGCOmqj/aZ9wb1O5cVjZkZpZP6h+Xy26zMEglcjX2VIoyFWcYu5UnEKjYX0XtwbP1c/fuj5g97hVFl3nOgrmy6JvnSpJiAKBn1f/lKNORQVwMktWnnn6BrIOAZuteHlGHCy7ItPKdkpvL/lff5J+YdWtVrxXvv3CPEM0TsspYr6Nf5Xxm4cyxf3fUHnup31DqfKUon+VkqXau5+WSvVmDAjplKnNSb+A7N7w93/gu4TK+c1K5CUkuXHl/NxzMfkFuXyfMTz9A3tq8o5ylWklAxZOYQ8Qx5L+y1Vn/5u4o7vjLVqpUs1D/8EjXvd8pC8QgOfrD3MD1tO0Ki2Ox0b+nBPAx8ig2rgaFdBs0qCOkCrx2Db1xA5DHwaVszrVBIhBP0a9KNDQAc+2vYR0/dMZ/qe6QS4BRBZO5I2ddoQVTuKALcAdeG2Gtt2ehtx5+KY0H6CSvJ3oPqO6AvzYO3b2nTH2yjV7EvKYtTCPRw9m03v5n6cvZjH7pOZFBklTvY2tA325p4GPnRo4EMTP3fzJqnss/B5a+0mrEcX3np/C3Lk/BFiTsew4/QOdpzZQWZ+JqA1n2pTuw1RdaJoU7sNge6BKvFXIy+ue5FD5w6xZuCaKrmgR1WiRvTXOhevlWpS95pcqik0GPnqz+N88cdRvN0c+OHptkSHaXPfs/OL2Ho8g03H0tl0LJ2Jq+IA8HFzoEMDHzo28OGehr7U8bzDDnlutSB6NKx7R7to27DrnZ2vCgmrEUZYjTAebfIoRmnkeOZxdpzZQczpGP5J+YcV8SsAqOVSi6jaUSUj/voe9VXit1KHzx3mn5R/GNlqpEryd6j6jegPLNFugBI20P//TCrVHDt7kVEL9xKblEX/lv5M6BuOp8uNp1umZuWy6aiW9P85lk56dgEADWq50bE48d8V6o2bYzn+zhYVwFd3afG/tKVipn1WMVJK4rPiS0b7MadjyMjTbtXwdfYlqnYUUXW0L3ULvPV4a+NbrD+5nnUD16m7X02gLsZCuUo1RqPk+82JfLz6EC4Otkwc0JxezW+vJbHRKDl85iKbjqaz8Vg62xMyyCs0YmcjaFXPi44NfOnY0IcWgZ7Y2ZpYgzyyBuYNhu6T4O6XbiseayClJPFColbqObODnad3cjb3LADeTt5X1fhDvUJV4rdAp3NO0/OXnjzc+OEKXTnNmqhEX45Szalzl/j3or1sjT9Hl8a1mPRQc2q53/niBHmFBnadOM/G4tH+vuQspAR3RzvuCvXmnobaiD/Yx/XGCUpKmDsQTsXAyF3g6nPHcVkyKSUnL55kx+kdxJzR6vxnLp0BoIZjDaLqRJUk/wZeDaz6ot7us7sZ/894Pu70MU289VmnwBw+2fEJcw7OYdWDq9RMLBNV70R/m6UaKSU/70ji/ZVac87xfZoyKKriLgCezylg8/EMNh1LY+PRdJLO5wIQ4OVMxwY+dGjoQ4dQb7zdrqlRph2G/2sPrR6HBz6rkNgslZSSpOykq0o9qTmpAHg6ehJZK5K+oX3pUr+LzpGaV6GhkEErBnE86zitarXihx4/WOSnmYsFF+m6qCudAjvxcaeP9Q7HYlTPi7FF+bBm3G2Vas5ezOOtxftYH3eWdsE1mTqoBXVrmn/FpNJquDrQO8KP3hF+SCk5kXFJG+0fTee3/aks2KE1Dm3m78HDbery+N1B2oG+jaDt87D1/yDqafBTt4VfJoSgrntd6rrXZUDDAQAkZydrI/7TMWw7vY2/kv7ip94/0dS7qc7Rms+cuDkczzpOj6AerE5czZrENfQI7qF3WLdt0ZFF5BTmMKzZML1DsRrWOaIvR6lm1b5Uxi3ZR06BgTE9GvNU+yBsbPQdDRUZjOxLzmLT0XTWxZ0hNimL/z3dlk7Fs33IPQ9fRIJvYxj2q8mN0Kq7CwUX6Le0H77OvszrPQ87G8sf76Rkp9B/WX/u9rub/3b+L0NWDuFCwQWW919uUeuhFhoK6bG4B8EewczsPlPvcCzKzUb01lesPLAUvonWWgM/PE+7i/QmST7rUiGvzt/NS3N3UbemC6tGduSZjsG6J3kAO1sbWtWrwYguDVn4wt2E+roy9pdYLuYVajs414D73oYT/8DBpbrGakk8HDwY23YscefimBs3V+9wzOI/2/8DwNi2Y7G1sWVM2zGk5qTywwHL6g3zW+JvnL10lmHhw/QOxapYT6Ivyodf39DWcvUJgxc2QuPeNz1kw5E0un+2gZWxqbx6f0N+Gd6eBrVuuj6KbpzsbZkyqAWnL+TxUfE8fQBaPwm1w2HtO1CYq1+AFqZb/W5EB0Yzfc90krOT9Q7njvx96m/+OPUHL7Z4ET83bVZYmzpt6Fq/K9/t/44zOWd0jtA0UkpmH5hNwxoN6eDfQe9wrIr1JHpDISRs0Eo1T/0GNerfcNdLBUW8vXQfT8zajpuTHUte6sCr94dhb+r0Rp20rleD5+4J4aftp9hwpHihERtb6DEZsk7B5i/0DdCCCCEY124cAB9u/bDMZessQW5RLpO2TyLUM5THmzx+1XOvRb5GkbGIabum6RTd7fkn5R+Onj/KsGbDLPIiclVWtTPb7XB0g+f/vGWpZkfiOXpO28jcbSd5tmMwK0d0pHmg5dyM8VrXsOtLOMH3QNN+sPG/kJWkb4AWxM/NjxGtRrApeRNrEtfoHU65fBv7LcnZybx919vXLYRd170uTzR9ghXxK4hNi9UpQtPN3j+bWi616BlUia3BqwnrSfRw0wU68osMTP7tEIO/2YLBKPnpubt4u09TnOwta2m7q0s4h6480fUDkEZY/55usVmiRxo/QjPvZkzaPoms/Cy9w7ktCVkJfH/ge/qG9iWqTpnX4Hgu4jl8nH34T8x/qvSnloMZB9l2ehuPNXnsuj9Yyp2zrkR/AwdTLtDvy3/4+u/jDI6qy+pXO3FXiLfeYZVb63o1ePaeEH7afpKNR4tLODXqQ4eRsO9nOLlV3wAtiK2NLe/e/S5Z+Vl8uvNTvcMxmZSSiVsn4mznzKjIUTfcz9XelZGtRhKbFsuvCb9WYoS3Z/b+2bjauzIwbKDeoVglq070RQYj0/88Rr/pm8jIKWDWsCgmPxRRvh4zVcyormGE+Loy9pd9V0o4HV8Dd3/4bQwYjfoGaEGaeDfh8aaP88vRX9hxWsclLG/Dbwm/se30Nl5t/SrezjcftPRr0I8mNZvw6c5PuVR4qZIiNF1ydjJrT6xlUNgg3B2q5mSICpV3QRucxczUyq8VwGoTfUJ6DoO+2cKUNYfp1rQOa1/txH2Na+sdltk42dsyZWALUrNyr5RwHFyh6wRI3QN75+kan6UZ3mI4AW4BTNgygQJDgd7h3NTFgotM2TGFcO9wHmr40C33txE2jGk7hrOXzjL7wOyKD/A2/XjwRwSCR5s8qncoFctQBGlHYP9i+P0D+GkofNYcJteFWd3h19dh+wytxYmZWf7Q9hpGo2TO1hNM+i0ORztbpj3ckr4t/K3yKn5kfa2EM2NDPL2b+9GxoQ80HwTbv4X1E6BJX3Dy0DtMi+Bi78Lbd73N8PXD+W7fdwxvOVzvkG7oy91fci7vHF92+RJbG9OuMUXWjqR7UHe+3/89DzZ8kDquVWPt4az8LH45+gs9g3tWmZjMIicdzuyHMwfgzEHt+7RDUJSnPS9stcWDAttoCwnVDofazcAjoEJufLSqRJ+SmcvoRbFsOpZOdJgvHw+MoLaH5dwVWB6juoaxPu4MY36JZfWr9+DuZA89J8O398HGqdD1fb1DtBgdAzrSM7gn3+77lu5B3Qnxqnrr2B7MOMj8w/MZ0mgIzbyb3daxoyJH8depv/jvzv9WmR4yCw8vJLcolyebPal3KOVTlK/1nTpzQEvmZw9q32eXunfBtZaWxNs8eyWh+4SBfeXlJqtpgZB5qYDoKX9RaDAyrncTHmlbzypH8WXZeeI8A7/ezNC29fhoQHNt49KXIHYh/GsbeIfqG6AFSc9Np9/SfjTwasD3Pb6vUp0ujdLIY6seIyU7heUDluPhcPuf1r7Y/QUzYmcwp+ccWtZqaf4gb0O+IZ/ui7rTuGZjvu76ta6x3JKUcCH5SkI/U5zQ04+ANGj72DpCrcZXknntZlCrGbj5VkqI1aKpmZeLA6N7NKJjAx/qe994mqU1iqxfg2c7BvPtxgR6hReXcLqMh4PLtB78Q3/SO0SL4ePswxtRbzB+83iWHF3CQ2G3roFXlkVHFrEvfR+T7plUriQP8Ez4Myw9upTJ2yczr/c8Xf+Q/Rr/Kxl5GVW33UFOOmz8ROuZdWY/5JWafutZT0vkjXsXJ/VwqBkCtlUzpVrNiL66yys00GvaRvKLjKx5rZM2s2jTp9q8+scWQwPraslbkaSUPL3maQ6fP8zy/svxcda/339GbgZ9l/alcc3GzOw2844+rS4/vpxxm8YxseNE+ob2NWOUpjNKI/2X9cfJ1okFfRZUvU/f5+Lhx4e0GxD9W10ZodcOh1pNwKnq3WRZvZqaVVPajVQRpGTlMulyL5y7XoIawbDmLa1FhGISIQTj7x5PXlFeSbMwvX2681MuFV1iXLtxd5wU+4T0oblPcz7b+Zlu0y03JG0gISuBJ5s9WfWSfMpu+K6b1h122K/wzFro86lWY693V5VM8reiEr0Viaxfk2c7BjN320n+OZYOdo7Q/SPtav+OWXqHZ1GCPYN5LuI5VieuZkPSBl1j2XlmJ8uOL2NYs2FmuUBsI2wY3WY0ablpzNynTyvg7/d/j5+rH92Cuuny+jd0bD183xvsnOHptVC3rd4RmYVK9Fbm9W6NCPFxZfSiWLLzi6BRTwi5F/6cCDkZeodnUZ4Jf4YQzxAmbp2o28i30FjIh1s/xN/Vn+cjnjfbeVvWakmv4F78cOCHSu/eGZsWy66zu3i86ePY21Shdgd7foJ5Q7Ra+zNrwTdM74jMRiV6K3NdCUcI6DEJ8rPhr4/0Ds+iONg68O7d75KSk8L0PdN1iWHuwbkcyzzGm+3exNnO2aznfi3yNWyEDf/dUTF3Y97I7AOzcXdwN+lmr0ohpXZH6tIXoX4HeGoVePhV2ssXGowcO5vN2gOn+WVnxTQlrJqXiJU7Elm/Js90CGbmpgR6NfejQ4MmWn0x5ltt2cHatzf/ujprXbs1A8MG8mPcj/QO6V2pSw+ezjnNV3u/onPdznSu29ns56/jWoenw5/mq71fseP0jhs2RjOnUxdO8fvJ33k6/Glc7Ct2mU6TGA2weqx2R2r4QG1d6VusRlceUkrSswuIT8smPj1H+zcth/j0HE6eu4TBqE2KcXey48HWAWa/bqFm3Vip62bhGC7AF621WQNPrlDLDt4GvZYefO3P19iUvIml/ZcS4BZQIa+RW5TLA0seoKZTTX7q/ZPJd9qW14dbP2Tx0cWseWgNvi6VM7/8hgrzYPFzELdcW8ei6wdgc2dFjrxCA4kZOVoSL07mx4sT+8W8opL9HOxsCPZ2JcS3+MvHrfh7Nzydy1fOqhbz6JWrOdnb8vHACAZ9s4XJv8XxYf/mcO84WPUGHFoJTR7QO0SLcXnpwTf+foO5cXMr5S7ODUkbWH9yPa+0fqXCkjyAs50zr0W+xtiNY1l+fHnJYuoV4XzeeZYdW0afkD76J/nc8zD/UW0Zzm4Tof3LJh8qpSQ1K694RF6czNOySUjPITkz96pWNX6eToT4utK/ZUBJIg/xccXfyxnbSlyu1KREL4ToAUwDbIGZUsrJ1zz/KDCm+GE2MFxKuVcIURf4H1AHMAIzpJSWsdyNFYgKKlXCCfejfeRT2uybNeOgQddKvQXb0nWr341OgZ2Yvmc699e/v0KTb15RHpO2TSLYM5gnm1b8H5Vewb346dBPTNs1ja71u+Lm4FYhrzP/0HzyDHkMazasQs5vsqwk+HEgZByDh76D5mW3RpZScvjMRQ6fvsjxUiP0hPQccgsNJfu5ONgS4utK63o1GBgZWJLMg31cca0inXJvWboRQtgCR4CuQBIQAwyVUh4stU97IE5KeV4I0RN4T0rZTgjhB/hJKXcJIdyBnUD/0seWRZVuzCe3wECvzzdScLmEk7wJ/tdPu3P2ntf1Ds+ipGSn0H9ZfyJrR/JVl68qbP73l7u/5JvYb/iu23e09auc6X370/cz9NehPB3+NK9Fvmb28+cW5dJ9UXcifCP4ssuXZj+/yc4c1G6Eyr8ID8+FkOgyd8svMjB+6QEW7DgFaJXOwBrOV5VYQn20f2t7OFaJewHutHTTFjgmpYwvPtl8oB9QkqyllJtL7b8VCCzengqkFn9/UQgRBwSUPlapWM4Otky5qoTTGRr3gQ2fQItHKnV2gaXzd/NnRKsRfBzzMWsS19AjuIfZXyMxK5FZ+2fRJ6RPpSV5gHCfcPqG9mXOwTkMbDiQuh51zXr+FcdXcD7/vL6j+cR/YP7Q4jnyv0Gd5mXudjorjxd/3MmeU5kM7xxK/5YB1Pd2sbjV6Eoz5cpDAHCq1OOk4m038gzw27UbhRBBQCtgW1kHCSGeF0LsEELsSEtLMyEsxVRRQTV5ukMwP249yeZj6dDtQzAWqmUHy+GRxo/Q1Lspk7dPNvvSg1JKJm6biJOtE69HVf6nrZGtRmJnY8cnOz8x63kNRgM/HPiBcO9wImtHmvXcJju4DOYM0DpJPrvuhkk+JvEcfb7YxNEzF/n6sdaM6dGYRnXcLTrJg2mJvqzPJGXWe4QQ96Il+jHXbHcDfgFelVJeKOtYKeUMKWWUlDLK11fnCzVW6I1ujQj2cWX0L7HkuNbVZhnEzodTMXqHZlFsbWx57+73yMzPNPvSg2sS17A1dSsjWo/Qpb9ObdfaPBP+DL+f/J3tqdvNdt4/T/3JyYsnGRY+TJ8Sx7YZsPBJ8Guh3QjlVe+6XaSUzNmSyNAZW3F3smPpvzrQI9x6Pu2akuiTgNKf4wKBlGt3EkJEADOBflLKjFLb7dGS/Fwp5eI7C1cpL2cHbRZOcmYuk387BPeMArc6sFotO3i7Si89uPPMTrOcM7sgm49jPqapd1MGhw02yznL48lmT+Lv6s/HMR9jMBpufcAtSCn5/sD3BLoFcn+9+80Q4W29uPap9bd/a3eIP7EMXGpet1teoYExv8TyzrIDdArzZem/OtCwtnUtaWhKoo8BGgohgoUQDsDDwPLSOwgh6gGLgcellEdKbRfAd2gXaiv39jvlOm2CavJU+2DmbD3B5qR8uP89SN4JsQv0Ds3iDG8xHH9Xf7MtPTh9z3TSc9N55653Knwu+8042TnxWtRrHD5/mMXH7nxctvvsbmLTYnmi2ROV+74MhbB0uNbBNXIYDJ4DDtffoJWalcuQGVtZuCOJkfc1YOYTUeWex16V3TLRSymLgJeBNUAcsFBKeUAI8aIQ4sXi3cYD3sBXQog9QojLU2Y6AI8D9xVv3yOE6GX+t6GY6t/dGxHk7cLoRbHkNH4IAiK1UU/+Rb1DsyiXlx5MyErgu33f3dG5Dp07xLxD8xjcaDDhPuFmirD8utfvTutarfly95dcLLiz34vZB2bj5ehF/wb9zROcKfIvaj1r9v6k3TvS57My+8RvTzjHA19s4tiZi3z9WCSjujXCphLntlcmk24Dk1KuklKGSSlDpZQTi7d9LaX8uvj7Z6WUNaSULYu/ooq3b5JSCillRKnnVlXc21FuxdnBlimDWpCcmct/1hyBHv+B7NMVtvq8Nbsn8J6SpQfjs+LLdQ6jNPLB1g/wcvRiRKsRZo6wfIQQjG47mvN55/lm7zflPk9CVgJ/nfqLIY2GmL1Pzw1ln4XZfSD+L3jgc4gefd1d4Jfr8Y98uxUPJ/vierwVrVdbBtXUrBq6XML535YTbC4IhoiHYcuXcC5B79Aszug2o3Gyc2LC5gkY5e1f61h8dDGxabG8EfUGno5Vp895M+9m9GvQj7mH5nLiwolyneOHAz/gYOvA0MZDzRzdDWQch++6amu4PjwPIq+/2Syv0MDoRVo9PjrMl6UvW189viwq0VdTl0s4Y36J5VKncWBjry07qNyWy0sP7jq7iyVHl9zWsefyzvHpzk+Jqh1Fn5A+FRRh+b3S+hUcbByYGjP1to9Nz01nxfEV9A3ti7ezdwVEd42kndpiIXkXYNhKaHT9PQ6pWbkM+WYLP+/U6vHfPhGFh5P11ePLohJ9NaXNwmlB0vlcJv9zQZuFc2il9pFXuS0DGgwgqnYUn+z8hPTcdJOPu7zC09t3vV0l7qy8lo+zD89FPMdfSX+xOWXzrQ8oZV7cPAqNhZXSF4ij6+CHPtrF1mfWQeD1N4derscfT8vhm8etux5fFpXoq7G2wTUZ1j6I/205wdY6j2jzi1e/CYaiWx+slCi99ODH2z826ZjdZ3ez5NgSnmj2BKFeoRUcYfk93vRxAtwCmBIzhSKjab8XlwovseDwAu6rdx/1PepXbIC752oXXr0bwDPrwafBVU9LKfnfVfX49nRvZt31+LKoRF/Nje7emPreLvx76SHy7nsfzh6End/rHZbFubz04G+Jv91y6cFCYyHvb3kfP1c/Xoh4oZIiLB9HW0feiHqDY5nHWHRkkUnHLDm2hAsFFyq23YGUsGEKLHsJgu/R1nZ1r33VLpfr8eNL1eMb1LL+enxZVKKv5rReOFoJZ1J8AwjupC07eOmc3qFZHFOXHpwXN49jmccY23Zs1Vh84xa61OtCVO0opu+Zfsu2D0XGIuYcnENL35a0rNWyYgIyGuDX1+GPD6H5YHjkZ3DyuGqXlMxcBl+ux3dpWK3q8WVRiV4pKeH8sPUke5uNhbws+GvyrQ9UrlJ66cGv9nxV5j6nc07z1Z6viA6M5t6691ZyhOUjhGBM2zFk5Wfx9d6vb7rv+hPrSc5OZlj4sIoJpjAXFj4BO76D9iNhwDfXrQi1LT6Dvl9uIj4thxmPRzKqa1i1qseXRSV6BdBm4dT3duHlP/IpbDUMYmbC2Ti9w7I4l5cenBM3h4MZ1zdp/TjmY4zSyNi2Y6vkBdgbaVyzMQ82fJD5h+bf8J6By+0OgjyCKuaPmJQw/xE49Cv0mAzdrl4RSkrJD5sTeXTmNjyctfnx3aphPb4sKtErALg42JWUcD4pHAiO7rDiVe1jsnJbXm39KjUca/De5veuuoC5KXkT606s4/mI5wl0D9QxwvIZ0WoETnZON5xuGXM6hoMZB3mi2RPYiApILaf3wfE/tNYddw2/6qm8QgNv/BzLu8sP0LmR1q+mQa2KWUDFEqlEr5RoG1yTJ+8O4uuYTI5Fvg2ntmqLJiu3xdPRk7HtxhJ3Lo55cfMAbdWoj7Z9RJBHUOVMOawA3s7evBDxAhuTN7IpedN1z88+MJuaTjV5IKSClqmMXaDd79H6ias2X67H/7IriVfvb8iMx6t3Pb4sKtErVxndQyvhPL0rFEODbrB+gnbHoXJbutfvTqfATny550uSs5OZtX8Wpy6e4u273sbB1uHWJ6iiHmnyCHXd6/JxzMcUGgtLth87f4yNyRsZ2ngoTnYVsESl0QD7FkHDbld1oNwan8EDX2j1+G+fiOLV+1U9viwq0StXcXGw4+OHIjh5PpepDsPB1gGWvaxaGd8mIQTj2o0DYMyGMXy37zt6BfeinV87nSO7Mw62DrwR9QYJWQksPLywZPvsA7NxtnPm4UYPV8wLJ/yt9WSK0Fo4Syn5/p8EHp25DU8XrR7ftWntW5yk+lKJXrlOuxBvnrsnmP/blcu+iLFwcjPEfKt3WBbH382fl1u+zN60vSUJ0hrcW/de2vm146s9X5GZl8mZnDP8mvAr/Rv0x8vJq2JeNHYhOHpCWA/yCg28/vNeJqw4yL2Naql6vAmqxhLlSpXz7+6N2Rp/jsdiQtkWdB9O69+Dhl2hZojeoVmUR5o8wtHMo3QM6Iivi3WsnCaEYHSb0QxaMYjpe6bjbO+MURp5vOnjFfOCBTkQtwLCHyQlR/LCnC3sS87itfvDGHFfA1WqMYEa0StlcrCz4fOhrSg0Sl699BTSxg6WjVAlnNtkZ2PHBx0+oHtQd71DMauwGmEMChvEz0d+ZsGhBdxf737qupt3QfESh1ZBQTZF4YMZMmMLiek5zHwiilfub6iSvIlUolduKNjHlQ/6hbP6pC2/13sFTmzSblRRFOClli/hYufCpaJLPBX+VMW9UOwC8KzL5sIwTp3LZfJDEdyv6vG3RZVulJt6sHUAG4+m8fxeye6QaDzXvQsN7oeawXqHpuisplNNxrcfz8GMgxW3Mlb2WW3ufIdXWLo3FXcnO7o0qVUxr2XF1IheuSkhBB/0DyewhivD0h9DChtYrko4iqZHUA9GRY6quBfY/wtIA3lNBrJm/2l6N/fDyV6/NXUtlUr0yi25O9nz+dBW7Lvoxo9eL0DiRtg5S++wlOogdgHUiWBteg1yCgz0axmgd0QWSSV6xSQt63rxRvdGvHOyFak+7WHteDhfviXmFMUkaUcgZTdEDGHp7mT8PJ1oF1zz1scp11GJXjHZ8/eE0LGBL4+ceQQDAparG6mUChS7AIQN50P7suFIGn1b+qtZNuWkEr1iMhsbwX8Ht+CCQx2+sBsGCRvUIiVKxTAaYd9CCOnMyngjRUbJgFaqbFNeKtErt6WWhxNTB7Xgs/N3c9y9DaxTJRylApzaBpknIWIIS3Yn07iOO43reNz6OKVMKtErt+3exrV4pmMIT6Q9RpFRarNwpNQ7LMWaxM4HexdO1erCrpOZ9Fej+TuiEr1SLqN7NMLLP5TJhke1hlM7Z+sdkmItivLhwBJo3IclBzMRAvq28Nc7KoumEr1SLo52tnwxtBXzDPex36Elcu3b2kdtRblTR9dCXhYyYjBL9yTTNqgm/l7Oekdl0VSiV8otxNeN9/qG8+LFpygqMsDykaqEo9y5vfPBtRb7HFsRn5ajLsKagUr0yh0ZFBlIq4gWvF8wFOL/hF3/0zskxZJdOqeN6JsPZOneszjY2tCzuZ/eUVk8leiVOyKEYOKAcP5y681Om3Dkmrcg85TeYSmW6uAyMBRQFD6I5XtTuK9xLTyd1bKAd0oleuWOeTjZM+2RSEblPUdBYRFyxSuqhKOUT+wC8GnE5pxA0rPz6d9KXYQ1B5MSvRCihxDisBDimBBibBnPPyqEiC3+2iyEaGHqsYp1aF2vBoO7duTDgocRx3+H3T/qHZJiac4nwsktEDGYpXtS8HCyo3Mj1anSHG6Z6IUQtsB0oCfQFBgqhGh6zW4JQLSUMgL4AJhxG8cqVuLF6FDi6w9hu2yKYfWbkJWsd0iKJdn3MwC5jR9kzYHT9FKdKs3GlBF9W+CYlDJeSlkAzAf6ld5BSrlZSnm++OFWINDUYxXrYWsj+O/DrfnAZjiFBQUY1CwcxVRSwt4FUL8Da1McySkwqJukzMiURB8AlL66llS87UaeAX673WOFEM8LIXYIIXakpaWZEJZSFdX2cOLVwd2ZVPgwtsfXw555eoekWIKU3ZBxFCIGs2xPCv6eTrQNUp0qzcWURF9Wu7gyh2lCiHvREv2Y2z1WSjlDShklpYzy9bWORZSrqy5NamPT9jm2GRtTuGoMXEjROySlqotdCLYOnKvfs7hTZYDqVGlGpiT6JKD0qr+BwHX/5wohIoCZQD8pZcbtHKtYnzG9mvKN1yiKCvPJX6J64Sg3YSiC/YsgrAcrj+ZSZJRqto2ZmZLoY4CGQohgIYQD8DCwvPQOQoh6wGLgcSnlkds5VrFOTva2vPVYbz41DsUxYT3GPT/pHZJSVcX/CTlpJQuMqE6V5nfLRC+lLAJeBtYAccBCKeUBIcSLQogXi3cbD3gDXwkh9gghdtzs2Ap4H0oV1KCWG6F9RrHd2IjCX0fDhVS9Q1KqotgF4OTFCe/2qlNlBbEzZScp5Spg1TXbvi71/bPAs6Yeq1Qfg9vUZ8LBt4lIeJqsn/+F59O/gFC1V6VY/kWIWwktHmbZvgzVqbKCqDtjlQolhOC1IT2ZYf8Ynqd+J3enmoWjlBK3EopykcVlm3bBqlNlRVCJXqlwns72dHh0HDuNYRhXjUGqEo5yWewC8KrHPpvGxKfn0L+lKttUBJXolUoRGezLwTYfYWvII3XucDULR9Gu2ST8rS0XuCdFdaqsQCrRK5Xmkd7387PHMPzP/MmZzaoXTrW3fxFII0XNBrFib6rqVFmBVKJXKo2tjeD+p99jLw1xXv8mBZmqhFOtxS4A/9b8k1WzuFOlKttUFJXolUrlV8ONi92n4WjMI2H286qEU12dOQin90HEEJbtTsbDyY57G6s74iuKSvRKpet4dwf+CniORpkbOLjue73DUfQQuwCELZca9WP1gdP0jvDD0U51qqwoKtEruoh+cgJxto3w3zyetNNqUfFqxWjUWhI36MK6E0YuFRjop2bbVCiV6BVdODk64Dzoa5xlHok/DMdoMOodklJZTvwDF5JLWh6oTpUVTyV6RTdBjVtzsNG/aJO7iT8Wf6N3OEpliZ0PDm5kBHZhw9F01amyEqhEr+iq5ZC3SXRsROv9Ezl87Kje4SgVrTAXDi6HJn359VAWBqNkgJptU+FUold0JWztqfnodziLAormP4mhMF/vkJSKdGQ15F+AiMEsKe5U2aiOu95RWT2V6BXdedRrzsE2E2lWdIDDP4zUOxylIu1dAO5+nPCIZPfJTDWaryQq0StVQutez7La/SGaJs3n3OYf9A5HqQg5GXBsHTQfyNK9Z7ROlS1Vp8rKoBK9UiUIIQgf9hnbZFPc1r2BTNmtd0iKuR1YDMYiZPPBLNuTzF3B3vh5qk6VlUEleqXKCPT24GinL0g3upH74yPaCFCxHrELoFYzYovqaZ0q1XKBlUYleqVKebhza6Z6vYPtpTQKFz6lrSeqWL6M45AUAxGDWbonGQdbG3qEq06VlUUleqVKsbO14enBDzG+6CnsT/wNf7yvd0iKOez7GRAUNXuIFXtT6NJEdaqsTCrRK1VOeIAnXu2f5seiLvDPNDiwRO+QlDshJeydD8H38E+aE+nZBarlQSVTiV6pkl65vyHfub/AQZtGyKX/grNxeoeklFfSDjifUNLyQHWqrHwq0StVkouDHe8NaM2wSyO5hCPMfxRyM/UOSymP2AVg58Sl0F6sUZ0qdaESvVJlRYf50r5lM57JHYHMPAFLXtA6HyqWo6gA9v8CjXqyLj6XSwUGtS6sDlSiV6q0t/s05ZBDOLNcn9Nun9/wsd4hKbfj+O+Qew4iHi7pVNlGdaqsdCrRK1Waj5sj43o14YO0jsQH9IW/JsHh1XqHpZgqdgE41yS9Tkc2HE2nXyvVqVIPKtErVd7AyEDuDvFhcPJgCmtFwOLntXnZStWWlwWHVkH4Q/x6IB2DUaqyjU5UoleqPCEEHz3YnAtFdrzv8ibY2GoXZ/Oz9Q5NuZmDy8GQDy0eZumeZJr4eahOlTpRiV6xCME+rrzSpSFzDkl2tf0E0g/D8pfV4uJVWewCqBlComNjdp/MpL9qYKYblegVi/HcPSE0qu3Oy1s9ye/8jnYj1ebP9Q5LKUtWEiRugoghLNubqjpV6kwlesViONjZ8NGDzUm9kMfkrG7QtD+sfw+O/6l3aMq19v0MSGRzrbeN6lSpL5XoFYsSWb8Gj99Vn9lbThAb9RH4NIJFT8P5E3qHplwmpbbASGBbYi/VJEF1qtSdSYleCNFDCHFYCHFMCDG2jOcbCyG2CCHyhRBvXPPca0KIA0KI/UKIn4QQTuYKXqme/t29EbXdnRi9/DiFg+aA0QALHtPWI1X0d3ofpMWVLBfoYKc6VertloleCGELTAd6Ak2BoUKIptfsdg4YCUy95tiA4u1RUspwwBZ42AxxK9WYu5M97/drxqHTF5l50AYenAGnY2Hla+ribFUQuwBs7ChqMoCVsSl0aaw6VerNlBF9W+CYlDJeSlkAzAf6ld5BSnlWShkDFJZxvB3gLISwA1yAlDuMWVHo1qwOPZrV4bP1Rzjhcw9Ej4W9P0HMTL1Dq96MBti3CBp2Y1OKkfTsAvqrdWF1Z0qiDwBOlXqcVLztlqSUyWij/JNAKpAlpVxb1r5CiOeFEDuEEDvS0tJMOb1Szb3XtxkOtjaMW7IfGT0awnrA6rFwYoveoVVfCX9D9mmIGMyyPSl4ONnRuZHqVKk3UxJ9Wfcrm/T5WAhRA230Hwz4A65CiMfK2ldKOUNKGSWljPL1Vb8Yyq3V8XRidM/GbDqWzpI9qTDgG/CqDz8/CRdS9Q6veopdCI4eXAruWtyp0l91qqwCTEn0SUDdUo8DMb38cj+QIKVMk1IWAouB9rcXoqLc2KNt6xFZvwYfrDzIOaMLPDxXu2N24RNa50Sl8hTkQNwKaNqPdUeyijtVqtk2VYEpiT4GaCiECBZCOKBdTF1u4vlPAncJIVyEEALoAqgVJBSzsbERTHqwOdn5RXz460Go1QT6T4ek7bB6jN7hVS+HVkFBNkQMYcnuZAK8nFWnyiriloleSlkEvAysQUvSC6WUB4QQLwohXgQQQtQRQiQBo4C3hRBJQggPKeU2YBGwC9hX/HozKui9KNVUWG13XowOZfGuZDYdTYdmA6D9SNgxC3bN0Tu86iN2AXgEku4Txcaj6fRt6a86VVYRQlbB6WhRUVFyx44deoehWJC8QgM9p23EYJSsebUTzrYSfnwQTm6Fp1dDQGu9Q7Ru2Wfhk8bQYSQ/uAzj3eUHWPtaJ8JqqyZmlUUIsVNKGVXWc+rOWMUqONnb8tGA5pw8d4nP/zgKtnYw8Htwqw0LHodsNZOrQu1fDNJQUrZp4uehknwVohK9YjXuDvVmcFQgMzbEE5d6AVy9YcgcuJQOi54CQ5HeIVqv2PlQJ4JEm3rsOZXJANXyoEpRiV6xKm/1aoKXsz1jF+/DYJTg3xL6fAaJG2H9u3qHZ53SjkDKbogYwtI9yVqnyhbqJqmqRCV6xap4uTgw/oGm7D2Vyf+2JGobWw6Fts/Dli+1uzYV89q3EIQNMvxBlu1J4e4Qb+p4qpZWVYlK9IrV6dvCn+gwX6asOUxyZnGjs24Toe5dsOxlOL1f3wCtidGozbYJjmZvlovWqVItF1jlqESvWB0hBB/2D0dKGL90P1JKsHOAwf8DJ09Y8Cjkntc7TOtwahtkntSWC7zcqbJ5Hb2jUq6hEr1ilerWdOH1bmH8fugsv+0/rW10r61dnM1K1u6czcvSN0hLln8Rtv4fLH4O7F0oCuvFytgU7m9SCw8n1amyqlGJXrFaw9oHER7gwbvLD5CVW9xYtW5b6PsFnNgM394HZw/pG6SlyTwJa8bBf5tqDeQ8AmDofDadzCM9u4B+qmxTJalEr1gtO1sbJj8YQUZ2Pv9ZXSqhtxwKTyzXRvQzu8DBZfoFaSmSdsDPw2BaS20k37ArPPsHPLMGQqJZujsZT2d71amyilKJXrFq4QGePNMxmHnbThKTeO7KE0Ed4IUN4NtYK+Osf0/rpa5cYTRofwS/66b9QTz2B9z9EryyFwbOgsBIAHLyi1hz4Ay9mvupTpVVlEr0itV7rWsYAV7OvLl4H/lFpZK5hz88tQoih8GmT+HHh+DSuRuep9rIuwBbvoLPW2p/BLPPQI//wKgD0O1D8Kp71e7rDp4ht9DAALXASJWlEr1i9Vwc7PhwQDjHzmbz9V/xVz9p5wgPTIMHPocT/8CMaEjdq0+gertcf/+0Gax5U6u/D/kRRuyCu14Ex7JbGizdo3WqjKpfo5IDVkylEr1SLdzbqBZ9W/gz/c9jHDubff0OkU/CU6u1csV33WDvgsoPUi+nYq6pv3eD5/7QmsE1eQBsblyOSc/OZ+PRdPqpTpVVmkr0SrXxTp+mODvY8tbifRiNZXRtDYyE5/+GgChY8jz8NgYMZS2DbAUMRXBgKczsCt/dr9Xf278Mr8bCwO8gIPKWp5BS8r/NiRiMUq0LW8XZ6R2AolQWX3dHxvVqwuhfYlmw4xRD29a7fic3X3hiKawbD1u/gtRYGDRbm4NvDfIuwO45sO1rrVRTIxh6fgwtHwVHN5NPcyIjh7eX7mfj0XTua1xLdaqs4lSiV6qVQVGBLN6dxDtL93Mi4xKvdGmIs8M1pQlbe+gxCfxbw/IRWt1+8Byo20afoM3h/AnYPgN2/gAFF6Fee+g+CRr1vGlp5lqFBiPfboxn2vqjONja8EG/ZjzSrn4FBq6Yg1p4RKl2si4V8tGqOBbsOEXdms5M7N+cTmE3mP99eh/MfxQupECvjyHyKRAWVIs+FaM1c4tbDsJGW33rrpfKtRDLrpPneWvxPg6dvkiPZnV4r28z1bysCrnZwiMq0SvV1tb4DN5aso/4tBz6tfTnnT5N8XFzvH7HS+fgl2fh+O/Q6nHoNRXsq3CCMxTBoRXaFMmk7Vp/n8intA6enrdfS7+QV8iU1Yf5cdsJ6ng48X6/cLo2tZJSlhVRiV5RbiC/yMBXfx7n//46jrODLeN6NWFQVCDi2lG70QB/fgQbp2olnSFzwDNQn6BvJOM4xC6EPfMgq7j+ftdL0PKR26q/XyalZPX+07y34gBpF/N5sn0Qr3drhJujqvhWRSrRK8otHDubzVtL9rE94Rztgmvy0YPNCfUtIznGrYAlw7X594NmQ/A9lR7rVXLStWX8YhdA8g5AQEi0NnoP63Fb9ffSUjJzGb9sP+vjztLUz4PJDzUnItDLrKEr5qUSvaKYwGiU/LzzFB+tOkRugYGX7g1leOfQ62/rTzsC8x+Bc/HQ7QNt1FyZdfuCS3B4lTZ6P7ZeW6u1TnOIGALhD2l3/JaTwSiZvTmRT9YeRkoY1TWMpzoEYWerZmJXdSrRK8ptSLuYzwcrD7J8bwqhvq58NKA57UK8r94p7wIsHQ6HVkL4QOj7OTi4VlxQRgMkbNCSe9xyKMgGj0CIGATNB0Ptpnf8EvuTs3hryT5ik7Lo3MiXD/qFU7emixmCVyqDSvSKUg5/HT7LO8v2c+pcLkOi6vJmr8Z4uThc2cFohE3/hT8+hNrNtLp9zRDzBSClNusndoG2BGL2aXD0hGb9tNF7vfZgc+cj7UsFRXy67giz/kmkhosD7z7QlD4Rftdfp1CqNJXoFaWccgsMfPb7EWZuTKCGiz3v9GlK3xb+VyfBo+vhl2cACQ99p7XwvROZp2Dfz9roPS0ObOwhrDtEDIaG3c064+fPQ2d5e+l+kjNzGdq2HmN7NMbTRS0cYolUoleUO3Qw5QJvLtnH3lOZdArzZWL/a8oa5xJgwWNw5gDcOw7uef32Rtu5mVpL4NiFcGKTtq3e3Vpyb9ofXGqa8+1w9mIeE1Yc5NfYVBrUcmPSg81pE2Te11Aql0r0imIGBqNkzpZEpqw5jEFKXr0/jGc6BmN/+UJlQQ6seEUbjTfqDQO+BiePG5+wKB+OrtNKM0dWg6EAvBtqZZnmA6FmsNnfg9Eo+SnmJJN/O0R+kZGX723AC9Ehqo+8FVCJXlHMKDUrl3eXHWDtwTM0ruPO5IciaFnXS3tSSq0D5Nq3tXr9w3PBt9GVg41GOLVVG7kfWAJ5meDqq13QjRgM/q0qbAbPkTMXeWvxPnacOM/dId5MHBBOSFlTSBWLpBK9olSANQdO8+6yA5y5mMeTdwfxercw3C8vjJ2wUWv9W5Snjex9wrTkvm+h1kzM3gUa99FG7yGdwbbibkLKKzTw5R/H+GbDcVwd7RjXqwkDI8u4KUyxaCrRK0oFuZhXyNQ1h/nf1hPUdndiQr9mdG9WR3syKxkWPg7JO7XHwgZC7tWSe+Pe5bpb9XZtPpbOW0v2kZhxiQdbBTCudxO8y2rzoFi8O070QogewDTAFpgppZx8zfONge+B1sA4KeXUUs95ATOBcEACT0spt9zs9VSiVyzN7pPnebO44Ve3prWZ0K8Zfp7OUJintTu2c9TKM5XU7vhcTgETf43jl11J1Pd2YWL/5nRs6FMpr63o444SvRDCFjgCdAWSgBhgqJTyYKl9agH1gf7A+WsS/Q/ARinlTCGEA+Aipcy82WuqRK9YokKDke82JfDZ+iPY2djwRrcwHr87CNsKXnmp0GAkJ7+I7PwiLhUY2HMyk0m/xXExr4gXokMYcV9DnOzVxVZrd7NEb0phsC1wTEoZX3yy+UA/oCTRSynPAmeFEL2veWEPoBMwrHi/AqCgHO9BUao8e1sbXowOpVe4H28v2897Kw6yZE8KkwY0p6n/ldk3BUVGLhVoiTkn31CcoIuKk7WhVNK+8nxOfhE5BdpzpZN6dn4RBUXG62JpXc+LSQ9G0KiOWhBEMS3RBwCnSj1OAtqZeP4QIA34XgjRAtgJvCKlzLmtKBXFgtTzduGHp9qwfG8KH6w8yANfbsLP06k4SRsoMFyfmMtiayNwdbDFzdEOV0c7XBztcHO0xdvVBTdHO1wcbXF1tMPNQXvetfhxTVcH7gr2Vmu4KiVMSfRl/baYegXXDq1uP0JKuU0IMQ0YC7xz3YsI8TzwPEC9emUs8aYoFkQIQb+WAUSH+fJ/fx0n7WJ+cTLWkrWLg11JAnd11JL5lW1awna0s1EzYxSzMCXRJwF1Sz0OBFJMPH8SkCSl3Fb8eBFaor+OlHIGMAO0Gr2J51eUKs3LxYE3ezXROwylmjPlHu0YoKEQIrj4YurDwHJTTi6lPA2cEkJcvmOkC6Vq+4qiKErFu+WIXkpZJIR4GViDNr1ylpTygBDixeLnvxZC1AF2AB6AUQjxKtBUSnkBGAHMLf4jEQ88VTFvRVEURSmLSbfjSSlXAauu2fZ1qe9Po5V0yjp2D1DmlB9FURSl4qllYxRFUaycSvSKoihWTiV6RVEUK6cSvaIoipVTiV5RFMXKVck2xUKINOBEOQ/3AdLNGE5Vot6b5bLm96feW9VQX0rpW9YTVTLR3wkhxI4bdXCzdOq9WS5rfn/qvVV9qnSjKIpi5VSiVxRFsXLWmOhn6B1ABVLvzXJZ8/tT762Ks7oavaIoinI1axzRK4qiKKWoRK8oimLlrCbRCyF6CCEOCyGOCSHKXNzEUgkh6goh/hRCxAkhDgghXtE7JnMTQtgKIXYLIVbqHYs5CSG8hBCLhBCHin9+d+sdkzkJIV4r/p3cL4T4SQjhpHdM5SWEmCWEOCuE2F9qW00hxDohxNHif2voGWN5WUWiF0LYAtOBnkBTYKgQoqm+UZlVEfC6lLIJcBfwLyt7fwCvAHF6B1EBpgGrpZSNgRZY0XsUQgQAI4EoKWU42noVD+sb1R2ZDfS4ZttY4HcpZUPgd26wQl5VZxWJHmgLHJNSxkspC4D5QD+dYzIbKWWqlHJX8fcX0ZJFgL5RmY8QIhDoDczUOxZzEkJ4AJ2A7wCklAVSykxdgzI/O8BZCGEHuGD6MqNVjpRyA3Dums39gB+Kv/8B6F+ZMZmLtST6AOBUqcdJWFEiLE0IEQS0ArbdYldL8hkwGjDqHIe5hQBpwPfFZamZQghXvYMyFyllMjAVOAmkAllSyrX6RmV2taWUqaANuIBaOsdTLtaS6EUZ26xu3qgQwg34BXi1eJlGiyeE6AOclVLu1DuWCmAHtAb+T0rZCsjBQj/6l6W4Xt0PCAb8AVchxGP6RqWUxVoSfRJQt9TjQCz4I2RZhBD2aEl+rpRysd7xmFEHoK8QIhGt5HafEOJHfUMymyQgSUp5+dPXIrTEby3uBxKklGlSykJgMdBe55jM7YwQwg+g+N+zOsdTLtaS6GOAhkKI4OJFyB8Glusck9kIIQRanTdOSvlfveMxJynlm1LKQCllENrP7Q8ppVWMCovXUj4lhGhUvKkLcFDHkMztJHCXEMKl+He0C1Z0sbnYcuDJ4u+fBJbpGEu5mbQ4eFUnpSwSQrwMrEG78j9LSnlA57DMqQPwOLBPCLGneNtbxYu2K1XbCGBu8QAkHnhK53jMRkq5TQixCNiFNjNsNxbcMkAI8RPQGfARQiQB7wKTgYVCiGfQ/rAN0i/C8lMtEBRFUayctZRuFEVRlBtQiV5RFMXKqUSvKIpi5VSiVxRFsXIq0SuKolg5legVRVGsnEr0iqIoVu7/AbUdpdj7FBEQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(r2_all, label=\"UNet, all\")\n",
    "plt.plot(r2_all_use_p, label=\"UNet, all, use previous month\")\n",
    "plt.plot(r2_single, label=\"UNet, single\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
